{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### For NNs using Images\n",
    "- Convolutions are helpful\n",
    "- SGD isn't as good as newer optimizers like Adam and NesterovAdam - https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f\n",
    "- normalization is helpful (both of the features and the output) and helps prevent divergence\n",
    "- batch normalization works well after the activation https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md\n",
    "- need to go deeper as validation loss is brought in line with training loss\n",
    "\n",
    "\n",
    "### Generally\n",
    "- Never trust the data blindly\n",
    "\n",
    "\n",
    "#### Link to models\n",
    "https://drive.google.com/drive/folders/1IcfVLCy_btNzYmqzvKzyVPFQQkKrYVvZ?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15191720741719057585\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 10459227909075539897\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 10805749470390720517\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11286970368\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 18342555859330319452\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from pandas.io.parsers import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, Input\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.optimizers import Nadam, Adam, SGD, RMSprop\n",
    "from keras.callbacks import EarlyStopping, Callback, History\n",
    "from keras.applications.inception_v3  import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from augmentdata.CustImageDataGenerator import CustImageDataGenerator,CustNumpyArrayIterator\n",
    "\n",
    "print(device_lib.list_local_devices()) # confirm using GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = '/data/data/training.csv'                # train dataset downloaded from Kaggle\n",
    "TEST_DATA = '/data/data/test.csv'                     # test dataset downloaded from Kaggle\n",
    "IMAGE_ROWS = 96\n",
    "IMAGE_COLS = 96\n",
    "INPUT_SHAPE = (IMAGE_ROWS, IMAGE_COLS, 1)\n",
    "RETRAIN = False                                    # bool to load and use existing saved models\n",
    "VERBOSE_TRAIN = False                              # bool to show/hide progress while training a model\n",
    "NUM_KEYPOINTS = 30                                 # maximum no. of facial keypoints for any image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by examining the train dataset of images (training.csv file), which is organised as (x,y) coodinates of each of the 15 facial features for a total of 30 keypoints. The 31st column contains 96x96 image pixel array data coded as raw grayscale values from 0 to 255. There are a total of about 7000 images in the train dataset. \n",
    "\n",
    "**Summary**: \n",
    "*For the EDA, the first pass was done over the whole dataset. From this, roughly 320 images were identified that had missing keypoints or peculiarities, needing further evaluation. In this second pass, we decided to prune the input train dataset to the models, by picking out images that were misfits, and would likely end up confusing the model, rather than helping it. This analysis and some interesting images we came across are illustrated below.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in train dataset:  7049\n"
     ]
    }
   ],
   "source": [
    "# Setting up the data for EDA\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_DATA)\n",
    "Y = np.array(df_train[df_train.columns.difference(['Image'])])\n",
    "X = df_train['Image']\n",
    "labels = list(df_train.columns.difference(['Image']))\n",
    "\n",
    "img_vec_len = IMAGE_ROWS*IMAGE_COLS                             # images pixel grid size\n",
    "\n",
    "imgArray = np.zeros((X.shape[0], img_vec_len), dtype=int)       # temporary array to save each image as numpy array\n",
    "\n",
    "idx=0\n",
    "for i in X.keys(): \n",
    "    imgArray[idx] = np.fromstring(X[i], dtype=int, sep=' ')\n",
    "    idx += 1\n",
    "X = np.reshape( imgArray, (X.shape[0], IMAGE_ROWS, IMAGE_COLS, 1) )\n",
    "print(\"Total images in train dataset: \", X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to subplot a group of images, and label the ones with missing keypoints distinctly\n",
    "\n",
    "def plot_images(images, points, type='actual', subplotting=False, gridRows=0, gridCols=0, \n",
    "                imageIndices=1, subtitles=True, title=None, labelsList=[] ):\n",
    "    \n",
    "    plt.figure(figsize=(4*gridCols, 4*gridCols))    \n",
    "    img_nums = images.shape[0]\n",
    "    points_nums = points.shape[0]\n",
    "    \n",
    "    if ( ( img_nums != points_nums) | ( img_nums != imageIndices.shape[0] ) ):\n",
    "        raise ValueError(\"Mismatch in number of images and keypoints' rows passed to plot_images().\")\n",
    "    \n",
    "    \n",
    "    for thisImg in range(0, gridRows*gridCols ):\n",
    "        \n",
    "        if subplotting:\n",
    "            plt.subplot(gridRows, gridCols, thisImg + 1)\n",
    "            noKeypNums  = np.isnan(points[thisImg]).sum()\n",
    "            \n",
    "            if subtitles:\n",
    "                if( noKeypNums == 0 ):                                                            # no missing keypoints (group1)\n",
    "                    plt.title(\"#: \" + str(imageIndices[thisImg]) + \n",
    "                              \",  Points: \" + str(NUM_KEYPOINTS - noKeypNums), color='k')         # title in black\n",
    "                \n",
    "                elif( ( noKeypNums > 0) & (noKeypNums < 22 ) ):                                   # (1,21) missing keypoints\n",
    "                    plt.title(\"#: \" + str(imageIndices[thisImg]) + \n",
    "                              \",  Points: \" + str(NUM_KEYPOINTS - noKeypNums), color='m')         # title in magenta\n",
    "                \n",
    "                elif( ( noKeypNums == 22 ) ):                                                     # 22 missing keypoints (group2)\n",
    "                    plt.title(\"#: \" + str(imageIndices[thisImg]) + \n",
    "                              \",  Points: \" + str(NUM_KEYPOINTS - noKeypNums), color='b')         # title in blue\n",
    "                \n",
    "                else:                                                                             # > 22 missing keypoints\n",
    "                    plt.title(\"#: \" + str(imageIndices[thisImg]) + \n",
    "                              \",  Points: \" + str(NUM_KEYPOINTS - noKeypNums), color='r')         # title in red\n",
    "        \n",
    "        plt.imshow(np.reshape(images[thisImg,:],(96,96)), cmap = 'gray')\n",
    "\n",
    "        x = 0\n",
    "        for idx in range(0, points[thisImg].shape[0]):\n",
    "            label = labelsList[idx]\n",
    "            if label[-1]=='x':\n",
    "                x = points[thisImg, idx]\n",
    "            else:\n",
    "                if label in ['left_eye_center_y',\n",
    "                             'left_eye_inner_corner_y', \n",
    "                             'left_eye_outer_corner_y', \n",
    "                             'left_eyebrow_inner_end_y', \n",
    "                             'left_eyebrow_outer_end_y',\n",
    "                             'mouth_left_corner_y'\n",
    "                            ]:\n",
    "                    if(type=='actual'):\n",
    "                        plt.plot(x, points[thisImg, idx], 'c<')\n",
    "                    else:\n",
    "                        plt.plot(x, points[thisImg, idx], 'c*')\n",
    "                        \n",
    "                elif label in ['right_eye_center_y',\n",
    "                             'right_eye_inner_corner_y', \n",
    "                             'right_eye_outer_corner_y', \n",
    "                             'right_eyebrow_inner_end_y', \n",
    "                             'right_eyebrow_outer_end_y',\n",
    "                              'mouth_right_corner_y']:\n",
    "                    if(type=='actual'):\n",
    "                        plt.plot(x, points[thisImg, idx], 'r>')\n",
    "                    else:\n",
    "                        plt.plot(x, points[thisImg, idx], 'r*')\n",
    "                \n",
    "                else:\n",
    "                    if(type=='actual'):\n",
    "                        plt.plot(x, points[thisImg, idx], 'mo')\n",
    "                    else:\n",
    "                        plt.plot(x, points[thisImg, idx], 'm*')\n",
    "                    \n",
    "        plt.axis('off')\n",
    "    \n",
    "    if (title != None):\n",
    "        plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot an array of image indices\n",
    "\n",
    "idx_max = df_train.shape[0]                      # all images\n",
    "grid_cols = 4                                    # grid columns size for a subplot of images\n",
    "grid_rows = 4                                    # grid rows size for images subplot\n",
    "subImgNum = grid_cols * grid_rows\n",
    "\n",
    "def plot_img_group( thisGroup, dataset='train', denorm=False, thisLabels=labels, thisSubTitle=True, thisTitle=None ):\n",
    "\n",
    "    if(dataset == 'train'):\n",
    "        thisX = X\n",
    "        thisY = Y\n",
    "    elif(dataset == 'group1'):\n",
    "        thisX = X1\n",
    "        thisY = Y1\n",
    "    elif(dataset == 'group2'):\n",
    "        thisX = X2\n",
    "        thisY = Y2     \n",
    "    \n",
    "    thisSubsetX = np.zeros( (subImgNum, IMAGE_ROWS, IMAGE_COLS, 1), dtype=float)\n",
    "    thisSubsetY = np.zeros( (subImgNum, thisY.shape[1]), dtype=float)\n",
    "    img_indices = np.zeros( (subImgNum, 1), dtype=int)\n",
    "\n",
    "    img_sub = 0                                  # local iterator for images in subplot\n",
    "    flushed = False\n",
    "    for img in thisGroup:\n",
    "        \n",
    "        if( ( (img_sub + 1 )  % subImgNum ) != 0 ):\n",
    "            thisSubsetX[img_sub,:] = thisX[img-1,:]\n",
    "            if denorm:\n",
    "                thisSubsetY[img_sub,:] = 48*thisY[img-1,:] + 48\n",
    "            else:\n",
    "                thisSubsetY[img_sub,:] = thisY[img-1,:]\n",
    "            img_indices[img_sub] = img\n",
    "            img_sub += 1\n",
    "            flushed = False\n",
    "            \n",
    "        else:\n",
    "            thisSubsetX[img_sub,:] = thisX[img-1,:]\n",
    "            if denorm:\n",
    "                thisSubsetY[img_sub,:] = 48*thisY[img-1,:] + 48\n",
    "            else:\n",
    "                thisSubsetY[img_sub,:] = thisY[img-1,:]\n",
    "            img_indices[img_sub] = img\n",
    "            \n",
    "            # plot when all images for the subplot are accumulated\n",
    "            plot_images(images=thisSubsetX, points=thisSubsetY, subplotting=True, \n",
    "                        gridRows=grid_rows, gridCols=grid_cols, imageIndices=img_indices, \n",
    "                        subtitles=thisSubTitle, title=thisTitle, labelsList=thisLabels )\n",
    "            \n",
    "            # reset subplot indexing pointer and subplot image/keypoints buckets\n",
    "            img_sub = 0\n",
    "            flushed = True\n",
    "            thisSubsetX = np.zeros( (subImgNum, IMAGE_ROWS, IMAGE_COLS, 1), dtype=int)\n",
    "            thisSubsetY = np.zeros( (subImgNum, Y.shape[1]), dtype=float)\n",
    "    \n",
    "    if not flushed:                              # for images leftover from partial subplot grid\n",
    "\n",
    "        thisGridRows = ( (img_sub - 1) // grid_rows ) + 1\n",
    "        if( thisGridRows > 1 ):\n",
    "            thisGridCols = grid_cols\n",
    "        else:\n",
    "            thisGridCols = img_sub\n",
    "\n",
    "        plot_images(images=thisSubsetX, points=thisSubsetY, subplotting=True, \n",
    "                    gridRows=thisGridRows, gridCols=thisGridCols, imageIndices=img_indices, \n",
    "                    subtitles=thisSubTitle, title=thisTitle, labelsList=thisLabels )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images in the train dataset seem to be broadly split into two groups. The first group starts from the beginning of the dataset and (index 1, or row 1 in .csv) and continues till index 2284. Beyond that, the images have a maximum of 8 keypoints.\n",
    "\n",
    "Below is a sample of images around this boundary - images with 30 keypoints or less and those with 8 keypoints or less. Also, there are some images with missing keypoints, with their titles highlighted in magenta or red colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Final indices of images to be dropped from the dataset\n",
    "\n",
    "IDX_BAD_IMAGES = np.array( [1621, 1862, 1748, 1878, 1927, 2200, 2431, 2584, 2647, \n",
    "                            2671, 2765, 4198, 1627, 1628, 1637, 1957, 4477, 1820, \n",
    "                            2064, 2089, 2091, 2109, 2195, 4264, 4491, 6490, 6493, \n",
    "                            6494, 1655, 2096, 2454, 3206, 3287, 5628, 5653, 6754, \n",
    "                            6755, 2321, 2322, 2414, 2428, 2462, 2574, 2584, 2663, \n",
    "                            2691, 2694, 2830, 2910, 2916, 3126, 3176, 3291, 3299, \n",
    "                            3361, 4061, 4483, 4484, 4494, 4766, 4809, 4837, 4880, \n",
    "                            4905, 5068, 5362, 5566, 5868, 6535, 6538, 6588, 6605, \n",
    "                            6659, 6724, 6733, 6753, 6758, 6766, 6907 ] )\n",
    "#plot_img_group( IDX_BAD_IMAGES )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the training data for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean up the train dataset, normalize it, drop bad images & labels, \n",
    "# and finally split the dataset into 2, for group1 and group2 modelling.\n",
    "\n",
    "def loaderV2(test=False, seed=None, keeplabels=None):\n",
    "    \n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    fileloc = TEST_DATA if test else TRAIN_DATA\n",
    "    \n",
    "    df = read_csv(fileloc)\n",
    "    \n",
    "    df['Image'] = df['Image'].apply(lambda x: np.fromstring(x, sep=' '))\n",
    "    \n",
    "    if keeplabels:\n",
    "        df = df[list(keeplabels) + ['Image']]\n",
    "        \n",
    "    X = np.vstack(df['Image'])\n",
    "    \n",
    "    if not test:                                                  # process train dataset\n",
    "        Y = df[df.columns.difference(['Image'])].values\n",
    "        Y = Y.astype(np.float32)\n",
    "        \n",
    "        # remove rows having bad images or labels\n",
    "        X = np.delete( X, (IDX_BAD_IMAGES - 1), axis=0 )\n",
    "        Y = np.delete( Y, (IDX_BAD_IMAGES - 1), axis=0 )\n",
    "        \n",
    "        # normalize - by pixel across the whole dataset subtract mean and divide by stdev\n",
    "        X = X - np.tile(np.mean(X,axis=0),(X.shape[0],1))\n",
    "        X = X / np.tile(np.std(X,axis=0),(X.shape[0],1))\n",
    "        X = X.astype(np.float32)\n",
    "    \n",
    "        Y = (Y - 48) / 48                     # this helps, but tanh on output doesnt\n",
    "        shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "        X, Y = X[shuffle], Y[shuffle]\n",
    "    \n",
    "        X = X.reshape(-1, 96, 96, 1)\n",
    "        \n",
    "        # split X and Y into dataset for model1 (more than 8 keypoints) and model2 (less than 8 keypoints)\n",
    "        X_model1 = np.zeros( (X.shape[0], 96,96,1), dtype=np.float32)\n",
    "        X_model2 = np.zeros( (X.shape[0], 96,96,1), dtype=np.float32)\n",
    "        Y_model1 = np.zeros( Y.shape, dtype=float)\n",
    "        Y_model2 = np.zeros( Y.shape, dtype=float)\n",
    "        tempIdx1 = 0\n",
    "        tempIdx2 = 0\n",
    "        \n",
    "        for thisIdx in range(0, Y.shape[0]):\n",
    "            numKeyps  = NUM_KEYPOINTS - np.isnan(Y[thisIdx]).sum()\n",
    "            \n",
    "            if( ( numKeyps > 8 ) ):\n",
    "                X_model1[tempIdx1] = X[thisIdx,:,:]\n",
    "                Y_model1[tempIdx1] = Y[thisIdx,:]\n",
    "                tempIdx1 = tempIdx1 + 1\n",
    "            else:\n",
    "                X_model2[tempIdx2] = X[thisIdx,:,:]\n",
    "                Y_model2[tempIdx2] = Y[thisIdx,:]\n",
    "                tempIdx2 = tempIdx2 + 1\n",
    "    \n",
    "        # remove empty rows\n",
    "        drop_idx1 = []\n",
    "        drop_idx2 = []\n",
    "        \n",
    "        for idx in range(0, X.shape[0]):\n",
    "            if( (np.all(Y_model1[idx] == 0)) | (np.isnan(Y_model1[idx]).sum() != 0) ):\n",
    "                drop_idx1.append(idx)\n",
    "            if( (np.all(Y_model2[idx] == 0)) | (np.isnan(Y_model2[idx]).sum() != 22) ):\n",
    "                drop_idx2.append(idx)\n",
    "        \n",
    "        X_model1 = np.delete( X_model1, np.array(drop_idx1), axis=0 )\n",
    "        Y_model1 = np.delete( Y_model1, np.array(drop_idx1), axis=0 )\n",
    "        X_model2 = np.delete( X_model2, np.array(drop_idx2), axis=0 )\n",
    "        Y_model2 = np.delete( Y_model2, np.array(drop_idx2), axis=0 )            \n",
    "        \n",
    "        # remove empty columns, setup lists of labels\n",
    "        labels = df.columns.difference(['Image'])\n",
    "        labels1 = labels\n",
    "        labels2 = []\n",
    "        drop_idx3 = []\n",
    "        for idx in range(0, Y.shape[1]):\n",
    "            if( (np.all(Y_model2[:,idx] == 0)) | (np.isnan(Y_model2[:,idx]).sum() != 0) ):\n",
    "                drop_idx3.append(idx)\n",
    "            else:\n",
    "                labels2.append(labels[idx])\n",
    "        Y_model2 = np.delete( Y_model2, np.array(drop_idx3), axis=1 ) \n",
    "        \n",
    "        # return the original dataset and the group splits\n",
    "        return X_model1, Y_model1, labels1, X_model2, Y_model2, labels2, X, Y, labels\n",
    "    \n",
    "    else:                                             # for test dataset\n",
    "        Y = None\n",
    "        \n",
    "        # normalize - by pixel across the whole dataset subtract mean and divide by stdev\n",
    "        X = X - np.tile(np.mean(X,axis=0),(X.shape[0],1))\n",
    "        X = X / np.tile(np.std(X,axis=0),(X.shape[0],1))\n",
    "        X = X.reshape(-1, 96, 96, 1)\n",
    "        labels = df.columns.difference(['Image'])\n",
    "        \n",
    "        return X, Y, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X1, Y1, labels1,   X2, Y2, labels2,   X, Y, labels = loaderV2(seed=42)\n",
    "\n",
    "X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42)\n",
    "\n",
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(X2, Y2,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42)\n",
    "\n",
    "data1 = [X1_train, X1_test, Y1_train, Y1_test]\n",
    "data2 = [X2_train, X2_test, Y2_train, Y2_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group1 data Y1 shape:  (2137, 30) , X1 shape:  (2137, 96, 96, 1)\n",
      "Group2 data Y2 shape:  (4697, 8) , X2 shape:  (4697, 96, 96, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Group1 data Y1 shape: \", Y1.shape, \", X1 shape: \", X1.shape)\n",
    "print(\"Group2 data Y2 shape: \", Y2.shape, \", X2 shape: \", X2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some sample images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, data, modelname,\n",
    "              generator=None,retrain=RETRAIN,\n",
    "              epochs=1000, patience=1000, optimizer='nadam'):\n",
    "    if retrain or not os.path.exists('/data/saved-models/' + modelname + '.h5'):\n",
    "        X_train = data[0]\n",
    "        y_train = data[2]\n",
    "        if len(data) == 4:\n",
    "            valid_dat = (data[1], data[3])\n",
    "        else:\n",
    "            valid_dat = None\n",
    "\n",
    "        model.compile(loss='mse', optimizer=optimizer)\n",
    "        \n",
    "        if valid_dat:\n",
    "            earlystop = EarlyStopping(monitor='val_loss',\n",
    "                                     patience=patience,\n",
    "                                     verbose=1,\n",
    "                                     mode=\"auto\")\n",
    "            callbacks = [earlystop]\n",
    "        else:\n",
    "            callbacks = None\n",
    "        \n",
    "        if generator:\n",
    "            history = model.fit_generator(generator,\n",
    "                        epochs=epochs,\n",
    "                        steps_per_epoch=data[0].shape[0]//32,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=valid_dat\n",
    "             )\n",
    "        else:\n",
    "            history = model.fit(X_train, y_train,\n",
    "                                epochs=epochs,\n",
    "                                batch_size=32,\n",
    "                                callbacks=callbacks,\n",
    "                                validation_data=valid_dat,\n",
    "                                verbose=VERBOSE_TRAIN\n",
    "                     )\n",
    "        model.save('/data/saved-models/'+ modelname + '.h5')\n",
    "        with open('/data/saved-models/histories/'+modelname+'_hist',\n",
    "                  'wb') as file_pi:\n",
    "            pickle.dump(history.history, file_pi)\n",
    "        history = history.history\n",
    "    else:\n",
    "        model = load_model('/data/saved-models/'+modelname+'.h5')\n",
    "        history = pickle.load(open( \"/data/saved-models/histories/\" + modelname + '_hist',\n",
    "                                   \"rb\" ))\n",
    "        \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(hists, names):\n",
    "    colordict = {\n",
    "        0 : 'b',\n",
    "        1 : 'g',\n",
    "        2 : 'r',\n",
    "        3 : 'c',\n",
    "        4 : 'm'\n",
    "    }\n",
    "    \n",
    "    for i, hist in enumerate(hists):\n",
    "        plt.plot([np.sqrt(x) * 48 for x in hist['loss']],\n",
    "                 color=colordict[i],\n",
    "                 label=\"train \" + names[i])\n",
    "        plt.plot([np.sqrt(x) * 48 for x in hist['val_loss']],\n",
    "                 color=colordict[i],\n",
    "                 linestyle=':',\n",
    "                 label=\"valid \" + names[i])\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"RMSE\")\n",
    "        plt.yscale(\"log\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>CNN_added layers model: </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = CustImageDataGenerator(\n",
    "    rotation_range=5. #degrees\n",
    "     ,horizontal_flip=True\n",
    "     ,width_shift_range=.05 # percent of image width\n",
    "     ,height_shift_range=.05 # percent of image height\n",
    "    ).flow(X1_train,Y1_train,whichlabels=list(labels1), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 91, 91, 32)        1184      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 91, 91, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 45, 45, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 41, 41, 64)        51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 41, 41, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 20, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 17, 17, 256)       262400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 17, 17, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 6, 6, 64)          147520    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 2, 2, 128)         32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 2, 2, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 500)               64500     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 30)                15030     \n",
      "=================================================================\n",
      "Total params: 831,470\n",
      "Trainable params: 828,382\n",
      "Non-trainable params: 3,088\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n",
      "53/53 [==============================] - 6s 115ms/step - loss: 1.1107 - val_loss: 0.4095\n",
      "Epoch 2/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.5005 - val_loss: 0.1370\n",
      "Epoch 3/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.3460 - val_loss: 0.0405\n",
      "Epoch 4/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.2385 - val_loss: 0.0320\n",
      "Epoch 5/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.1663 - val_loss: 0.0317\n",
      "Epoch 6/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.1157 - val_loss: 0.0150\n",
      "Epoch 7/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0851 - val_loss: 0.0124\n",
      "Epoch 8/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0615 - val_loss: 0.0241\n",
      "Epoch 9/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0461 - val_loss: 0.0088\n",
      "Epoch 10/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0355 - val_loss: 0.0101\n",
      "Epoch 11/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0270 - val_loss: 0.0064\n",
      "Epoch 12/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0214 - val_loss: 0.0073\n",
      "Epoch 13/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0175 - val_loss: 0.0065\n",
      "Epoch 14/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0159 - val_loss: 0.0058\n",
      "Epoch 15/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0143 - val_loss: 0.0054\n",
      "Epoch 16/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0136 - val_loss: 0.0079\n",
      "Epoch 17/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0128 - val_loss: 0.0056\n",
      "Epoch 18/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0122 - val_loss: 0.0064\n",
      "Epoch 19/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0114 - val_loss: 0.0057\n",
      "Epoch 20/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0117 - val_loss: 0.0070\n",
      "Epoch 21/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0122 - val_loss: 0.0064\n",
      "Epoch 22/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0123 - val_loss: 0.0051\n",
      "Epoch 23/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0110 - val_loss: 0.0058\n",
      "Epoch 24/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0116 - val_loss: 0.0059\n",
      "Epoch 25/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0116 - val_loss: 0.0055\n",
      "Epoch 26/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0111 - val_loss: 0.0051\n",
      "Epoch 27/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0109 - val_loss: 0.0065\n",
      "Epoch 28/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0107 - val_loss: 0.0050\n",
      "Epoch 29/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0111 - val_loss: 0.0051\n",
      "Epoch 30/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0104 - val_loss: 0.0057\n",
      "Epoch 31/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0100 - val_loss: 0.0050\n",
      "Epoch 32/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0094 - val_loss: 0.0050\n",
      "Epoch 33/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0087 - val_loss: 0.0047\n",
      "Epoch 34/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0084 - val_loss: 0.0050\n",
      "Epoch 35/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0075 - val_loss: 0.0042\n",
      "Epoch 36/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0070 - val_loss: 0.0043\n",
      "Epoch 37/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0069 - val_loss: 0.0038\n",
      "Epoch 38/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0060 - val_loss: 0.0039\n",
      "Epoch 39/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0057 - val_loss: 0.0040\n",
      "Epoch 40/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0054 - val_loss: 0.0038\n",
      "Epoch 41/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0051 - val_loss: 0.0033\n",
      "Epoch 42/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0050 - val_loss: 0.0034\n",
      "Epoch 43/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0048 - val_loss: 0.0034\n",
      "Epoch 44/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0045 - val_loss: 0.0030\n",
      "Epoch 45/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0045 - val_loss: 0.0031\n",
      "Epoch 46/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0043 - val_loss: 0.0032\n",
      "Epoch 47/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 48/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0041 - val_loss: 0.0029\n",
      "Epoch 49/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0041 - val_loss: 0.0026\n",
      "Epoch 50/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0039 - val_loss: 0.0028\n",
      "Epoch 51/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0039 - val_loss: 0.0025\n",
      "Epoch 52/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0039 - val_loss: 0.0023\n",
      "Epoch 53/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0039 - val_loss: 0.0026\n",
      "Epoch 54/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0038 - val_loss: 0.0025\n",
      "Epoch 55/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0037 - val_loss: 0.0028\n",
      "Epoch 56/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0037 - val_loss: 0.0024\n",
      "Epoch 57/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0036 - val_loss: 0.0024\n",
      "Epoch 58/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0037 - val_loss: 0.0027\n",
      "Epoch 59/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0035 - val_loss: 0.0024\n",
      "Epoch 60/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0035 - val_loss: 0.0022\n",
      "Epoch 61/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0035 - val_loss: 0.0027\n",
      "Epoch 62/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0036 - val_loss: 0.0025\n",
      "Epoch 63/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0035 - val_loss: 0.0021\n",
      "Epoch 64/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0035 - val_loss: 0.0024\n",
      "Epoch 65/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0035 - val_loss: 0.0028\n",
      "Epoch 66/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0035 - val_loss: 0.0022\n",
      "Epoch 67/1000\n",
      "53/53 [==============================] - 4s 71ms/step - loss: 0.0035 - val_loss: 0.0024\n",
      "Epoch 68/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0033 - val_loss: 0.0025\n",
      "Epoch 69/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0035 - val_loss: 0.0025\n",
      "Epoch 70/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 71/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0033 - val_loss: 0.0023\n",
      "Epoch 72/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 73/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0035 - val_loss: 0.0031\n",
      "Epoch 74/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0036 - val_loss: 0.0027\n",
      "Epoch 75/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0035 - val_loss: 0.0025\n",
      "Epoch 76/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0035 - val_loss: 0.0024\n",
      "Epoch 77/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0036 - val_loss: 0.0029\n",
      "Epoch 78/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0035 - val_loss: 0.0026\n",
      "Epoch 79/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0036 - val_loss: 0.0026\n",
      "Epoch 80/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 81/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0035 - val_loss: 0.0026\n",
      "Epoch 82/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0035 - val_loss: 0.0021\n",
      "Epoch 83/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0035 - val_loss: 0.0022\n",
      "Epoch 84/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0034 - val_loss: 0.0024\n",
      "Epoch 85/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0035 - val_loss: 0.0023\n",
      "Epoch 86/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0033 - val_loss: 0.0023\n",
      "Epoch 87/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0035 - val_loss: 0.0023\n",
      "Epoch 88/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0033 - val_loss: 0.0024\n",
      "Epoch 89/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0032 - val_loss: 0.0022\n",
      "Epoch 90/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0033 - val_loss: 0.0021\n",
      "Epoch 91/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0032 - val_loss: 0.0023\n",
      "Epoch 92/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0035 - val_loss: 0.0023\n",
      "Epoch 93/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0032 - val_loss: 0.0027\n",
      "Epoch 94/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0034 - val_loss: 0.0019\n",
      "Epoch 95/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0032 - val_loss: 0.0018\n",
      "Epoch 96/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0032 - val_loss: 0.0021\n",
      "Epoch 97/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0031 - val_loss: 0.0018\n",
      "Epoch 98/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0031 - val_loss: 0.0020\n",
      "Epoch 99/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0031 - val_loss: 0.0019\n",
      "Epoch 100/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0030 - val_loss: 0.0021\n",
      "Epoch 101/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0029 - val_loss: 0.0023\n",
      "Epoch 102/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0030 - val_loss: 0.0017\n",
      "Epoch 103/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0029 - val_loss: 0.0017\n",
      "Epoch 104/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0028 - val_loss: 0.0017\n",
      "Epoch 105/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0028 - val_loss: 0.0018\n",
      "Epoch 106/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0028 - val_loss: 0.0018\n",
      "Epoch 107/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0027 - val_loss: 0.0019\n",
      "Epoch 108/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0027 - val_loss: 0.0019\n",
      "Epoch 109/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 110/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0025 - val_loss: 0.0018\n",
      "Epoch 111/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0026 - val_loss: 0.0015\n",
      "Epoch 112/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0025 - val_loss: 0.0017\n",
      "Epoch 113/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0025 - val_loss: 0.0015\n",
      "Epoch 114/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 115/1000\n",
      "53/53 [==============================] - 4s 71ms/step - loss: 0.0023 - val_loss: 0.0019\n",
      "Epoch 116/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 117/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 118/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 119/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 120/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 121/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 122/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 123/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 124/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 125/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 126/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 127/1000\n",
      "53/53 [==============================] - 4s 71ms/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 128/1000\n",
      "53/53 [==============================] - 4s 71ms/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 129/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 130/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 131/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 132/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 133/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 134/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 135/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 136/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 137/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 138/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0015 - val_loss: 9.9229e-04\n",
      "Epoch 139/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0015 - val_loss: 0.0010\n",
      "Epoch 140/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0015 - val_loss: 9.8618e-04\n",
      "Epoch 141/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0015 - val_loss: 9.2540e-04\n",
      "Epoch 142/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0014 - val_loss: 9.8193e-04\n",
      "Epoch 143/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0015 - val_loss: 9.7288e-04\n",
      "Epoch 144/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 145/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0013 - val_loss: 8.7277e-04\n",
      "Epoch 146/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0014 - val_loss: 8.7299e-04\n",
      "Epoch 147/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0013 - val_loss: 8.9985e-04\n",
      "Epoch 148/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0013 - val_loss: 9.0352e-04\n",
      "Epoch 149/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0013 - val_loss: 8.3467e-04\n",
      "Epoch 150/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0014 - val_loss: 8.6242e-04\n",
      "Epoch 151/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0013 - val_loss: 9.2831e-04\n",
      "Epoch 152/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 153/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0012 - val_loss: 8.3190e-04\n",
      "Epoch 154/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0012 - val_loss: 8.1768e-04\n",
      "Epoch 155/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0012 - val_loss: 8.3400e-04\n",
      "Epoch 156/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0012 - val_loss: 8.5134e-04\n",
      "Epoch 157/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0011 - val_loss: 8.0273e-04\n",
      "Epoch 158/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0011 - val_loss: 8.6642e-04\n",
      "Epoch 159/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0011 - val_loss: 8.3673e-04\n",
      "Epoch 160/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0012 - val_loss: 7.8075e-04\n",
      "Epoch 161/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0011 - val_loss: 7.6686e-04\n",
      "Epoch 162/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0011 - val_loss: 7.7875e-04\n",
      "Epoch 163/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0011 - val_loss: 7.5797e-04\n",
      "Epoch 164/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0012 - val_loss: 7.4356e-04\n",
      "Epoch 165/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0011 - val_loss: 7.7243e-04\n",
      "Epoch 166/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0010 - val_loss: 7.6458e-04\n",
      "Epoch 167/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0010 - val_loss: 6.8062e-04\n",
      "Epoch 168/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0012 - val_loss: 7.8481e-04\n",
      "Epoch 169/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0011 - val_loss: 7.6529e-04\n",
      "Epoch 170/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0010 - val_loss: 7.1773e-04\n",
      "Epoch 171/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0011 - val_loss: 8.4448e-04\n",
      "Epoch 172/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0011 - val_loss: 7.1646e-04\n",
      "Epoch 173/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0011 - val_loss: 8.1700e-04\n",
      "Epoch 174/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0010 - val_loss: 7.4256e-04\n",
      "Epoch 175/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0011 - val_loss: 7.0346e-04\n",
      "Epoch 176/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0010 - val_loss: 7.4653e-04\n",
      "Epoch 177/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0011 - val_loss: 7.6023e-04\n",
      "Epoch 178/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0010 - val_loss: 7.1730e-04\n",
      "Epoch 179/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.9999e-04 - val_loss: 7.7648e-04\n",
      "Epoch 180/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.6942e-04 - val_loss: 6.9943e-04\n",
      "Epoch 181/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0010 - val_loss: 7.4152e-04\n",
      "Epoch 182/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0010 - val_loss: 7.2034e-04\n",
      "Epoch 183/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.6885e-04 - val_loss: 7.4303e-04\n",
      "Epoch 184/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.5582e-04 - val_loss: 6.8558e-04\n",
      "Epoch 185/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0010 - val_loss: 7.3878e-04\n",
      "Epoch 186/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0010 - val_loss: 7.0187e-04\n",
      "Epoch 187/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0010 - val_loss: 7.5296e-04\n",
      "Epoch 188/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0010 - val_loss: 6.5169e-04\n",
      "Epoch 189/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.8860e-04 - val_loss: 6.5231e-04\n",
      "Epoch 190/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0010 - val_loss: 7.2829e-04\n",
      "Epoch 191/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.8408e-04 - val_loss: 7.6179e-04\n",
      "Epoch 192/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.8440e-04 - val_loss: 6.7885e-04\n",
      "Epoch 193/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.9680e-04 - val_loss: 6.9047e-04\n",
      "Epoch 194/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.8659e-04 - val_loss: 7.9174e-04\n",
      "Epoch 195/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.8961e-04 - val_loss: 9.2312e-04\n",
      "Epoch 196/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.7354e-04 - val_loss: 7.1126e-04\n",
      "Epoch 197/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0010 - val_loss: 7.2764e-04\n",
      "Epoch 198/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.8303e-04 - val_loss: 7.1671e-04\n",
      "Epoch 199/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.4714e-04 - val_loss: 7.5540e-04\n",
      "Epoch 200/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.4785e-04 - val_loss: 7.3010e-04\n",
      "Epoch 201/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.4704e-04 - val_loss: 7.1541e-04\n",
      "Epoch 202/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.2533e-04 - val_loss: 7.0000e-04\n",
      "Epoch 203/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.7392e-04 - val_loss: 7.2572e-04\n",
      "Epoch 204/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.8583e-04 - val_loss: 6.8053e-04\n",
      "Epoch 205/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 4s 68ms/step - loss: 9.9942e-04 - val_loss: 8.2720e-04\n",
      "Epoch 206/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0010 - val_loss: 7.0351e-04\n",
      "Epoch 207/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.3854e-04 - val_loss: 7.4242e-04\n",
      "Epoch 208/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 9.6038e-04 - val_loss: 7.8034e-04\n",
      "Epoch 209/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.7075e-04 - val_loss: 7.1487e-04\n",
      "Epoch 210/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 0.0010 - val_loss: 6.8220e-04\n",
      "Epoch 211/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.9803e-04 - val_loss: 7.4608e-04\n",
      "Epoch 212/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0010 - val_loss: 6.9395e-04\n",
      "Epoch 213/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0010 - val_loss: 6.9529e-04\n",
      "Epoch 214/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.5188e-04 - val_loss: 6.9556e-04\n",
      "Epoch 215/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.7141e-04 - val_loss: 8.4734e-04\n",
      "Epoch 216/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.4641e-04 - val_loss: 7.3573e-04\n",
      "Epoch 217/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0010 - val_loss: 6.9505e-04\n",
      "Epoch 218/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.6160e-04 - val_loss: 6.7219e-04\n",
      "Epoch 219/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0010 - val_loss: 8.3848e-04\n",
      "Epoch 220/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.0281e-04 - val_loss: 7.6701e-04\n",
      "Epoch 221/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.5971e-04 - val_loss: 7.8713e-04\n",
      "Epoch 222/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.6367e-04 - val_loss: 7.3604e-04\n",
      "Epoch 223/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.9959e-04 - val_loss: 7.3408e-04\n",
      "Epoch 224/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.8740e-04 - val_loss: 7.8996e-04\n",
      "Epoch 225/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.8452e-04 - val_loss: 7.4867e-04\n",
      "Epoch 226/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.7726e-04 - val_loss: 7.3483e-04\n",
      "Epoch 227/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0010 - val_loss: 6.9236e-04\n",
      "Epoch 228/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.6938e-04 - val_loss: 7.3222e-04\n",
      "Epoch 229/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.3217e-04 - val_loss: 8.0825e-04\n",
      "Epoch 230/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.6919e-04 - val_loss: 6.8122e-04\n",
      "Epoch 231/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.4094e-04 - val_loss: 7.0188e-04\n",
      "Epoch 232/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.8242e-04 - val_loss: 7.6505e-04\n",
      "Epoch 233/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.3856e-04 - val_loss: 6.9640e-04\n",
      "Epoch 234/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.6537e-04 - val_loss: 7.5614e-04\n",
      "Epoch 235/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.8108e-04 - val_loss: 6.6386e-04\n",
      "Epoch 236/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.2180e-04 - val_loss: 7.0526e-04\n",
      "Epoch 237/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.1265e-04 - val_loss: 8.0106e-04\n",
      "Epoch 238/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.4515e-04 - val_loss: 7.1257e-04\n",
      "Epoch 239/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.9456e-04 - val_loss: 6.6844e-04\n",
      "Epoch 240/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.1759e-04 - val_loss: 7.6524e-04\n",
      "Epoch 241/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.3539e-04 - val_loss: 7.3662e-04\n",
      "Epoch 242/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.3933e-04 - val_loss: 7.0925e-04\n",
      "Epoch 243/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.3380e-04 - val_loss: 7.2055e-04\n",
      "Epoch 244/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.5506e-04 - val_loss: 6.4100e-04\n",
      "Epoch 245/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.0345e-04 - val_loss: 7.0454e-04\n",
      "Epoch 246/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.5505e-04 - val_loss: 7.0140e-04\n",
      "Epoch 247/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.2808e-04 - val_loss: 6.8104e-04\n",
      "Epoch 248/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.2685e-04 - val_loss: 7.4154e-04\n",
      "Epoch 249/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.3885e-04 - val_loss: 7.8143e-04\n",
      "Epoch 250/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.1192e-04 - val_loss: 6.9197e-04\n",
      "Epoch 251/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.1127e-04 - val_loss: 8.0334e-04\n",
      "Epoch 252/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.8811e-04 - val_loss: 7.0301e-04\n",
      "Epoch 253/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.8913e-04 - val_loss: 9.7822e-04\n",
      "Epoch 254/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.3672e-04 - val_loss: 6.8187e-04\n",
      "Epoch 255/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.7636e-04 - val_loss: 0.0010\n",
      "Epoch 256/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.2441e-04 - val_loss: 8.4622e-04\n",
      "Epoch 257/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.0189e-04 - val_loss: 6.7308e-04\n",
      "Epoch 258/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.4820e-04 - val_loss: 6.9295e-04\n",
      "Epoch 259/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.1351e-04 - val_loss: 6.7917e-04\n",
      "Epoch 260/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.6118e-04 - val_loss: 7.3399e-04\n",
      "Epoch 261/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.4212e-04 - val_loss: 0.0010\n",
      "Epoch 262/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.6441e-04 - val_loss: 6.8778e-04\n",
      "Epoch 263/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.2621e-04 - val_loss: 9.3980e-04\n",
      "Epoch 264/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.9873e-04 - val_loss: 7.3904e-04\n",
      "Epoch 265/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.5901e-04 - val_loss: 6.7792e-04\n",
      "Epoch 266/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.3245e-04 - val_loss: 7.5949e-04\n",
      "Epoch 267/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.9219e-04 - val_loss: 6.3259e-04\n",
      "Epoch 268/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.0131e-04 - val_loss: 7.0593e-04\n",
      "Epoch 269/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.8586e-04 - val_loss: 6.7072e-04\n",
      "Epoch 270/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.2477e-04 - val_loss: 6.9543e-04\n",
      "Epoch 271/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.0557e-04 - val_loss: 7.5874e-04\n",
      "Epoch 272/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.2833e-04 - val_loss: 6.8582e-04\n",
      "Epoch 273/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.3220e-04 - val_loss: 6.3434e-04\n",
      "Epoch 274/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.7496e-04 - val_loss: 7.5881e-04\n",
      "Epoch 275/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0010 - val_loss: 6.7769e-04\n",
      "Epoch 276/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.5820e-04 - val_loss: 7.5968e-04\n",
      "Epoch 277/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.2054e-04 - val_loss: 6.5774e-04\n",
      "Epoch 278/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.7950e-04 - val_loss: 8.1880e-04\n",
      "Epoch 279/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 4s 68ms/step - loss: 9.1334e-04 - val_loss: 6.6712e-04\n",
      "Epoch 280/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.3862e-04 - val_loss: 7.9076e-04\n",
      "Epoch 281/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.2715e-04 - val_loss: 6.6025e-04\n",
      "Epoch 282/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.1626e-04 - val_loss: 7.2312e-04\n",
      "Epoch 283/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.8457e-04 - val_loss: 6.8801e-04\n",
      "Epoch 284/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.8592e-04 - val_loss: 6.4118e-04\n",
      "Epoch 285/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.8713e-04 - val_loss: 7.0745e-04\n",
      "Epoch 286/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.9896e-04 - val_loss: 6.6512e-04\n",
      "Epoch 287/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.9542e-04 - val_loss: 6.9663e-04\n",
      "Epoch 288/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.9748e-04 - val_loss: 6.8403e-04\n",
      "Epoch 289/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.0502e-04 - val_loss: 7.3360e-04\n",
      "Epoch 290/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.8140e-04 - val_loss: 5.8367e-04\n",
      "Epoch 291/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.0800e-04 - val_loss: 6.2400e-04\n",
      "Epoch 292/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.7397e-04 - val_loss: 7.9156e-04\n",
      "Epoch 293/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.3390e-04 - val_loss: 7.6371e-04\n",
      "Epoch 294/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.3663e-04 - val_loss: 7.6518e-04\n",
      "Epoch 295/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.8353e-04 - val_loss: 7.5337e-04\n",
      "Epoch 296/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.0288e-04 - val_loss: 6.2601e-04\n",
      "Epoch 297/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.7219e-04 - val_loss: 6.1910e-04\n",
      "Epoch 298/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.9444e-04 - val_loss: 7.4988e-04\n",
      "Epoch 299/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.0809e-04 - val_loss: 7.0922e-04\n",
      "Epoch 300/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.9720e-04 - val_loss: 6.9617e-04\n",
      "Epoch 301/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.9432e-04 - val_loss: 7.6841e-04\n",
      "Epoch 302/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.7169e-04 - val_loss: 6.1296e-04\n",
      "Epoch 303/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.8738e-04 - val_loss: 7.4094e-04\n",
      "Epoch 304/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.0637e-04 - val_loss: 7.1456e-04\n",
      "Epoch 305/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.6598e-04 - val_loss: 6.1219e-04\n",
      "Epoch 306/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.8337e-04 - val_loss: 7.1402e-04\n",
      "Epoch 307/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.4834e-04 - val_loss: 6.9860e-04\n",
      "Epoch 308/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.2775e-04 - val_loss: 6.6403e-04\n",
      "Epoch 309/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.7972e-04 - val_loss: 8.8754e-04\n",
      "Epoch 310/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.1899e-04 - val_loss: 6.7741e-04\n",
      "Epoch 311/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.0094e-04 - val_loss: 7.1133e-04\n",
      "Epoch 312/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.6796e-04 - val_loss: 7.0275e-04\n",
      "Epoch 313/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.2291e-04 - val_loss: 7.1509e-04\n",
      "Epoch 314/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.1081e-04 - val_loss: 7.4668e-04\n",
      "Epoch 315/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.6136e-04 - val_loss: 7.0176e-04\n",
      "Epoch 316/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.5385e-04 - val_loss: 6.5157e-04\n",
      "Epoch 317/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.1539e-04 - val_loss: 6.8621e-04\n",
      "Epoch 318/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.0127e-04 - val_loss: 6.4985e-04\n",
      "Epoch 319/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.6924e-04 - val_loss: 6.5118e-04\n",
      "Epoch 320/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.5369e-04 - val_loss: 7.0907e-04\n",
      "Epoch 321/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.2380e-04 - val_loss: 7.2435e-04\n",
      "Epoch 322/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.6587e-04 - val_loss: 6.8441e-04\n",
      "Epoch 323/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.3409e-04 - val_loss: 7.6489e-04\n",
      "Epoch 324/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.9948e-04 - val_loss: 6.6528e-04\n",
      "Epoch 325/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.7735e-04 - val_loss: 6.5751e-04\n",
      "Epoch 326/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.9300e-04 - val_loss: 7.2024e-04\n",
      "Epoch 327/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.9148e-04 - val_loss: 6.0700e-04\n",
      "Epoch 328/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.8955e-04 - val_loss: 7.0309e-04\n",
      "Epoch 329/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.7021e-04 - val_loss: 8.2079e-04\n",
      "Epoch 330/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.9259e-04 - val_loss: 6.9199e-04\n",
      "Epoch 331/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.5122e-04 - val_loss: 6.5217e-04\n",
      "Epoch 332/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.4812e-04 - val_loss: 6.3369e-04\n",
      "Epoch 333/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.3657e-04 - val_loss: 7.3732e-04\n",
      "Epoch 334/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.6074e-04 - val_loss: 6.6168e-04\n",
      "Epoch 335/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.8925e-04 - val_loss: 6.7966e-04\n",
      "Epoch 336/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.9288e-04 - val_loss: 7.3121e-04\n",
      "Epoch 337/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.4474e-04 - val_loss: 6.1712e-04\n",
      "Epoch 338/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.7482e-04 - val_loss: 6.8232e-04\n",
      "Epoch 339/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.5257e-04 - val_loss: 6.8548e-04\n",
      "Epoch 340/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.3111e-04 - val_loss: 6.2658e-04\n",
      "Epoch 341/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.6563e-04 - val_loss: 6.5914e-04\n",
      "Epoch 342/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.7960e-04 - val_loss: 6.0699e-04\n",
      "Epoch 343/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.6046e-04 - val_loss: 6.1842e-04\n",
      "Epoch 344/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.7024e-04 - val_loss: 7.0531e-04\n",
      "Epoch 345/1000\n",
      "53/53 [==============================] - 4s 71ms/step - loss: 8.0956e-04 - val_loss: 6.3867e-04\n",
      "Epoch 346/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.9200e-04 - val_loss: 6.9107e-04\n",
      "Epoch 347/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.5842e-04 - val_loss: 8.3605e-04\n",
      "Epoch 348/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.1247e-04 - val_loss: 6.0649e-04\n",
      "Epoch 349/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.4574e-04 - val_loss: 7.0847e-04\n",
      "Epoch 350/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.6296e-04 - val_loss: 6.4498e-04\n",
      "Epoch 351/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.6785e-04 - val_loss: 8.1017e-04\n",
      "Epoch 352/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.2912e-04 - val_loss: 6.5110e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.9515e-04 - val_loss: 5.6284e-04\n",
      "Epoch 354/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.4417e-04 - val_loss: 6.8463e-04\n",
      "Epoch 355/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.0200e-04 - val_loss: 7.0177e-04\n",
      "Epoch 356/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.4124e-04 - val_loss: 8.0272e-04\n",
      "Epoch 357/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.4219e-04 - val_loss: 6.1590e-04\n",
      "Epoch 358/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.3580e-04 - val_loss: 6.0615e-04\n",
      "Epoch 359/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.4573e-04 - val_loss: 5.9761e-04\n",
      "Epoch 360/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.0176e-04 - val_loss: 6.5818e-04\n",
      "Epoch 361/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.5826e-04 - val_loss: 6.9396e-04\n",
      "Epoch 362/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.4463e-04 - val_loss: 6.2243e-04\n",
      "Epoch 363/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.8770e-04 - val_loss: 7.5066e-04\n",
      "Epoch 364/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.9016e-04 - val_loss: 6.1525e-04\n",
      "Epoch 365/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.6370e-04 - val_loss: 5.8536e-04\n",
      "Epoch 366/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.9792e-04 - val_loss: 7.8060e-04\n",
      "Epoch 367/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.4347e-04 - val_loss: 7.1555e-04\n",
      "Epoch 368/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.7058e-04 - val_loss: 6.6140e-04\n",
      "Epoch 369/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.9175e-04 - val_loss: 6.2583e-04\n",
      "Epoch 370/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.9846e-04 - val_loss: 7.8971e-04\n",
      "Epoch 371/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.4834e-04 - val_loss: 7.3642e-04\n",
      "Epoch 372/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.3125e-04 - val_loss: 6.5818e-04\n",
      "Epoch 373/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.8441e-04 - val_loss: 6.6506e-04\n",
      "Epoch 374/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.4577e-04 - val_loss: 6.6382e-04\n",
      "Epoch 375/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.0043e-04 - val_loss: 7.7012e-04\n",
      "Epoch 376/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.4502e-04 - val_loss: 6.9092e-04\n",
      "Epoch 377/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.2053e-04 - val_loss: 6.6547e-04\n",
      "Epoch 378/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.2754e-04 - val_loss: 7.3038e-04\n",
      "Epoch 379/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.4685e-04 - val_loss: 6.5528e-04\n",
      "Epoch 380/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.9055e-04 - val_loss: 5.4779e-04\n",
      "Epoch 381/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.3615e-04 - val_loss: 7.4435e-04\n",
      "Epoch 382/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7748e-04 - val_loss: 7.4643e-04\n",
      "Epoch 383/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.4372e-04 - val_loss: 7.3607e-04\n",
      "Epoch 384/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.4117e-04 - val_loss: 5.6379e-04\n",
      "Epoch 385/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.0980e-04 - val_loss: 5.8472e-04\n",
      "Epoch 386/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.7601e-04 - val_loss: 5.6949e-04\n",
      "Epoch 387/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.9893e-04 - val_loss: 6.4947e-04\n",
      "Epoch 388/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.0941e-04 - val_loss: 6.2905e-04\n",
      "Epoch 389/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.9485e-04 - val_loss: 6.2070e-04\n",
      "Epoch 390/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.9427e-04 - val_loss: 5.6238e-04\n",
      "Epoch 391/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.0611e-04 - val_loss: 5.6471e-04\n",
      "Epoch 392/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.6061e-04 - val_loss: 7.0509e-04\n",
      "Epoch 393/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.1042e-04 - val_loss: 6.9644e-04\n",
      "Epoch 394/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.8915e-04 - val_loss: 5.7325e-04\n",
      "Epoch 395/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7152e-04 - val_loss: 6.5184e-04\n",
      "Epoch 396/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.0673e-04 - val_loss: 6.5983e-04\n",
      "Epoch 397/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.8987e-04 - val_loss: 5.6996e-04\n",
      "Epoch 398/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.2260e-04 - val_loss: 5.6771e-04\n",
      "Epoch 399/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.6333e-04 - val_loss: 5.4263e-04\n",
      "Epoch 400/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.4384e-04 - val_loss: 6.2840e-04\n",
      "Epoch 401/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7065e-04 - val_loss: 6.5290e-04\n",
      "Epoch 402/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7079e-04 - val_loss: 6.1831e-04\n",
      "Epoch 403/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7770e-04 - val_loss: 6.0456e-04\n",
      "Epoch 404/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.7331e-04 - val_loss: 6.1826e-04\n",
      "Epoch 405/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.2261e-04 - val_loss: 7.0441e-04\n",
      "Epoch 406/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.9755e-04 - val_loss: 6.6056e-04\n",
      "Epoch 407/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.3142e-04 - val_loss: 6.1602e-04\n",
      "Epoch 408/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.3909e-04 - val_loss: 5.7665e-04\n",
      "Epoch 409/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.3014e-04 - val_loss: 5.6268e-04\n",
      "Epoch 410/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.8693e-04 - val_loss: 6.4856e-04\n",
      "Epoch 411/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.8516e-04 - val_loss: 6.0378e-04\n",
      "Epoch 412/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.6054e-04 - val_loss: 6.2174e-04\n",
      "Epoch 413/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.6794e-04 - val_loss: 5.8214e-04\n",
      "Epoch 414/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.5663e-04 - val_loss: 5.7260e-04\n",
      "Epoch 415/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.0059e-04 - val_loss: 5.5373e-04\n",
      "Epoch 416/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.8533e-04 - val_loss: 6.4286e-04\n",
      "Epoch 417/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.0598e-04 - val_loss: 6.7228e-04\n",
      "Epoch 418/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.1701e-04 - val_loss: 6.8572e-04\n",
      "Epoch 419/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.1763e-04 - val_loss: 7.2735e-04\n",
      "Epoch 420/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.2837e-04 - val_loss: 6.1963e-04\n",
      "Epoch 421/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.8174e-04 - val_loss: 7.9921e-04\n",
      "Epoch 422/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.3620e-04 - val_loss: 8.4058e-04\n",
      "Epoch 423/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.4969e-04 - val_loss: 6.2730e-04\n",
      "Epoch 424/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.3879e-04 - val_loss: 5.7919e-04\n",
      "Epoch 425/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.7467e-04 - val_loss: 6.0467e-04\n",
      "Epoch 426/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.8610e-04 - val_loss: 5.8872e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.8035e-04 - val_loss: 6.3196e-04\n",
      "Epoch 428/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.4838e-04 - val_loss: 7.2292e-04\n",
      "Epoch 429/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.0907e-04 - val_loss: 7.1472e-04\n",
      "Epoch 430/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.0457e-04 - val_loss: 6.0322e-04\n",
      "Epoch 431/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7524e-04 - val_loss: 6.4371e-04\n",
      "Epoch 432/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.2891e-04 - val_loss: 5.9892e-04\n",
      "Epoch 433/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.2489e-04 - val_loss: 6.2086e-04\n",
      "Epoch 434/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.7529e-04 - val_loss: 7.1165e-04\n",
      "Epoch 435/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.3376e-04 - val_loss: 6.4550e-04\n",
      "Epoch 436/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.0427e-04 - val_loss: 6.8038e-04\n",
      "Epoch 437/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.5407e-04 - val_loss: 6.5537e-04\n",
      "Epoch 438/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.2046e-04 - val_loss: 5.9066e-04\n",
      "Epoch 439/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.1382e-04 - val_loss: 5.9840e-04\n",
      "Epoch 440/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.9249e-04 - val_loss: 5.7866e-04\n",
      "Epoch 441/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.6859e-04 - val_loss: 5.4333e-04\n",
      "Epoch 442/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.7846e-04 - val_loss: 6.4280e-04\n",
      "Epoch 443/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.8297e-04 - val_loss: 6.5178e-04\n",
      "Epoch 444/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7332e-04 - val_loss: 5.8233e-04\n",
      "Epoch 445/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.9315e-04 - val_loss: 6.8094e-04\n",
      "Epoch 446/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7935e-04 - val_loss: 6.4200e-04\n",
      "Epoch 447/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7564e-04 - val_loss: 6.2033e-04\n",
      "Epoch 448/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.4928e-04 - val_loss: 6.0588e-04\n",
      "Epoch 449/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.6348e-04 - val_loss: 6.7495e-04\n",
      "Epoch 450/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0931e-04 - val_loss: 5.3588e-04\n",
      "Epoch 451/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.3913e-04 - val_loss: 5.6605e-04\n",
      "Epoch 452/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.9409e-04 - val_loss: 5.7755e-04\n",
      "Epoch 453/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.1140e-04 - val_loss: 5.9036e-04\n",
      "Epoch 454/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7564e-04 - val_loss: 7.2559e-04\n",
      "Epoch 455/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.6300e-04 - val_loss: 5.8641e-04\n",
      "Epoch 456/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7988e-04 - val_loss: 6.7169e-04\n",
      "Epoch 457/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.7461e-04 - val_loss: 5.9014e-04\n",
      "Epoch 458/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.3683e-04 - val_loss: 6.0106e-04\n",
      "Epoch 459/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.3560e-04 - val_loss: 5.0265e-04\n",
      "Epoch 460/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.2703e-04 - val_loss: 6.7225e-04\n",
      "Epoch 461/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0577e-04 - val_loss: 5.4580e-04\n",
      "Epoch 462/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.7417e-04 - val_loss: 0.0011\n",
      "Epoch 463/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 9.4870e-04 - val_loss: 9.3603e-04\n",
      "Epoch 464/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.9209e-04 - val_loss: 7.1480e-04\n",
      "Epoch 465/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.9265e-04 - val_loss: 7.5018e-04\n",
      "Epoch 466/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.2749e-04 - val_loss: 6.2071e-04\n",
      "Epoch 467/1000\n",
      "53/53 [==============================] - 4s 71ms/step - loss: 7.8480e-04 - val_loss: 7.2758e-04\n",
      "Epoch 468/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.4681e-04 - val_loss: 6.6851e-04\n",
      "Epoch 469/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.4397e-04 - val_loss: 5.8452e-04\n",
      "Epoch 470/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.3287e-04 - val_loss: 5.6973e-04\n",
      "Epoch 471/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.1700e-04 - val_loss: 7.0557e-04\n",
      "Epoch 472/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1692e-04 - val_loss: 5.4360e-04\n",
      "Epoch 473/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.3132e-04 - val_loss: 5.6015e-04\n",
      "Epoch 474/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.3338e-04 - val_loss: 5.8519e-04\n",
      "Epoch 475/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.6940e-04 - val_loss: 7.1131e-04\n",
      "Epoch 476/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0542e-04 - val_loss: 6.7890e-04\n",
      "Epoch 477/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 8.1517e-04 - val_loss: 5.9846e-04\n",
      "Epoch 478/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.2430e-04 - val_loss: 6.4811e-04\n",
      "Epoch 479/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.4863e-04 - val_loss: 6.1985e-04\n",
      "Epoch 480/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.2793e-04 - val_loss: 6.0534e-04\n",
      "Epoch 481/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7117e-04 - val_loss: 5.4534e-04\n",
      "Epoch 482/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.2680e-04 - val_loss: 6.1299e-04\n",
      "Epoch 483/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.5747e-04 - val_loss: 5.7628e-04\n",
      "Epoch 484/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.8694e-04 - val_loss: 5.4920e-04\n",
      "Epoch 485/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7385e-04 - val_loss: 6.1887e-04\n",
      "Epoch 486/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.1992e-04 - val_loss: 5.7786e-04\n",
      "Epoch 487/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.7047e-04 - val_loss: 0.0027\n",
      "Epoch 488/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0010 - val_loss: 8.7759e-04\n",
      "Epoch 489/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.3602e-04 - val_loss: 6.2987e-04\n",
      "Epoch 490/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.7746e-04 - val_loss: 6.5392e-04\n",
      "Epoch 491/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.9545e-04 - val_loss: 6.0379e-04\n",
      "Epoch 492/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.9300e-04 - val_loss: 5.6731e-04\n",
      "Epoch 493/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.4239e-04 - val_loss: 6.4652e-04\n",
      "Epoch 494/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.9719e-04 - val_loss: 5.4199e-04\n",
      "Epoch 495/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.7499e-04 - val_loss: 6.8585e-04\n",
      "Epoch 496/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.3045e-04 - val_loss: 6.8554e-04\n",
      "Epoch 497/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.2401e-04 - val_loss: 6.2237e-04\n",
      "Epoch 498/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7237e-04 - val_loss: 5.6607e-04\n",
      "Epoch 499/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.8763e-04 - val_loss: 5.5324e-04\n",
      "Epoch 500/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.8123e-04 - val_loss: 6.8367e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 501/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.2728e-04 - val_loss: 6.3790e-04\n",
      "Epoch 502/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.6106e-04 - val_loss: 5.6911e-04\n",
      "Epoch 503/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.2213e-04 - val_loss: 5.5979e-04\n",
      "Epoch 504/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.4714e-04 - val_loss: 5.2264e-04\n",
      "Epoch 505/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7615e-04 - val_loss: 5.6277e-04\n",
      "Epoch 506/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.6919e-04 - val_loss: 6.4569e-04\n",
      "Epoch 507/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.5738e-04 - val_loss: 6.1442e-04\n",
      "Epoch 508/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0291e-04 - val_loss: 6.7914e-04\n",
      "Epoch 509/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.5598e-04 - val_loss: 5.7741e-04\n",
      "Epoch 510/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7124e-04 - val_loss: 6.5415e-04\n",
      "Epoch 511/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.5499e-04 - val_loss: 5.9648e-04\n",
      "Epoch 512/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.6071e-04 - val_loss: 5.3466e-04\n",
      "Epoch 513/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.9853e-04 - val_loss: 6.1295e-04\n",
      "Epoch 514/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.2641e-04 - val_loss: 6.5654e-04\n",
      "Epoch 515/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.7665e-04 - val_loss: 6.9167e-04\n",
      "Epoch 516/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6853e-04 - val_loss: 6.0484e-04\n",
      "Epoch 517/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.6370e-04 - val_loss: 5.4259e-04\n",
      "Epoch 518/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.3875e-04 - val_loss: 5.5177e-04\n",
      "Epoch 519/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.9054e-04 - val_loss: 7.1208e-04\n",
      "Epoch 520/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.3418e-04 - val_loss: 5.7614e-04\n",
      "Epoch 521/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.2177e-04 - val_loss: 5.6781e-04\n",
      "Epoch 522/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1730e-04 - val_loss: 5.7987e-04\n",
      "Epoch 523/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.8208e-04 - val_loss: 5.9660e-04\n",
      "Epoch 524/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.7411e-04 - val_loss: 5.3784e-04\n",
      "Epoch 525/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1416e-04 - val_loss: 5.3568e-04\n",
      "Epoch 526/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7613e-04 - val_loss: 5.6669e-04\n",
      "Epoch 527/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7508e-04 - val_loss: 5.9697e-04\n",
      "Epoch 528/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.4900e-04 - val_loss: 5.7791e-04\n",
      "Epoch 529/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.6262e-04 - val_loss: 5.1819e-04\n",
      "Epoch 530/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.9165e-04 - val_loss: 6.0509e-04\n",
      "Epoch 531/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1105e-04 - val_loss: 5.8243e-04\n",
      "Epoch 532/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.4180e-04 - val_loss: 5.1710e-04\n",
      "Epoch 533/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.9181e-04 - val_loss: 6.4158e-04\n",
      "Epoch 534/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.4051e-04 - val_loss: 5.2290e-04\n",
      "Epoch 535/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.8396e-04 - val_loss: 6.0368e-04\n",
      "Epoch 536/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.5379e-04 - val_loss: 5.4656e-04\n",
      "Epoch 537/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0766e-04 - val_loss: 5.6022e-04\n",
      "Epoch 538/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1999e-04 - val_loss: 6.1110e-04\n",
      "Epoch 539/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1610e-04 - val_loss: 6.5184e-04\n",
      "Epoch 540/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.8962e-04 - val_loss: 6.8679e-04\n",
      "Epoch 541/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.3491e-04 - val_loss: 5.5631e-04\n",
      "Epoch 542/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.7117e-04 - val_loss: 5.6414e-04\n",
      "Epoch 543/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1032e-04 - val_loss: 5.8734e-04\n",
      "Epoch 544/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.4709e-04 - val_loss: 5.9295e-04\n",
      "Epoch 545/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.1106e-04 - val_loss: 5.7102e-04\n",
      "Epoch 546/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.2113e-04 - val_loss: 6.0761e-04\n",
      "Epoch 547/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.4784e-04 - val_loss: 6.1450e-04\n",
      "Epoch 548/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.5908e-04 - val_loss: 8.1958e-04\n",
      "Epoch 549/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9623e-04 - val_loss: 5.2150e-04\n",
      "Epoch 550/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.3049e-04 - val_loss: 5.4536e-04\n",
      "Epoch 551/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.0335e-04 - val_loss: 5.6995e-04\n",
      "Epoch 552/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1599e-04 - val_loss: 6.4290e-04\n",
      "Epoch 553/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0607e-04 - val_loss: 5.8912e-04\n",
      "Epoch 554/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.9732e-04 - val_loss: 6.1718e-04\n",
      "Epoch 555/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.2882e-04 - val_loss: 5.8975e-04\n",
      "Epoch 556/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.5906e-04 - val_loss: 6.0841e-04\n",
      "Epoch 557/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.4360e-04 - val_loss: 6.0359e-04\n",
      "Epoch 558/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.6910e-04 - val_loss: 6.8938e-04\n",
      "Epoch 559/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0270e-04 - val_loss: 5.9957e-04\n",
      "Epoch 560/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.9822e-04 - val_loss: 5.7524e-04\n",
      "Epoch 561/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1814e-04 - val_loss: 5.5843e-04\n",
      "Epoch 562/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.8792e-04 - val_loss: 5.9313e-04\n",
      "Epoch 563/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1383e-04 - val_loss: 6.4131e-04\n",
      "Epoch 564/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.9403e-04 - val_loss: 6.3524e-04\n",
      "Epoch 565/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.4427e-04 - val_loss: 5.6724e-04\n",
      "Epoch 566/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.3102e-04 - val_loss: 6.5723e-04\n",
      "Epoch 567/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.3624e-04 - val_loss: 7.3788e-04\n",
      "Epoch 568/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.3159e-04 - val_loss: 5.7723e-04\n",
      "Epoch 569/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1970e-04 - val_loss: 6.4395e-04\n",
      "Epoch 570/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8979e-04 - val_loss: 5.5816e-04\n",
      "Epoch 571/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7864e-04 - val_loss: 6.4042e-04\n",
      "Epoch 572/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.3223e-04 - val_loss: 5.6930e-04\n",
      "Epoch 573/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.3038e-04 - val_loss: 5.5974e-04\n",
      "Epoch 574/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.3658e-04 - val_loss: 5.9600e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 575/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.9415e-04 - val_loss: 6.3891e-04\n",
      "Epoch 576/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.7524e-04 - val_loss: 6.2400e-04\n",
      "Epoch 577/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0765e-04 - val_loss: 5.8242e-04\n",
      "Epoch 578/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1136e-04 - val_loss: 5.6745e-04\n",
      "Epoch 579/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1158e-04 - val_loss: 6.8549e-04\n",
      "Epoch 580/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.9701e-04 - val_loss: 5.3874e-04\n",
      "Epoch 581/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.5565e-04 - val_loss: 5.9956e-04\n",
      "Epoch 582/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.8181e-04 - val_loss: 7.2961e-04\n",
      "Epoch 583/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.8064e-04 - val_loss: 5.9395e-04\n",
      "Epoch 584/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7910e-04 - val_loss: 5.9749e-04\n",
      "Epoch 585/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0906e-04 - val_loss: 6.2535e-04\n",
      "Epoch 586/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.3805e-04 - val_loss: 5.9165e-04\n",
      "Epoch 587/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.4536e-04 - val_loss: 6.0031e-04\n",
      "Epoch 588/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.9592e-04 - val_loss: 6.3203e-04\n",
      "Epoch 589/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.4296e-04 - val_loss: 6.3532e-04\n",
      "Epoch 590/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0223e-04 - val_loss: 7.2015e-04\n",
      "Epoch 591/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0281e-04 - val_loss: 5.4076e-04\n",
      "Epoch 592/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7901e-04 - val_loss: 6.1228e-04\n",
      "Epoch 593/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0303e-04 - val_loss: 5.5894e-04\n",
      "Epoch 594/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.5872e-04 - val_loss: 6.2191e-04\n",
      "Epoch 595/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.7711e-04 - val_loss: 5.7014e-04\n",
      "Epoch 596/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.6916e-04 - val_loss: 6.9195e-04\n",
      "Epoch 597/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.2839e-04 - val_loss: 6.8272e-04\n",
      "Epoch 598/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9339e-04 - val_loss: 6.2082e-04\n",
      "Epoch 599/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9940e-04 - val_loss: 5.7726e-04\n",
      "Epoch 600/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9693e-04 - val_loss: 5.3972e-04\n",
      "Epoch 601/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.1470e-04 - val_loss: 5.9546e-04\n",
      "Epoch 602/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.8231e-04 - val_loss: 5.4579e-04\n",
      "Epoch 603/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9521e-04 - val_loss: 5.9335e-04\n",
      "Epoch 604/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.2607e-04 - val_loss: 5.5715e-04\n",
      "Epoch 605/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6591e-04 - val_loss: 5.8628e-04\n",
      "Epoch 606/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.2071e-04 - val_loss: 6.1930e-04\n",
      "Epoch 607/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.9835e-04 - val_loss: 5.5571e-04\n",
      "Epoch 608/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1365e-04 - val_loss: 5.4257e-04\n",
      "Epoch 609/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.9139e-04 - val_loss: 5.6692e-04\n",
      "Epoch 610/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.9179e-04 - val_loss: 5.7052e-04\n",
      "Epoch 611/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.2451e-04 - val_loss: 6.2090e-04\n",
      "Epoch 612/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0289e-04 - val_loss: 5.3998e-04\n",
      "Epoch 613/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0735e-04 - val_loss: 5.6893e-04\n",
      "Epoch 614/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0268e-04 - val_loss: 5.2311e-04\n",
      "Epoch 615/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8329e-04 - val_loss: 5.9091e-04\n",
      "Epoch 616/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.5967e-04 - val_loss: 6.2148e-04\n",
      "Epoch 617/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.2472e-04 - val_loss: 5.8424e-04\n",
      "Epoch 618/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8774e-04 - val_loss: 5.5706e-04\n",
      "Epoch 619/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.4542e-04 - val_loss: 7.4759e-04\n",
      "Epoch 620/1000\n",
      "53/53 [==============================] - 4s 71ms/step - loss: 6.6822e-04 - val_loss: 5.4979e-04\n",
      "Epoch 621/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.5077e-04 - val_loss: 6.0973e-04\n",
      "Epoch 622/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6566e-04 - val_loss: 6.0260e-04\n",
      "Epoch 623/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8978e-04 - val_loss: 5.8936e-04\n",
      "Epoch 624/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5676e-04 - val_loss: 5.4982e-04\n",
      "Epoch 625/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.3776e-04 - val_loss: 5.2439e-04\n",
      "Epoch 626/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0181e-04 - val_loss: 5.6346e-04\n",
      "Epoch 627/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.1980e-04 - val_loss: 5.3571e-04\n",
      "Epoch 628/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.7375e-04 - val_loss: 5.2294e-04\n",
      "Epoch 629/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.2474e-04 - val_loss: 6.6675e-04\n",
      "Epoch 630/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.5056e-04 - val_loss: 5.7044e-04\n",
      "Epoch 631/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.5132e-04 - val_loss: 6.9599e-04\n",
      "Epoch 632/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.3207e-04 - val_loss: 6.4771e-04\n",
      "Epoch 633/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8419e-04 - val_loss: 5.6339e-04\n",
      "Epoch 634/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.3895e-04 - val_loss: 6.2995e-04\n",
      "Epoch 635/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.1130e-04 - val_loss: 5.3576e-04\n",
      "Epoch 636/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6678e-04 - val_loss: 5.5780e-04\n",
      "Epoch 637/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0496e-04 - val_loss: 5.3598e-04\n",
      "Epoch 638/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0681e-04 - val_loss: 7.1330e-04\n",
      "Epoch 639/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7088e-04 - val_loss: 5.9186e-04\n",
      "Epoch 640/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.8274e-04 - val_loss: 5.4598e-04\n",
      "Epoch 641/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.4681e-04 - val_loss: 5.2391e-04\n",
      "Epoch 642/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8550e-04 - val_loss: 5.2583e-04\n",
      "Epoch 643/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8195e-04 - val_loss: 5.2555e-04\n",
      "Epoch 644/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.3189e-04 - val_loss: 5.4128e-04\n",
      "Epoch 645/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.5874e-04 - val_loss: 6.0213e-04\n",
      "Epoch 646/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8964e-04 - val_loss: 5.5743e-04\n",
      "Epoch 647/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.4969e-04 - val_loss: 6.5707e-04\n",
      "Epoch 648/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.9392e-04 - val_loss: 5.6380e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 649/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.5929e-04 - val_loss: 5.6380e-04\n",
      "Epoch 650/1000\n",
      "53/53 [==============================] - 4s 71ms/step - loss: 6.9077e-04 - val_loss: 5.5227e-04\n",
      "Epoch 651/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5503e-04 - val_loss: 6.3467e-04\n",
      "Epoch 652/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.7530e-04 - val_loss: 5.7216e-04\n",
      "Epoch 653/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1551e-04 - val_loss: 5.8869e-04\n",
      "Epoch 654/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.7176e-04 - val_loss: 6.0169e-04\n",
      "Epoch 655/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.9762e-04 - val_loss: 7.3981e-04\n",
      "Epoch 656/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.8732e-04 - val_loss: 6.2163e-04\n",
      "Epoch 657/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.2046e-04 - val_loss: 5.3046e-04\n",
      "Epoch 658/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0290e-04 - val_loss: 5.7656e-04\n",
      "Epoch 659/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1922e-04 - val_loss: 5.8267e-04\n",
      "Epoch 660/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0195e-04 - val_loss: 6.2170e-04\n",
      "Epoch 661/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.4104e-04 - val_loss: 5.8641e-04\n",
      "Epoch 662/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.3792e-04 - val_loss: 5.8677e-04\n",
      "Epoch 663/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0419e-04 - val_loss: 5.1540e-04\n",
      "Epoch 664/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5858e-04 - val_loss: 6.1057e-04\n",
      "Epoch 665/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.9714e-04 - val_loss: 5.1578e-04\n",
      "Epoch 666/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.1186e-04 - val_loss: 5.3221e-04\n",
      "Epoch 667/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.3487e-04 - val_loss: 6.1053e-04\n",
      "Epoch 668/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5644e-04 - val_loss: 5.3612e-04\n",
      "Epoch 669/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7218e-04 - val_loss: 5.5229e-04\n",
      "Epoch 670/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.4236e-04 - val_loss: 5.4392e-04\n",
      "Epoch 671/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0919e-04 - val_loss: 5.5743e-04\n",
      "Epoch 672/1000\n",
      "53/53 [==============================] - 4s 71ms/step - loss: 6.8389e-04 - val_loss: 5.6592e-04\n",
      "Epoch 673/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6872e-04 - val_loss: 7.1384e-04\n",
      "Epoch 674/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.2804e-04 - val_loss: 5.6495e-04\n",
      "Epoch 675/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8328e-04 - val_loss: 5.2138e-04\n",
      "Epoch 676/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8148e-04 - val_loss: 6.1117e-04\n",
      "Epoch 677/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0589e-04 - val_loss: 5.4675e-04\n",
      "Epoch 678/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7318e-04 - val_loss: 5.8262e-04\n",
      "Epoch 679/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.9404e-04 - val_loss: 5.3765e-04\n",
      "Epoch 680/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.3894e-04 - val_loss: 5.7997e-04\n",
      "Epoch 681/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6415e-04 - val_loss: 6.0220e-04\n",
      "Epoch 682/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8353e-04 - val_loss: 6.1588e-04\n",
      "Epoch 683/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.2165e-04 - val_loss: 6.1243e-04\n",
      "Epoch 684/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8091e-04 - val_loss: 5.3537e-04\n",
      "Epoch 685/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.3206e-04 - val_loss: 5.3658e-04\n",
      "Epoch 686/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.4886e-04 - val_loss: 5.4815e-04\n",
      "Epoch 687/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.3027e-04 - val_loss: 5.2727e-04\n",
      "Epoch 688/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.2572e-04 - val_loss: 5.6389e-04\n",
      "Epoch 689/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.5106e-04 - val_loss: 5.3626e-04\n",
      "Epoch 690/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.1656e-04 - val_loss: 5.1467e-04\n",
      "Epoch 691/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.5508e-04 - val_loss: 5.5646e-04\n",
      "Epoch 692/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.3538e-04 - val_loss: 5.4734e-04\n",
      "Epoch 693/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6871e-04 - val_loss: 5.7961e-04\n",
      "Epoch 694/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7101e-04 - val_loss: 5.5599e-04\n",
      "Epoch 695/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1841e-04 - val_loss: 5.6284e-04\n",
      "Epoch 696/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5087e-04 - val_loss: 5.9541e-04\n",
      "Epoch 697/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8562e-04 - val_loss: 6.1669e-04\n",
      "Epoch 698/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6067e-04 - val_loss: 5.9718e-04\n",
      "Epoch 699/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.7819e-04 - val_loss: 5.7999e-04\n",
      "Epoch 700/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.9929e-04 - val_loss: 5.2716e-04\n",
      "Epoch 701/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.9123e-04 - val_loss: 5.2689e-04\n",
      "Epoch 702/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.7360e-04 - val_loss: 5.6523e-04\n",
      "Epoch 703/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.3640e-04 - val_loss: 5.7103e-04\n",
      "Epoch 704/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1590e-04 - val_loss: 6.3075e-04\n",
      "Epoch 705/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.8480e-04 - val_loss: 5.8627e-04\n",
      "Epoch 706/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.2154e-04 - val_loss: 5.5530e-04\n",
      "Epoch 707/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.1821e-04 - val_loss: 6.6125e-04\n",
      "Epoch 708/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.9459e-04 - val_loss: 6.0747e-04\n",
      "Epoch 709/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5967e-04 - val_loss: 5.2657e-04\n",
      "Epoch 710/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.5647e-04 - val_loss: 5.3176e-04\n",
      "Epoch 711/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0351e-04 - val_loss: 5.1637e-04\n",
      "Epoch 712/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.2093e-04 - val_loss: 5.2446e-04\n",
      "Epoch 713/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0785e-04 - val_loss: 5.6900e-04\n",
      "Epoch 714/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.2815e-04 - val_loss: 5.4424e-04\n",
      "Epoch 715/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5377e-04 - val_loss: 5.3602e-04\n",
      "Epoch 716/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.1757e-04 - val_loss: 6.1150e-04\n",
      "Epoch 717/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7228e-04 - val_loss: 5.8616e-04\n",
      "Epoch 718/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.3502e-04 - val_loss: 5.9815e-04\n",
      "Epoch 719/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.4697e-04 - val_loss: 5.6018e-04\n",
      "Epoch 720/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.2845e-04 - val_loss: 5.9254e-04\n",
      "Epoch 721/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6232e-04 - val_loss: 5.0442e-04\n",
      "Epoch 722/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.4068e-04 - val_loss: 6.3151e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 723/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.1799e-04 - val_loss: 5.7274e-04\n",
      "Epoch 724/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.5900e-04 - val_loss: 5.2722e-04\n",
      "Epoch 725/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5604e-04 - val_loss: 6.8338e-04\n",
      "Epoch 726/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.3498e-04 - val_loss: 5.6630e-04\n",
      "Epoch 727/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.1523e-04 - val_loss: 5.5298e-04\n",
      "Epoch 728/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6049e-04 - val_loss: 7.0965e-04\n",
      "Epoch 729/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8021e-04 - val_loss: 5.7830e-04\n",
      "Epoch 730/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.5394e-04 - val_loss: 5.4651e-04\n",
      "Epoch 731/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.3542e-04 - val_loss: 6.0984e-04\n",
      "Epoch 732/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7018e-04 - val_loss: 5.6549e-04\n",
      "Epoch 733/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6552e-04 - val_loss: 5.3924e-04\n",
      "Epoch 734/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8655e-04 - val_loss: 5.7468e-04\n",
      "Epoch 735/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8027e-04 - val_loss: 5.4283e-04\n",
      "Epoch 736/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0559e-04 - val_loss: 6.1556e-04\n",
      "Epoch 737/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.5311e-04 - val_loss: 5.7712e-04\n",
      "Epoch 738/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.4767e-04 - val_loss: 5.3288e-04\n",
      "Epoch 739/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6212e-04 - val_loss: 5.0407e-04\n",
      "Epoch 740/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.4397e-04 - val_loss: 5.6808e-04\n",
      "Epoch 741/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.1014e-04 - val_loss: 5.3378e-04\n",
      "Epoch 742/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.3566e-04 - val_loss: 5.9151e-04\n",
      "Epoch 743/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7340e-04 - val_loss: 5.3714e-04\n",
      "Epoch 744/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.5321e-04 - val_loss: 5.9748e-04\n",
      "Epoch 745/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0631e-04 - val_loss: 6.1521e-04\n",
      "Epoch 746/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6021e-04 - val_loss: 5.6255e-04\n",
      "Epoch 747/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0549e-04 - val_loss: 5.4964e-04\n",
      "Epoch 748/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5041e-04 - val_loss: 6.3411e-04\n",
      "Epoch 749/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.4970e-04 - val_loss: 5.8609e-04\n",
      "Epoch 750/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.3343e-04 - val_loss: 5.5658e-04\n",
      "Epoch 751/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8314e-04 - val_loss: 6.2588e-04\n",
      "Epoch 752/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7749e-04 - val_loss: 5.8012e-04\n",
      "Epoch 753/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0855e-04 - val_loss: 5.6886e-04\n",
      "Epoch 754/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.1801e-04 - val_loss: 6.6202e-04\n",
      "Epoch 755/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0978e-04 - val_loss: 5.3210e-04\n",
      "Epoch 756/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.8710e-04 - val_loss: 5.4482e-04\n",
      "Epoch 757/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.2338e-04 - val_loss: 6.2266e-04\n",
      "Epoch 758/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.3983e-04 - val_loss: 5.6178e-04\n",
      "Epoch 759/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.2223e-04 - val_loss: 5.3503e-04\n",
      "Epoch 760/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.9749e-04 - val_loss: 5.2848e-04\n",
      "Epoch 761/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.4618e-04 - val_loss: 5.6062e-04\n",
      "Epoch 762/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.4596e-04 - val_loss: 5.9652e-04\n",
      "Epoch 763/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0808e-04 - val_loss: 6.2406e-04\n",
      "Epoch 764/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6303e-04 - val_loss: 5.5643e-04\n",
      "Epoch 765/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.5045e-04 - val_loss: 6.8177e-04\n",
      "Epoch 766/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.8240e-04 - val_loss: 6.4268e-04\n",
      "Epoch 767/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.8887e-04 - val_loss: 6.2745e-04\n",
      "Epoch 768/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8598e-04 - val_loss: 6.5940e-04\n",
      "Epoch 769/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7092e-04 - val_loss: 5.3146e-04\n",
      "Epoch 770/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.2312e-04 - val_loss: 5.6331e-04\n",
      "Epoch 771/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8791e-04 - val_loss: 5.6406e-04\n",
      "Epoch 772/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0753e-04 - val_loss: 6.0350e-04\n",
      "Epoch 773/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.5845e-04 - val_loss: 5.2390e-04\n",
      "Epoch 774/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.9213e-04 - val_loss: 5.9141e-04\n",
      "Epoch 775/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.4873e-04 - val_loss: 6.3463e-04\n",
      "Epoch 776/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.3992e-04 - val_loss: 6.1465e-04\n",
      "Epoch 777/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.1482e-04 - val_loss: 6.3856e-04\n",
      "Epoch 778/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.4154e-04 - val_loss: 5.4714e-04\n",
      "Epoch 779/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7065e-04 - val_loss: 6.3379e-04\n",
      "Epoch 780/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6476e-04 - val_loss: 5.4651e-04\n",
      "Epoch 781/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5965e-04 - val_loss: 5.9129e-04\n",
      "Epoch 782/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7396e-04 - val_loss: 6.2592e-04\n",
      "Epoch 783/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6156e-04 - val_loss: 5.8305e-04\n",
      "Epoch 784/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.9026e-04 - val_loss: 6.5445e-04\n",
      "Epoch 785/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5210e-04 - val_loss: 6.0711e-04\n",
      "Epoch 786/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.2005e-04 - val_loss: 5.7309e-04\n",
      "Epoch 787/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.3200e-04 - val_loss: 6.1791e-04\n",
      "Epoch 788/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.9859e-04 - val_loss: 5.8224e-04\n",
      "Epoch 789/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.4643e-04 - val_loss: 5.4142e-04\n",
      "Epoch 790/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5456e-04 - val_loss: 5.7998e-04\n",
      "Epoch 791/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.4623e-04 - val_loss: 5.5250e-04\n",
      "Epoch 792/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.4100e-04 - val_loss: 5.6932e-04\n",
      "Epoch 793/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.1596e-04 - val_loss: 5.2126e-04\n",
      "Epoch 794/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.3258e-04 - val_loss: 5.9880e-04\n",
      "Epoch 795/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.2537e-04 - val_loss: 5.9212e-04\n",
      "Epoch 796/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.7345e-04 - val_loss: 5.9110e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 797/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 5.8968e-04 - val_loss: 5.2607e-04\n",
      "Epoch 798/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.3890e-04 - val_loss: 6.2008e-04\n",
      "Epoch 799/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8375e-04 - val_loss: 5.3089e-04\n",
      "Epoch 800/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.5537e-04 - val_loss: 5.5355e-04\n",
      "Epoch 801/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.4707e-04 - val_loss: 5.5668e-04\n",
      "Epoch 802/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.1708e-04 - val_loss: 5.4637e-04\n",
      "Epoch 803/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.5995e-04 - val_loss: 5.9349e-04\n",
      "Epoch 804/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.5417e-04 - val_loss: 5.5546e-04\n",
      "Epoch 805/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.4164e-04 - val_loss: 6.1314e-04\n",
      "Epoch 806/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0735e-04 - val_loss: 5.7035e-04\n",
      "Epoch 807/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.1918e-04 - val_loss: 6.3066e-04\n",
      "Epoch 808/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.4988e-04 - val_loss: 5.2259e-04\n",
      "Epoch 809/1000\n",
      "53/53 [==============================] - 4s 71ms/step - loss: 6.4299e-04 - val_loss: 6.6666e-04\n",
      "Epoch 810/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0460e-04 - val_loss: 5.7726e-04\n",
      "Epoch 811/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.2373e-04 - val_loss: 7.0737e-04\n",
      "Epoch 812/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6054e-04 - val_loss: 5.2949e-04\n",
      "Epoch 813/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.4275e-04 - val_loss: 5.6613e-04\n",
      "Epoch 814/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.5371e-04 - val_loss: 5.8099e-04\n",
      "Epoch 815/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.3991e-04 - val_loss: 5.1419e-04\n",
      "Epoch 816/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.5301e-04 - val_loss: 5.5137e-04\n",
      "Epoch 817/1000\n",
      "53/53 [==============================] - 4s 71ms/step - loss: 6.6790e-04 - val_loss: 6.5273e-04\n",
      "Epoch 818/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.8441e-04 - val_loss: 5.3193e-04\n",
      "Epoch 819/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6209e-04 - val_loss: 6.5550e-04\n",
      "Epoch 820/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.9210e-04 - val_loss: 5.6161e-04\n",
      "Epoch 821/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0837e-04 - val_loss: 5.2556e-04\n",
      "Epoch 822/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.2024e-04 - val_loss: 5.4731e-04\n",
      "Epoch 823/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6123e-04 - val_loss: 5.1167e-04\n",
      "Epoch 824/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 5.7173e-04 - val_loss: 5.2061e-04\n",
      "Epoch 825/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5973e-04 - val_loss: 5.2828e-04\n",
      "Epoch 826/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6645e-04 - val_loss: 5.9925e-04\n",
      "Epoch 827/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5079e-04 - val_loss: 6.0314e-04\n",
      "Epoch 828/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.3219e-04 - val_loss: 7.2027e-04\n",
      "Epoch 829/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.4821e-04 - val_loss: 5.8265e-04\n",
      "Epoch 830/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0208e-04 - val_loss: 5.4102e-04\n",
      "Epoch 831/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.5719e-04 - val_loss: 5.2351e-04\n",
      "Epoch 832/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.1479e-04 - val_loss: 5.6368e-04\n",
      "Epoch 833/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.3527e-04 - val_loss: 5.8678e-04\n",
      "Epoch 834/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.8801e-04 - val_loss: 6.2168e-04\n",
      "Epoch 835/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0569e-04 - val_loss: 6.5208e-04\n",
      "Epoch 836/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.2614e-04 - val_loss: 6.0214e-04\n",
      "Epoch 837/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7537e-04 - val_loss: 5.3970e-04\n",
      "Epoch 838/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.7166e-04 - val_loss: 7.1868e-04\n",
      "Epoch 839/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7865e-04 - val_loss: 5.3612e-04\n",
      "Epoch 840/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.0410e-04 - val_loss: 5.6915e-04\n",
      "Epoch 841/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6204e-04 - val_loss: 5.9125e-04\n",
      "Epoch 842/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7372e-04 - val_loss: 5.6748e-04\n",
      "Epoch 843/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.4868e-04 - val_loss: 5.2877e-04\n",
      "Epoch 844/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6680e-04 - val_loss: 5.2898e-04\n",
      "Epoch 845/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5270e-04 - val_loss: 6.1416e-04\n",
      "Epoch 846/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.4561e-04 - val_loss: 5.3721e-04\n",
      "Epoch 847/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 5.9771e-04 - val_loss: 5.4696e-04\n",
      "Epoch 848/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.2792e-04 - val_loss: 5.7782e-04\n",
      "Epoch 849/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.0836e-04 - val_loss: 5.8776e-04\n",
      "Epoch 850/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.4940e-04 - val_loss: 5.3495e-04\n",
      "Epoch 851/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.2204e-04 - val_loss: 7.0673e-04\n",
      "Epoch 852/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1656e-04 - val_loss: 5.4441e-04\n",
      "Epoch 853/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 5.9699e-04 - val_loss: 5.0656e-04\n",
      "Epoch 854/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7581e-04 - val_loss: 6.8656e-04\n",
      "Epoch 855/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8088e-04 - val_loss: 5.9327e-04\n",
      "Epoch 856/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.7884e-04 - val_loss: 5.1537e-04\n",
      "Epoch 857/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6675e-04 - val_loss: 5.7147e-04\n",
      "Epoch 858/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.3569e-04 - val_loss: 5.1951e-04\n",
      "Epoch 859/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.3650e-04 - val_loss: 5.9039e-04\n",
      "Epoch 860/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0547e-04 - val_loss: 5.3289e-04\n",
      "Epoch 861/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.0365e-04 - val_loss: 5.2551e-04\n",
      "Epoch 862/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8543e-04 - val_loss: 5.6086e-04\n",
      "Epoch 863/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.4432e-04 - val_loss: 5.5615e-04\n",
      "Epoch 864/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.5077e-04 - val_loss: 5.6873e-04\n",
      "Epoch 865/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.1648e-04 - val_loss: 5.3596e-04\n",
      "Epoch 866/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.2777e-04 - val_loss: 5.2854e-04\n",
      "Epoch 867/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.2472e-04 - val_loss: 5.9572e-04\n",
      "Epoch 868/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.2975e-04 - val_loss: 5.5478e-04\n",
      "Epoch 869/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.8186e-04 - val_loss: 5.2153e-04\n",
      "Epoch 870/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.1599e-04 - val_loss: 5.6269e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 871/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0178e-04 - val_loss: 5.6776e-04\n",
      "Epoch 872/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.1743e-04 - val_loss: 5.4750e-04\n",
      "Epoch 873/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6838e-04 - val_loss: 5.2867e-04\n",
      "Epoch 874/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.2933e-04 - val_loss: 5.4441e-04\n",
      "Epoch 875/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.4243e-04 - val_loss: 6.4064e-04\n",
      "Epoch 876/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.3688e-04 - val_loss: 6.7300e-04\n",
      "Epoch 877/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.2034e-04 - val_loss: 5.3032e-04\n",
      "Epoch 878/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.3500e-04 - val_loss: 6.7772e-04\n",
      "Epoch 879/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 5.9126e-04 - val_loss: 5.6883e-04\n",
      "Epoch 880/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6259e-04 - val_loss: 5.1731e-04\n",
      "Epoch 881/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.1608e-04 - val_loss: 5.1067e-04\n",
      "Epoch 882/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6464e-04 - val_loss: 5.8775e-04\n",
      "Epoch 883/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.5115e-04 - val_loss: 6.1515e-04\n",
      "Epoch 884/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7721e-04 - val_loss: 5.9528e-04\n",
      "Epoch 885/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.9702e-04 - val_loss: 5.8419e-04\n",
      "Epoch 886/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.2739e-04 - val_loss: 5.7698e-04\n",
      "Epoch 887/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7535e-04 - val_loss: 5.9279e-04\n",
      "Epoch 888/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 5.9731e-04 - val_loss: 5.4084e-04\n",
      "Epoch 889/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7774e-04 - val_loss: 5.5358e-04\n",
      "Epoch 890/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.9301e-04 - val_loss: 5.7496e-04\n",
      "Epoch 891/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.4721e-04 - val_loss: 5.3799e-04\n",
      "Epoch 892/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6483e-04 - val_loss: 7.3976e-04\n",
      "Epoch 893/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.9444e-04 - val_loss: 6.6472e-04\n",
      "Epoch 894/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.3678e-04 - val_loss: 7.1753e-04\n",
      "Epoch 895/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6355e-04 - val_loss: 5.3221e-04\n",
      "Epoch 896/1000\n",
      "53/53 [==============================] - 4s 71ms/step - loss: 6.3623e-04 - val_loss: 5.0878e-04\n",
      "Epoch 897/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.4003e-04 - val_loss: 5.4409e-04\n",
      "Epoch 898/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 5.8548e-04 - val_loss: 5.2314e-04\n",
      "Epoch 899/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.2577e-04 - val_loss: 5.7761e-04\n",
      "Epoch 900/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5085e-04 - val_loss: 5.4488e-04\n",
      "Epoch 901/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6439e-04 - val_loss: 5.5106e-04\n",
      "Epoch 902/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.3837e-04 - val_loss: 5.0979e-04\n",
      "Epoch 903/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7131e-04 - val_loss: 5.4185e-04\n",
      "Epoch 904/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.8950e-04 - val_loss: 5.7729e-04\n",
      "Epoch 905/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6088e-04 - val_loss: 5.2501e-04\n",
      "Epoch 906/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.3844e-04 - val_loss: 5.3276e-04\n",
      "Epoch 907/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.2540e-04 - val_loss: 5.3969e-04\n",
      "Epoch 908/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6169e-04 - val_loss: 5.1350e-04\n",
      "Epoch 909/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.8287e-04 - val_loss: 5.7065e-04\n",
      "Epoch 910/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6900e-04 - val_loss: 5.1713e-04\n",
      "Epoch 911/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7040e-04 - val_loss: 5.4977e-04\n",
      "Epoch 912/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.5203e-04 - val_loss: 6.9468e-04\n",
      "Epoch 913/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.8547e-04 - val_loss: 6.1253e-04\n",
      "Epoch 914/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6866e-04 - val_loss: 5.2767e-04\n",
      "Epoch 915/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.8224e-04 - val_loss: 6.3308e-04\n",
      "Epoch 916/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.4931e-04 - val_loss: 6.1993e-04\n",
      "Epoch 917/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.5301e-04 - val_loss: 5.7426e-04\n",
      "Epoch 918/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.5381e-04 - val_loss: 5.8537e-04\n",
      "Epoch 919/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6993e-04 - val_loss: 5.8760e-04\n",
      "Epoch 920/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.5150e-04 - val_loss: 5.1270e-04\n",
      "Epoch 921/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.6473e-04 - val_loss: 5.7773e-04\n",
      "Epoch 922/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.3326e-04 - val_loss: 6.7582e-04\n",
      "Epoch 923/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.4274e-04 - val_loss: 5.1709e-04\n",
      "Epoch 924/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.7714e-04 - val_loss: 5.7344e-04\n",
      "Epoch 925/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.5843e-04 - val_loss: 6.1983e-04\n",
      "Epoch 926/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.1651e-04 - val_loss: 5.6979e-04\n",
      "Epoch 927/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6175e-04 - val_loss: 5.4374e-04\n",
      "Epoch 928/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.7565e-04 - val_loss: 5.9242e-04\n",
      "Epoch 929/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.0260e-04 - val_loss: 5.1946e-04\n",
      "Epoch 930/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.2953e-04 - val_loss: 6.3057e-04\n",
      "Epoch 931/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.2275e-04 - val_loss: 5.7994e-04\n",
      "Epoch 932/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.3204e-04 - val_loss: 5.5015e-04\n",
      "Epoch 933/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.4008e-04 - val_loss: 5.1267e-04\n",
      "Epoch 934/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7213e-04 - val_loss: 5.7978e-04\n",
      "Epoch 935/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.9925e-04 - val_loss: 6.8150e-04\n",
      "Epoch 936/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5726e-04 - val_loss: 5.1319e-04\n",
      "Epoch 937/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.8988e-04 - val_loss: 5.1355e-04\n",
      "Epoch 938/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.3057e-04 - val_loss: 5.3782e-04\n",
      "Epoch 939/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7007e-04 - val_loss: 5.5394e-04\n",
      "Epoch 940/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.3076e-04 - val_loss: 5.0268e-04\n",
      "Epoch 941/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 5.9813e-04 - val_loss: 5.1350e-04\n",
      "Epoch 942/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5445e-04 - val_loss: 6.3797e-04\n",
      "Epoch 943/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7539e-04 - val_loss: 5.5822e-04\n",
      "Epoch 944/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.4227e-04 - val_loss: 5.3906e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 945/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.3105e-04 - val_loss: 5.7598e-04\n",
      "Epoch 946/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.1805e-04 - val_loss: 6.6722e-04\n",
      "Epoch 947/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.1785e-04 - val_loss: 5.5684e-04\n",
      "Epoch 948/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.1171e-04 - val_loss: 5.4408e-04\n",
      "Epoch 949/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5750e-04 - val_loss: 5.4446e-04\n",
      "Epoch 950/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6913e-04 - val_loss: 5.3046e-04\n",
      "Epoch 951/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 5.6489e-04 - val_loss: 6.3671e-04\n",
      "Epoch 952/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7739e-04 - val_loss: 5.7133e-04\n",
      "Epoch 953/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5143e-04 - val_loss: 5.6990e-04\n",
      "Epoch 954/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.5784e-04 - val_loss: 5.4649e-04\n",
      "Epoch 955/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.9539e-04 - val_loss: 5.0782e-04\n",
      "Epoch 956/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 5.9763e-04 - val_loss: 5.7227e-04\n",
      "Epoch 957/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.3926e-04 - val_loss: 5.4397e-04\n",
      "Epoch 958/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.1961e-04 - val_loss: 5.3331e-04\n",
      "Epoch 959/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.0973e-04 - val_loss: 5.4504e-04\n",
      "Epoch 960/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.4687e-04 - val_loss: 5.7662e-04\n",
      "Epoch 961/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.9849e-04 - val_loss: 6.5623e-04\n",
      "Epoch 962/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.4118e-04 - val_loss: 6.2549e-04\n",
      "Epoch 963/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0461e-04 - val_loss: 5.6264e-04\n",
      "Epoch 964/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.8813e-04 - val_loss: 5.1624e-04\n",
      "Epoch 965/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.1712e-04 - val_loss: 6.9656e-04\n",
      "Epoch 966/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.3942e-04 - val_loss: 5.8930e-04\n",
      "Epoch 967/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.4245e-04 - val_loss: 5.8275e-04\n",
      "Epoch 968/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.1244e-04 - val_loss: 4.9961e-04\n",
      "Epoch 969/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.3992e-04 - val_loss: 7.0913e-04\n",
      "Epoch 970/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6935e-04 - val_loss: 5.7270e-04\n",
      "Epoch 971/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.0123e-04 - val_loss: 5.6769e-04\n",
      "Epoch 972/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.4526e-04 - val_loss: 5.3177e-04\n",
      "Epoch 973/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.4697e-04 - val_loss: 4.9686e-04\n",
      "Epoch 974/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9370e-04 - val_loss: 5.6986e-04\n",
      "Epoch 975/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.1802e-04 - val_loss: 5.1861e-04\n",
      "Epoch 976/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.3356e-04 - val_loss: 5.1931e-04\n",
      "Epoch 977/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.1289e-04 - val_loss: 5.2968e-04\n",
      "Epoch 978/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.0368e-04 - val_loss: 5.2399e-04\n",
      "Epoch 979/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9208e-04 - val_loss: 5.2559e-04\n",
      "Epoch 980/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.4830e-04 - val_loss: 6.1837e-04\n",
      "Epoch 981/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.4767e-04 - val_loss: 5.8007e-04\n",
      "Epoch 982/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.3741e-04 - val_loss: 6.0986e-04\n",
      "Epoch 983/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.4603e-04 - val_loss: 5.4567e-04\n",
      "Epoch 984/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6552e-04 - val_loss: 6.8546e-04\n",
      "Epoch 985/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 5.9826e-04 - val_loss: 5.6085e-04\n",
      "Epoch 986/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 7.0370e-04 - val_loss: 6.2371e-04\n",
      "Epoch 987/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.3230e-04 - val_loss: 5.5548e-04\n",
      "Epoch 988/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6956e-04 - val_loss: 6.4080e-04\n",
      "Epoch 989/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.5171e-04 - val_loss: 6.5418e-04\n",
      "Epoch 990/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.9584e-04 - val_loss: 5.6486e-04\n",
      "Epoch 991/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.9316e-04 - val_loss: 5.0875e-04\n",
      "Epoch 992/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.6845e-04 - val_loss: 5.1530e-04\n",
      "Epoch 993/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.0889e-04 - val_loss: 5.5472e-04\n",
      "Epoch 994/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0472e-04 - val_loss: 5.9576e-04\n",
      "Epoch 995/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.1716e-04 - val_loss: 5.2979e-04\n",
      "Epoch 996/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.6733e-04 - val_loss: 5.4449e-04\n",
      "Epoch 997/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.7382e-04 - val_loss: 5.9721e-04\n",
      "Epoch 998/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.7498e-04 - val_loss: 5.8170e-04\n",
      "Epoch 999/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.3153e-04 - val_loss: 5.8540e-04\n",
      "Epoch 1000/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.3954e-04 - val_loss: 6.4599e-04\n"
     ]
    }
   ],
   "source": [
    "g1_model3_nadam = Sequential()\n",
    "g1_model3_nadam.add(Conv2D(32,\n",
    "                 (6, 6),\n",
    "                 activation='relu',\n",
    "                input_shape=INPUT_SHAPE))\n",
    "g1_model3_nadam.add(BatchNormalization())\n",
    "g1_model3_nadam.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "g1_model3_nadam.add(Conv2D(filters=64,\n",
    "                 kernel_size=(5, 5),\n",
    "                 activation='relu'))\n",
    "g1_model3_nadam.add(BatchNormalization())\n",
    "g1_model3_nadam.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "g1_model3_nadam.add(Conv2D(filters=256,\n",
    "                 kernel_size=(4, 4),\n",
    "                 activation='relu'))\n",
    "g1_model3_nadam.add(BatchNormalization())\n",
    "g1_model3_nadam.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "g1_model3_nadam.add(Conv2D(filters=64,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "g1_model3_nadam.add(BatchNormalization())\n",
    "g1_model3_nadam.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "g1_model3_nadam.add(Conv2D(filters=128,\n",
    "                 kernel_size=(2, 2),\n",
    "                 activation='relu'))\n",
    "g1_model3_nadam.add(BatchNormalization())\n",
    "g1_model3_nadam.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "g1_model3_nadam.add(Flatten())\n",
    "g1_model3_nadam.add(Dense(500, activation = \"relu\"))\n",
    "g1_model3_nadam.add(BatchNormalization())\n",
    "g1_model3_nadam.add(Dense(500, activation = \"relu\"))\n",
    "g1_model3_nadam.add(BatchNormalization())\n",
    "g1_model3_nadam.add(Dropout(.3))\n",
    "g1_model3_nadam.add(Dense(30))\n",
    "print(g1_model3_nadam.summary())\n",
    "g1_CNN_aug_addedLayers_hist_nadam, g1_model3_new_nadam = fit_model(g1_model3_nadam, data1,'g1_CNN_aug_addedLayers',\n",
    "                                                       datagen, patience=1000, retrain = True, optimizer='nadam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 91, 91, 32)        1184      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 91, 91, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 45, 45, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 41, 41, 64)        51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 41, 41, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 20, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 17, 17, 256)       262400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 17, 17, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 6, 6, 64)          147520    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 2, 2, 128)         32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 2, 2, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 500)               64500     \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 30)                15030     \n",
      "=================================================================\n",
      "Total params: 831,470\n",
      "Trainable params: 828,382\n",
      "Non-trainable params: 3,088\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n",
      "53/53 [==============================] - 5s 101ms/step - loss: 1.3058 - val_loss: 1.0708\n",
      "Epoch 2/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.6353 - val_loss: 0.1651\n",
      "Epoch 3/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.4851 - val_loss: 0.0855\n",
      "Epoch 4/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.3881 - val_loss: 0.0536\n",
      "Epoch 5/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.3097 - val_loss: 0.0347\n",
      "Epoch 6/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.2502 - val_loss: 0.0245\n",
      "Epoch 7/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.2011 - val_loss: 0.0171\n",
      "Epoch 8/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.1638 - val_loss: 0.0132\n",
      "Epoch 9/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.1337 - val_loss: 0.0142\n",
      "Epoch 10/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.1072 - val_loss: 0.0096\n",
      "Epoch 11/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0881 - val_loss: 0.0088\n",
      "Epoch 12/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0714 - val_loss: 0.0084\n",
      "Epoch 13/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0585 - val_loss: 0.0074\n",
      "Epoch 14/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0482 - val_loss: 0.0070\n",
      "Epoch 15/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0406 - val_loss: 0.0066\n",
      "Epoch 16/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0337 - val_loss: 0.0061\n",
      "Epoch 17/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0282 - val_loss: 0.0061\n",
      "Epoch 18/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0245 - val_loss: 0.0059\n",
      "Epoch 19/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0218 - val_loss: 0.0053\n",
      "Epoch 20/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0184 - val_loss: 0.0058\n",
      "Epoch 21/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0166 - val_loss: 0.0053\n",
      "Epoch 22/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0148 - val_loss: 0.0055\n",
      "Epoch 23/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0140 - val_loss: 0.0054\n",
      "Epoch 24/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0127 - val_loss: 0.0051\n",
      "Epoch 25/1000\n",
      "53/53 [==============================] - 3s 65ms/step - loss: 0.0120 - val_loss: 0.0051\n",
      "Epoch 26/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0118 - val_loss: 0.0050\n",
      "Epoch 27/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0110 - val_loss: 0.0048\n",
      "Epoch 28/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0109 - val_loss: 0.0052\n",
      "Epoch 29/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0107 - val_loss: 0.0048\n",
      "Epoch 30/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0105 - val_loss: 0.0051\n",
      "Epoch 31/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0103 - val_loss: 0.0050\n",
      "Epoch 32/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0102 - val_loss: 0.0050\n",
      "Epoch 33/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0097 - val_loss: 0.0050\n",
      "Epoch 34/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0102 - val_loss: 0.0046\n",
      "Epoch 35/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0096 - val_loss: 0.0048\n",
      "Epoch 36/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0095 - val_loss: 0.0048\n",
      "Epoch 37/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0098 - val_loss: 0.0050\n",
      "Epoch 38/1000\n",
      "53/53 [==============================] - 3s 65ms/step - loss: 0.0096 - val_loss: 0.0054\n",
      "Epoch 39/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0096 - val_loss: 0.0058\n",
      "Epoch 40/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0096 - val_loss: 0.0047\n",
      "Epoch 41/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0093 - val_loss: 0.0046\n",
      "Epoch 42/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0093 - val_loss: 0.0045\n",
      "Epoch 43/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0092 - val_loss: 0.0048\n",
      "Epoch 44/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0091 - val_loss: 0.0046\n",
      "Epoch 45/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0086 - val_loss: 0.0048\n",
      "Epoch 46/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0085 - val_loss: 0.0048\n",
      "Epoch 47/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0083 - val_loss: 0.0045\n",
      "Epoch 48/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0084 - val_loss: 0.0043\n",
      "Epoch 49/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0076 - val_loss: 0.0043\n",
      "Epoch 50/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0079 - val_loss: 0.0038\n",
      "Epoch 51/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0075 - val_loss: 0.0042\n",
      "Epoch 52/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0071 - val_loss: 0.0041\n",
      "Epoch 53/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0070 - val_loss: 0.0039\n",
      "Epoch 54/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0068 - val_loss: 0.0041\n",
      "Epoch 55/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0064 - val_loss: 0.0037\n",
      "Epoch 56/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0059 - val_loss: 0.0037\n",
      "Epoch 57/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0057 - val_loss: 0.0032\n",
      "Epoch 58/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0056 - val_loss: 0.0032\n",
      "Epoch 59/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0054 - val_loss: 0.0033\n",
      "Epoch 60/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0052 - val_loss: 0.0030\n",
      "Epoch 61/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0048 - val_loss: 0.0028\n",
      "Epoch 62/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0045 - val_loss: 0.0028\n",
      "Epoch 63/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0046 - val_loss: 0.0027\n",
      "Epoch 64/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0043 - val_loss: 0.0028\n",
      "Epoch 65/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0042 - val_loss: 0.0030\n",
      "Epoch 66/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0042 - val_loss: 0.0025\n",
      "Epoch 67/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0040 - val_loss: 0.0025\n",
      "Epoch 68/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0039 - val_loss: 0.0024\n",
      "Epoch 69/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0039 - val_loss: 0.0023\n",
      "Epoch 70/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0038 - val_loss: 0.0024\n",
      "Epoch 71/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0037 - val_loss: 0.0024\n",
      "Epoch 72/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0037 - val_loss: 0.0024\n",
      "Epoch 73/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0037 - val_loss: 0.0021\n",
      "Epoch 74/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0037 - val_loss: 0.0025\n",
      "Epoch 75/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0035 - val_loss: 0.0022\n",
      "Epoch 76/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0034 - val_loss: 0.0021\n",
      "Epoch 77/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0033 - val_loss: 0.0023\n",
      "Epoch 78/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0033 - val_loss: 0.0023\n",
      "Epoch 79/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0033 - val_loss: 0.0022\n",
      "Epoch 80/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0032 - val_loss: 0.0024\n",
      "Epoch 81/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0033 - val_loss: 0.0023\n",
      "Epoch 82/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0033 - val_loss: 0.0022\n",
      "Epoch 83/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0032 - val_loss: 0.0020\n",
      "Epoch 84/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0033 - val_loss: 0.0021\n",
      "Epoch 85/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0033 - val_loss: 0.0025\n",
      "Epoch 86/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0033 - val_loss: 0.0025\n",
      "Epoch 87/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0032 - val_loss: 0.0022\n",
      "Epoch 88/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0033 - val_loss: 0.0021\n",
      "Epoch 89/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0033 - val_loss: 0.0024\n",
      "Epoch 90/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0033 - val_loss: 0.0021\n",
      "Epoch 91/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0033 - val_loss: 0.0023\n",
      "Epoch 92/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0033 - val_loss: 0.0020\n",
      "Epoch 93/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0032 - val_loss: 0.0029\n",
      "Epoch 94/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0032 - val_loss: 0.0020\n",
      "Epoch 95/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0031 - val_loss: 0.0020\n",
      "Epoch 96/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0031 - val_loss: 0.0021\n",
      "Epoch 97/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0032 - val_loss: 0.0026\n",
      "Epoch 98/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0032 - val_loss: 0.0021\n",
      "Epoch 99/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0032 - val_loss: 0.0021\n",
      "Epoch 100/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0031 - val_loss: 0.0022\n",
      "Epoch 101/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0031 - val_loss: 0.0022\n",
      "Epoch 102/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0032 - val_loss: 0.0020\n",
      "Epoch 103/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0031 - val_loss: 0.0019\n",
      "Epoch 104/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0031 - val_loss: 0.0022\n",
      "Epoch 105/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0032 - val_loss: 0.0022\n",
      "Epoch 106/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0032 - val_loss: 0.0021\n",
      "Epoch 107/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0030 - val_loss: 0.0022\n",
      "Epoch 108/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0031 - val_loss: 0.0020\n",
      "Epoch 109/1000\n",
      "53/53 [==============================] - 3s 65ms/step - loss: 0.0031 - val_loss: 0.0020\n",
      "Epoch 110/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0029 - val_loss: 0.0020\n",
      "Epoch 111/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0030 - val_loss: 0.0020\n",
      "Epoch 112/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0032 - val_loss: 0.0021\n",
      "Epoch 113/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0031 - val_loss: 0.0020\n",
      "Epoch 114/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0032 - val_loss: 0.0022\n",
      "Epoch 115/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0035 - val_loss: 0.0023\n",
      "Epoch 116/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0033 - val_loss: 0.0024\n",
      "Epoch 117/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0032 - val_loss: 0.0021\n",
      "Epoch 118/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0030 - val_loss: 0.0020\n",
      "Epoch 119/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0030 - val_loss: 0.0022\n",
      "Epoch 120/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0030 - val_loss: 0.0023\n",
      "Epoch 121/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0030 - val_loss: 0.0020\n",
      "Epoch 122/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0031 - val_loss: 0.0021\n",
      "Epoch 123/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0028 - val_loss: 0.0017\n",
      "Epoch 124/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0030 - val_loss: 0.0021\n",
      "Epoch 125/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0031 - val_loss: 0.0022\n",
      "Epoch 126/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0030 - val_loss: 0.0022\n",
      "Epoch 127/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0029 - val_loss: 0.0021\n",
      "Epoch 128/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0028 - val_loss: 0.0021\n",
      "Epoch 129/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0028 - val_loss: 0.0018\n",
      "Epoch 130/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0027 - val_loss: 0.0019\n",
      "Epoch 131/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0028 - val_loss: 0.0021\n",
      "Epoch 132/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0028 - val_loss: 0.0019\n",
      "Epoch 133/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0028 - val_loss: 0.0021\n",
      "Epoch 134/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0027 - val_loss: 0.0018\n",
      "Epoch 135/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0027 - val_loss: 0.0018\n",
      "Epoch 136/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0027 - val_loss: 0.0017\n",
      "Epoch 137/1000\n",
      "53/53 [==============================] - 3s 65ms/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 138/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0026 - val_loss: 0.0017\n",
      "Epoch 139/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0025 - val_loss: 0.0021\n",
      "Epoch 140/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 141/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0025 - val_loss: 0.0017\n",
      "Epoch 142/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0025 - val_loss: 0.0017\n",
      "Epoch 143/1000\n",
      "53/53 [==============================] - 3s 65ms/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 144/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 145/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0024 - val_loss: 0.0015\n",
      "Epoch 146/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 147/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0025 - val_loss: 0.0017\n",
      "Epoch 148/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0024 - val_loss: 0.0015\n",
      "Epoch 149/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 150/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 151/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 152/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 153/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 154/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 155/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 156/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 157/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 158/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 159/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 160/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 161/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 162/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 163/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 164/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 165/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 166/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0018 - val_loss: 0.0012\n",
      "Epoch 167/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 168/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 169/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 170/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 171/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 172/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 173/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 174/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 175/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0016 - val_loss: 9.8885e-04\n",
      "Epoch 176/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 177/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 178/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 179/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0015 - val_loss: 9.6997e-04\n",
      "Epoch 180/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 0.0015 - val_loss: 9.8394e-04\n",
      "Epoch 181/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0015 - val_loss: 9.7796e-04\n",
      "Epoch 182/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0014 - val_loss: 9.7843e-04\n",
      "Epoch 183/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 184/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 185/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0014 - val_loss: 9.2919e-04\n",
      "Epoch 186/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 187/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 188/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0014 - val_loss: 9.8292e-04\n",
      "Epoch 189/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0013 - val_loss: 8.2599e-04\n",
      "Epoch 190/1000\n",
      "53/53 [==============================] - 3s 65ms/step - loss: 0.0013 - val_loss: 9.1295e-04\n",
      "Epoch 191/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0013 - val_loss: 9.1043e-04\n",
      "Epoch 192/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0013 - val_loss: 9.5984e-04\n",
      "Epoch 193/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0013 - val_loss: 9.2734e-04\n",
      "Epoch 194/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0013 - val_loss: 8.5072e-04\n",
      "Epoch 195/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0013 - val_loss: 8.5988e-04\n",
      "Epoch 196/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0012 - val_loss: 8.3645e-04\n",
      "Epoch 197/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0012 - val_loss: 8.4188e-04\n",
      "Epoch 198/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0012 - val_loss: 8.0890e-04\n",
      "Epoch 199/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0012 - val_loss: 8.4490e-04\n",
      "Epoch 200/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0012 - val_loss: 7.9445e-04\n",
      "Epoch 201/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0011 - val_loss: 8.1418e-04\n",
      "Epoch 202/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0011 - val_loss: 8.1725e-04\n",
      "Epoch 203/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0012 - val_loss: 7.5089e-04\n",
      "Epoch 204/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0011 - val_loss: 7.8152e-04\n",
      "Epoch 205/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0011 - val_loss: 8.9679e-04\n",
      "Epoch 206/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0011 - val_loss: 7.5545e-04\n",
      "Epoch 207/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0011 - val_loss: 7.6999e-04\n",
      "Epoch 208/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0011 - val_loss: 7.9952e-04\n",
      "Epoch 209/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0011 - val_loss: 7.1226e-04\n",
      "Epoch 210/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0011 - val_loss: 7.5695e-04\n",
      "Epoch 211/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0011 - val_loss: 7.5005e-04\n",
      "Epoch 212/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0010 - val_loss: 7.5220e-04\n",
      "Epoch 213/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0011 - val_loss: 7.9378e-04\n",
      "Epoch 214/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 0.0010 - val_loss: 7.9632e-04\n",
      "Epoch 215/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0010 - val_loss: 6.8992e-04\n",
      "Epoch 216/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0010 - val_loss: 8.2669e-04\n",
      "Epoch 217/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0010 - val_loss: 7.1334e-04\n",
      "Epoch 218/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 0.0010 - val_loss: 7.4723e-04\n",
      "Epoch 219/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0010 - val_loss: 7.7990e-04\n",
      "Epoch 220/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 9.9901e-04 - val_loss: 7.4593e-04\n",
      "Epoch 221/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 9.7647e-04 - val_loss: 7.6361e-04\n",
      "Epoch 222/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0010 - val_loss: 7.4806e-04\n",
      "Epoch 223/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0010 - val_loss: 7.4283e-04\n",
      "Epoch 224/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0011 - val_loss: 7.4308e-04\n",
      "Epoch 225/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0010 - val_loss: 7.8526e-04\n",
      "Epoch 226/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 9.7202e-04 - val_loss: 7.9299e-04\n",
      "Epoch 227/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 0.0010 - val_loss: 6.9049e-04\n",
      "Epoch 228/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.7508e-04 - val_loss: 7.4153e-04\n",
      "Epoch 229/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 9.9304e-04 - val_loss: 6.9115e-04\n",
      "Epoch 230/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 0.0010 - val_loss: 7.0021e-04\n",
      "Epoch 231/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 9.9040e-04 - val_loss: 7.3748e-04\n",
      "Epoch 232/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 9.4595e-04 - val_loss: 7.3102e-04\n",
      "Epoch 233/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 9.9913e-04 - val_loss: 7.0250e-04\n",
      "Epoch 234/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.7501e-04 - val_loss: 6.7620e-04\n",
      "Epoch 235/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 9.8487e-04 - val_loss: 7.1790e-04\n",
      "Epoch 236/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.8916e-04 - val_loss: 7.3327e-04\n",
      "Epoch 237/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.3053e-04 - val_loss: 7.5169e-04\n",
      "Epoch 238/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 9.2480e-04 - val_loss: 6.7367e-04\n",
      "Epoch 239/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 9.8773e-04 - val_loss: 6.8093e-04\n",
      "Epoch 240/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 9.9776e-04 - val_loss: 8.5073e-04\n",
      "Epoch 241/1000\n",
      "53/53 [==============================] - 3s 65ms/step - loss: 9.3220e-04 - val_loss: 7.0761e-04\n",
      "Epoch 242/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.1978e-04 - val_loss: 6.9822e-04\n",
      "Epoch 243/1000\n",
      "53/53 [==============================] - 3s 65ms/step - loss: 0.0010 - val_loss: 7.1851e-04\n",
      "Epoch 244/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.6645e-04 - val_loss: 6.4318e-04\n",
      "Epoch 245/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 9.2520e-04 - val_loss: 6.7329e-04\n",
      "Epoch 246/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.6195e-04 - val_loss: 7.2506e-04\n",
      "Epoch 247/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 9.1725e-04 - val_loss: 7.9744e-04\n",
      "Epoch 248/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.5095e-04 - val_loss: 6.9448e-04\n",
      "Epoch 249/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 9.4049e-04 - val_loss: 6.8454e-04\n",
      "Epoch 250/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 9.2794e-04 - val_loss: 8.0784e-04\n",
      "Epoch 251/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.9381e-04 - val_loss: 6.9871e-04\n",
      "Epoch 252/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 9.6764e-04 - val_loss: 7.8744e-04\n",
      "Epoch 253/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.2073e-04 - val_loss: 6.5304e-04\n",
      "Epoch 254/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 9.7120e-04 - val_loss: 7.3905e-04\n",
      "Epoch 255/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.4970e-04 - val_loss: 6.2351e-04\n",
      "Epoch 256/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 9.2957e-04 - val_loss: 6.7788e-04\n",
      "Epoch 257/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.6635e-04 - val_loss: 7.1018e-04\n",
      "Epoch 258/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.8841e-04 - val_loss: 8.5958e-04\n",
      "Epoch 259/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.1893e-04 - val_loss: 8.3278e-04\n",
      "Epoch 260/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.8045e-04 - val_loss: 6.6105e-04\n",
      "Epoch 261/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.3998e-04 - val_loss: 6.7015e-04\n",
      "Epoch 262/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.0511e-04 - val_loss: 6.8296e-04\n",
      "Epoch 263/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.8167e-04 - val_loss: 7.2190e-04\n",
      "Epoch 264/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.2467e-04 - val_loss: 7.1751e-04\n",
      "Epoch 265/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.2204e-04 - val_loss: 6.7353e-04\n",
      "Epoch 266/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.2011e-04 - val_loss: 6.4570e-04\n",
      "Epoch 267/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 9.4448e-04 - val_loss: 7.3443e-04\n",
      "Epoch 268/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.3530e-04 - val_loss: 7.4522e-04\n",
      "Epoch 269/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.5961e-04 - val_loss: 7.2352e-04\n",
      "Epoch 270/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 9.9199e-04 - val_loss: 7.7531e-04\n",
      "Epoch 271/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 9.0662e-04 - val_loss: 6.8675e-04\n",
      "Epoch 272/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.9768e-04 - val_loss: 6.6575e-04\n",
      "Epoch 273/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 9.4544e-04 - val_loss: 6.2331e-04\n",
      "Epoch 274/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.4988e-04 - val_loss: 8.0481e-04\n",
      "Epoch 275/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.8548e-04 - val_loss: 6.6862e-04\n",
      "Epoch 276/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 9.2421e-04 - val_loss: 7.0769e-04\n",
      "Epoch 277/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.7902e-04 - val_loss: 6.6120e-04\n",
      "Epoch 278/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.2232e-04 - val_loss: 6.5498e-04\n",
      "Epoch 279/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.7578e-04 - val_loss: 7.5126e-04\n",
      "Epoch 280/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.9551e-04 - val_loss: 6.3964e-04\n",
      "Epoch 281/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 4s 67ms/step - loss: 9.0854e-04 - val_loss: 7.0438e-04\n",
      "Epoch 282/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.0143e-04 - val_loss: 8.1385e-04\n",
      "Epoch 283/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.0646e-04 - val_loss: 6.7430e-04\n",
      "Epoch 284/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 8.6178e-04 - val_loss: 6.7714e-04\n",
      "Epoch 285/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.2725e-04 - val_loss: 6.4790e-04\n",
      "Epoch 286/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.6578e-04 - val_loss: 6.5247e-04\n",
      "Epoch 287/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.2809e-04 - val_loss: 7.4068e-04\n",
      "Epoch 288/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.2735e-04 - val_loss: 6.3779e-04\n",
      "Epoch 289/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.7733e-04 - val_loss: 6.2306e-04\n",
      "Epoch 290/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.5712e-04 - val_loss: 6.7277e-04\n",
      "Epoch 291/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.6043e-04 - val_loss: 6.8037e-04\n",
      "Epoch 292/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.9081e-04 - val_loss: 6.2944e-04\n",
      "Epoch 293/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 9.1159e-04 - val_loss: 7.6822e-04\n",
      "Epoch 294/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.9227e-04 - val_loss: 6.8604e-04\n",
      "Epoch 295/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 9.2117e-04 - val_loss: 6.3378e-04\n",
      "Epoch 296/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.4024e-04 - val_loss: 7.1580e-04\n",
      "Epoch 297/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.2508e-04 - val_loss: 6.6211e-04\n",
      "Epoch 298/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.6701e-04 - val_loss: 6.4350e-04\n",
      "Epoch 299/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.6903e-04 - val_loss: 6.1625e-04\n",
      "Epoch 300/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.2247e-04 - val_loss: 6.5453e-04\n",
      "Epoch 301/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 9.2946e-04 - val_loss: 7.4443e-04\n",
      "Epoch 302/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.5130e-04 - val_loss: 6.7238e-04\n",
      "Epoch 303/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.7182e-04 - val_loss: 7.1503e-04\n",
      "Epoch 304/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.2010e-04 - val_loss: 5.8130e-04\n",
      "Epoch 305/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.6271e-04 - val_loss: 6.1807e-04\n",
      "Epoch 306/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.4646e-04 - val_loss: 6.1411e-04\n",
      "Epoch 307/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.8732e-04 - val_loss: 5.8633e-04\n",
      "Epoch 308/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.6529e-04 - val_loss: 7.8938e-04\n",
      "Epoch 309/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.9674e-04 - val_loss: 6.6893e-04\n",
      "Epoch 310/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.5298e-04 - val_loss: 6.2488e-04\n",
      "Epoch 311/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.4982e-04 - val_loss: 6.1743e-04\n",
      "Epoch 312/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.6930e-04 - val_loss: 6.6205e-04\n",
      "Epoch 313/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.6013e-04 - val_loss: 7.0911e-04\n",
      "Epoch 314/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.3316e-04 - val_loss: 6.3032e-04\n",
      "Epoch 315/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.1981e-04 - val_loss: 6.6752e-04\n",
      "Epoch 316/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.5832e-04 - val_loss: 7.1099e-04\n",
      "Epoch 317/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.6583e-04 - val_loss: 6.6065e-04\n",
      "Epoch 318/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.6636e-04 - val_loss: 6.2158e-04\n",
      "Epoch 319/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.5010e-04 - val_loss: 6.1722e-04\n",
      "Epoch 320/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.5091e-04 - val_loss: 6.0810e-04\n",
      "Epoch 321/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.5321e-04 - val_loss: 6.9973e-04\n",
      "Epoch 322/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.3098e-04 - val_loss: 7.0525e-04\n",
      "Epoch 323/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.6903e-04 - val_loss: 6.6337e-04\n",
      "Epoch 324/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 8.5255e-04 - val_loss: 5.8262e-04\n",
      "Epoch 325/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.5441e-04 - val_loss: 6.4265e-04\n",
      "Epoch 326/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.6738e-04 - val_loss: 6.5614e-04\n",
      "Epoch 327/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.1624e-04 - val_loss: 6.3444e-04\n",
      "Epoch 328/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.5384e-04 - val_loss: 6.4611e-04\n",
      "Epoch 329/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.2408e-04 - val_loss: 6.9701e-04\n",
      "Epoch 330/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.3440e-04 - val_loss: 8.1457e-04\n",
      "Epoch 331/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.6374e-04 - val_loss: 6.4627e-04\n",
      "Epoch 332/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.4039e-04 - val_loss: 6.2931e-04\n",
      "Epoch 333/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.5480e-04 - val_loss: 6.2675e-04\n",
      "Epoch 334/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.5337e-04 - val_loss: 7.4472e-04\n",
      "Epoch 335/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.4367e-04 - val_loss: 6.6888e-04\n",
      "Epoch 336/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.7350e-04 - val_loss: 6.2489e-04\n",
      "Epoch 337/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.2090e-04 - val_loss: 6.2528e-04\n",
      "Epoch 338/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 8.8670e-04 - val_loss: 8.1238e-04\n",
      "Epoch 339/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.0361e-04 - val_loss: 7.7532e-04\n",
      "Epoch 340/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.0955e-04 - val_loss: 6.0510e-04\n",
      "Epoch 341/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.6595e-04 - val_loss: 7.0885e-04\n",
      "Epoch 342/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.5947e-04 - val_loss: 5.7916e-04\n",
      "Epoch 343/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.1151e-04 - val_loss: 5.8724e-04\n",
      "Epoch 344/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 8.0699e-04 - val_loss: 7.0677e-04\n",
      "Epoch 345/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.0923e-04 - val_loss: 6.2651e-04\n",
      "Epoch 346/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.5266e-04 - val_loss: 6.2958e-04\n",
      "Epoch 347/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.4318e-04 - val_loss: 6.4929e-04\n",
      "Epoch 348/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.3021e-04 - val_loss: 5.8473e-04\n",
      "Epoch 349/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.1750e-04 - val_loss: 6.5391e-04\n",
      "Epoch 350/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 8.2688e-04 - val_loss: 5.9677e-04\n",
      "Epoch 351/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.4518e-04 - val_loss: 6.7391e-04\n",
      "Epoch 352/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.1400e-04 - val_loss: 6.0154e-04\n",
      "Epoch 353/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.2531e-04 - val_loss: 5.7304e-04\n",
      "Epoch 354/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.6350e-04 - val_loss: 7.1324e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.2857e-04 - val_loss: 6.5056e-04\n",
      "Epoch 356/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.7687e-04 - val_loss: 6.4481e-04\n",
      "Epoch 357/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 7.8288e-04 - val_loss: 6.5756e-04\n",
      "Epoch 358/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.0669e-04 - val_loss: 5.7686e-04\n",
      "Epoch 359/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.7804e-04 - val_loss: 6.3404e-04\n",
      "Epoch 360/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.8383e-04 - val_loss: 5.9724e-04\n",
      "Epoch 361/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.5383e-04 - val_loss: 5.8364e-04\n",
      "Epoch 362/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.7849e-04 - val_loss: 6.4808e-04\n",
      "Epoch 363/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 7.8306e-04 - val_loss: 5.8615e-04\n",
      "Epoch 364/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.3262e-04 - val_loss: 6.2598e-04\n",
      "Epoch 365/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.8082e-04 - val_loss: 5.7105e-04\n",
      "Epoch 366/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.7578e-04 - val_loss: 5.9860e-04\n",
      "Epoch 367/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.1832e-04 - val_loss: 6.1147e-04\n",
      "Epoch 368/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.7345e-04 - val_loss: 5.7861e-04\n",
      "Epoch 369/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 8.0553e-04 - val_loss: 5.8052e-04\n",
      "Epoch 370/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.1673e-04 - val_loss: 6.7363e-04\n",
      "Epoch 371/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.7069e-04 - val_loss: 5.7604e-04\n",
      "Epoch 372/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.0901e-04 - val_loss: 6.1477e-04\n",
      "Epoch 373/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.9607e-04 - val_loss: 8.7348e-04\n",
      "Epoch 374/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.6801e-04 - val_loss: 6.1520e-04\n",
      "Epoch 375/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.7653e-04 - val_loss: 5.4852e-04\n",
      "Epoch 376/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.4464e-04 - val_loss: 6.5722e-04\n",
      "Epoch 377/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.6831e-04 - val_loss: 6.1823e-04\n",
      "Epoch 378/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 8.3215e-04 - val_loss: 5.5760e-04\n",
      "Epoch 379/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.5498e-04 - val_loss: 6.8928e-04\n",
      "Epoch 380/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.6995e-04 - val_loss: 6.4302e-04\n",
      "Epoch 381/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.5418e-04 - val_loss: 7.2610e-04\n",
      "Epoch 382/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.2917e-04 - val_loss: 5.8497e-04\n",
      "Epoch 383/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.9394e-04 - val_loss: 6.4617e-04\n",
      "Epoch 384/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.3325e-04 - val_loss: 6.1274e-04\n",
      "Epoch 385/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.1819e-04 - val_loss: 5.8323e-04\n",
      "Epoch 386/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.7041e-04 - val_loss: 6.5573e-04\n",
      "Epoch 387/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.5379e-04 - val_loss: 5.6718e-04\n",
      "Epoch 388/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.7640e-04 - val_loss: 5.9237e-04\n",
      "Epoch 389/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.6347e-04 - val_loss: 5.7462e-04\n",
      "Epoch 390/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.2581e-04 - val_loss: 6.2915e-04\n",
      "Epoch 391/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 8.2578e-04 - val_loss: 5.9008e-04\n",
      "Epoch 392/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.6580e-04 - val_loss: 6.4309e-04\n",
      "Epoch 393/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.0661e-04 - val_loss: 5.7286e-04\n",
      "Epoch 394/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.2038e-04 - val_loss: 5.7353e-04\n",
      "Epoch 395/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.4979e-04 - val_loss: 6.1128e-04\n",
      "Epoch 396/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.7646e-04 - val_loss: 6.1514e-04\n",
      "Epoch 397/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.7795e-04 - val_loss: 6.0336e-04\n",
      "Epoch 398/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.2548e-04 - val_loss: 5.3641e-04\n",
      "Epoch 399/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9222e-04 - val_loss: 5.6840e-04\n",
      "Epoch 400/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.8085e-04 - val_loss: 5.3575e-04\n",
      "Epoch 401/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.4787e-04 - val_loss: 5.7299e-04\n",
      "Epoch 402/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.6417e-04 - val_loss: 5.8866e-04\n",
      "Epoch 403/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.4347e-04 - val_loss: 6.1177e-04\n",
      "Epoch 404/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.5602e-04 - val_loss: 5.6641e-04\n",
      "Epoch 405/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 8.0469e-04 - val_loss: 6.6610e-04\n",
      "Epoch 406/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.3305e-04 - val_loss: 6.2040e-04\n",
      "Epoch 407/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.4119e-04 - val_loss: 5.8077e-04\n",
      "Epoch 408/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.7521e-04 - val_loss: 6.2724e-04\n",
      "Epoch 409/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.7007e-04 - val_loss: 6.4331e-04\n",
      "Epoch 410/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.9190e-04 - val_loss: 5.8240e-04\n",
      "Epoch 411/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.4841e-04 - val_loss: 6.1923e-04\n",
      "Epoch 412/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.6792e-04 - val_loss: 5.6171e-04\n",
      "Epoch 413/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.5897e-04 - val_loss: 6.1888e-04\n",
      "Epoch 414/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 8.0358e-04 - val_loss: 5.8151e-04\n",
      "Epoch 415/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.4683e-04 - val_loss: 5.9592e-04\n",
      "Epoch 416/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.3421e-04 - val_loss: 6.4423e-04\n",
      "Epoch 417/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9299e-04 - val_loss: 6.3319e-04\n",
      "Epoch 418/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.6285e-04 - val_loss: 5.5926e-04\n",
      "Epoch 419/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.8598e-04 - val_loss: 5.5067e-04\n",
      "Epoch 420/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.9061e-04 - val_loss: 6.1116e-04\n",
      "Epoch 421/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.8730e-04 - val_loss: 5.6750e-04\n",
      "Epoch 422/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.7715e-04 - val_loss: 5.7140e-04\n",
      "Epoch 423/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.3101e-04 - val_loss: 6.1049e-04\n",
      "Epoch 424/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.3921e-04 - val_loss: 5.7702e-04\n",
      "Epoch 425/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.2891e-04 - val_loss: 6.4513e-04\n",
      "Epoch 426/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.9488e-04 - val_loss: 6.4592e-04\n",
      "Epoch 427/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.2106e-04 - val_loss: 6.4485e-04\n",
      "Epoch 428/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.1881e-04 - val_loss: 6.1872e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 429/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.5414e-04 - val_loss: 5.4296e-04\n",
      "Epoch 430/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.6437e-04 - val_loss: 5.5038e-04\n",
      "Epoch 431/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.0688e-04 - val_loss: 5.3613e-04\n",
      "Epoch 432/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.1877e-04 - val_loss: 6.2196e-04\n",
      "Epoch 433/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.1564e-04 - val_loss: 5.9640e-04\n",
      "Epoch 434/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.5197e-04 - val_loss: 5.9132e-04\n",
      "Epoch 435/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.8108e-04 - val_loss: 5.6287e-04\n",
      "Epoch 436/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.2736e-04 - val_loss: 5.7952e-04\n",
      "Epoch 437/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.2420e-04 - val_loss: 5.6552e-04\n",
      "Epoch 438/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.4449e-04 - val_loss: 5.7630e-04\n",
      "Epoch 439/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.3534e-04 - val_loss: 6.4703e-04\n",
      "Epoch 440/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.9087e-04 - val_loss: 6.0237e-04\n",
      "Epoch 441/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.8384e-04 - val_loss: 6.2969e-04\n",
      "Epoch 442/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.9726e-04 - val_loss: 5.9097e-04\n",
      "Epoch 443/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.9614e-04 - val_loss: 5.9075e-04\n",
      "Epoch 444/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 7.3700e-04 - val_loss: 6.0213e-04\n",
      "Epoch 445/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.1851e-04 - val_loss: 6.3850e-04\n",
      "Epoch 446/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.7862e-04 - val_loss: 5.6473e-04\n",
      "Epoch 447/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.7507e-04 - val_loss: 5.6410e-04\n",
      "Epoch 448/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9283e-04 - val_loss: 5.4198e-04\n",
      "Epoch 449/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.4835e-04 - val_loss: 6.1398e-04\n",
      "Epoch 450/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.6162e-04 - val_loss: 5.3799e-04\n",
      "Epoch 451/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9524e-04 - val_loss: 6.1809e-04\n",
      "Epoch 452/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.1899e-04 - val_loss: 5.9171e-04\n",
      "Epoch 453/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.1745e-04 - val_loss: 5.2709e-04\n",
      "Epoch 454/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.1539e-04 - val_loss: 5.4722e-04\n",
      "Epoch 455/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.3529e-04 - val_loss: 5.3710e-04\n",
      "Epoch 456/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.1583e-04 - val_loss: 5.5418e-04\n",
      "Epoch 457/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.5121e-04 - val_loss: 5.8563e-04\n",
      "Epoch 458/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.5832e-04 - val_loss: 5.9849e-04\n",
      "Epoch 459/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 7.1479e-04 - val_loss: 5.3994e-04\n",
      "Epoch 460/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.2530e-04 - val_loss: 5.9073e-04\n",
      "Epoch 461/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.3893e-04 - val_loss: 5.5306e-04\n",
      "Epoch 462/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.1120e-04 - val_loss: 5.8260e-04\n",
      "Epoch 463/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.5153e-04 - val_loss: 5.6617e-04\n",
      "Epoch 464/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.0166e-04 - val_loss: 5.4673e-04\n",
      "Epoch 465/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 7.4699e-04 - val_loss: 5.3386e-04\n",
      "Epoch 466/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.1620e-04 - val_loss: 6.0072e-04\n",
      "Epoch 467/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.4013e-04 - val_loss: 5.5218e-04\n",
      "Epoch 468/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9967e-04 - val_loss: 5.4991e-04\n",
      "Epoch 469/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.1745e-04 - val_loss: 5.9094e-04\n",
      "Epoch 470/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.3614e-04 - val_loss: 5.5225e-04\n",
      "Epoch 471/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9077e-04 - val_loss: 6.0513e-04\n",
      "Epoch 472/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.8298e-04 - val_loss: 6.2486e-04\n",
      "Epoch 473/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.4197e-04 - val_loss: 6.0724e-04\n",
      "Epoch 474/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.3374e-04 - val_loss: 6.4708e-04\n",
      "Epoch 475/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.4361e-04 - val_loss: 6.0952e-04\n",
      "Epoch 476/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 8.1264e-04 - val_loss: 5.7992e-04\n",
      "Epoch 477/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.7339e-04 - val_loss: 6.2753e-04\n",
      "Epoch 478/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 7.2294e-04 - val_loss: 6.7013e-04\n",
      "Epoch 479/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.7075e-04 - val_loss: 5.9539e-04\n",
      "Epoch 480/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.3981e-04 - val_loss: 5.4097e-04\n",
      "Epoch 481/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9853e-04 - val_loss: 5.7134e-04\n",
      "Epoch 482/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.7866e-04 - val_loss: 5.5755e-04\n",
      "Epoch 483/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.5631e-04 - val_loss: 5.6670e-04\n",
      "Epoch 484/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.6040e-04 - val_loss: 5.7980e-04\n",
      "Epoch 485/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.0625e-04 - val_loss: 5.6924e-04\n",
      "Epoch 486/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.1349e-04 - val_loss: 5.8403e-04\n",
      "Epoch 487/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.6522e-04 - val_loss: 5.1598e-04\n",
      "Epoch 488/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.7037e-04 - val_loss: 5.5253e-04\n",
      "Epoch 489/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.0935e-04 - val_loss: 5.5363e-04\n",
      "Epoch 490/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.4670e-04 - val_loss: 5.2596e-04\n",
      "Epoch 491/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.2611e-04 - val_loss: 5.2872e-04\n",
      "Epoch 492/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 7.1098e-04 - val_loss: 5.8469e-04\n",
      "Epoch 493/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.0027e-04 - val_loss: 5.4300e-04\n",
      "Epoch 494/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.3565e-04 - val_loss: 5.9357e-04\n",
      "Epoch 495/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.9384e-04 - val_loss: 5.7185e-04\n",
      "Epoch 496/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.0637e-04 - val_loss: 5.4720e-04\n",
      "Epoch 497/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.7401e-04 - val_loss: 5.8491e-04\n",
      "Epoch 498/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 7.3196e-04 - val_loss: 6.3138e-04\n",
      "Epoch 499/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.8075e-04 - val_loss: 5.4908e-04\n",
      "Epoch 500/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.2401e-04 - val_loss: 6.1769e-04\n",
      "Epoch 501/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.3547e-04 - val_loss: 7.0556e-04\n",
      "Epoch 502/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.4149e-04 - val_loss: 7.5849e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 503/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.6882e-04 - val_loss: 6.4994e-04\n",
      "Epoch 504/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.0667e-04 - val_loss: 6.2216e-04\n",
      "Epoch 505/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.7980e-04 - val_loss: 5.7378e-04\n",
      "Epoch 506/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.3170e-04 - val_loss: 5.5491e-04\n",
      "Epoch 507/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.7629e-04 - val_loss: 5.4113e-04\n",
      "Epoch 508/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.1791e-04 - val_loss: 5.8192e-04\n",
      "Epoch 509/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 7.3782e-04 - val_loss: 5.4114e-04\n",
      "Epoch 510/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.8522e-04 - val_loss: 6.9263e-04\n",
      "Epoch 511/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.7651e-04 - val_loss: 5.6915e-04\n",
      "Epoch 512/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.7374e-04 - val_loss: 5.5907e-04\n",
      "Epoch 513/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.8657e-04 - val_loss: 6.6618e-04\n",
      "Epoch 514/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.2901e-04 - val_loss: 5.3255e-04\n",
      "Epoch 515/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.6456e-04 - val_loss: 5.4085e-04\n",
      "Epoch 516/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.4146e-04 - val_loss: 5.4594e-04\n",
      "Epoch 517/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 7.0754e-04 - val_loss: 5.2475e-04\n",
      "Epoch 518/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9027e-04 - val_loss: 6.7770e-04\n",
      "Epoch 519/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.0472e-04 - val_loss: 5.5489e-04\n",
      "Epoch 520/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.4119e-04 - val_loss: 5.6393e-04\n",
      "Epoch 521/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.9016e-04 - val_loss: 7.0718e-04\n",
      "Epoch 522/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.8814e-04 - val_loss: 5.4727e-04\n",
      "Epoch 523/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.7103e-04 - val_loss: 5.5187e-04\n",
      "Epoch 524/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.7877e-04 - val_loss: 5.3436e-04\n",
      "Epoch 525/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.0894e-04 - val_loss: 5.9951e-04\n",
      "Epoch 526/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.7268e-04 - val_loss: 5.6461e-04\n",
      "Epoch 527/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 7.5681e-04 - val_loss: 5.3244e-04\n",
      "Epoch 528/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9961e-04 - val_loss: 5.1751e-04\n",
      "Epoch 529/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3628e-04 - val_loss: 5.6322e-04\n",
      "Epoch 530/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.3088e-04 - val_loss: 5.5576e-04\n",
      "Epoch 531/1000\n",
      "53/53 [==============================] - 3s 65ms/step - loss: 6.7523e-04 - val_loss: 5.3754e-04\n",
      "Epoch 532/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.1278e-04 - val_loss: 6.3727e-04\n",
      "Epoch 533/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.8433e-04 - val_loss: 5.2642e-04\n",
      "Epoch 534/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.8170e-04 - val_loss: 5.3604e-04\n",
      "Epoch 535/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.3262e-04 - val_loss: 5.8739e-04\n",
      "Epoch 536/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5895e-04 - val_loss: 6.1544e-04\n",
      "Epoch 537/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.6204e-04 - val_loss: 5.8012e-04\n",
      "Epoch 538/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 7.0743e-04 - val_loss: 5.4539e-04\n",
      "Epoch 539/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.8070e-04 - val_loss: 6.1435e-04\n",
      "Epoch 540/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 7.0463e-04 - val_loss: 6.2377e-04\n",
      "Epoch 541/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.5259e-04 - val_loss: 5.2783e-04\n",
      "Epoch 542/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.6316e-04 - val_loss: 5.6015e-04\n",
      "Epoch 543/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 7.1483e-04 - val_loss: 5.5061e-04\n",
      "Epoch 544/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.9910e-04 - val_loss: 5.3438e-04\n",
      "Epoch 545/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.8265e-04 - val_loss: 5.5864e-04\n",
      "Epoch 546/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.9395e-04 - val_loss: 5.5677e-04\n",
      "Epoch 547/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.7793e-04 - val_loss: 6.2779e-04\n",
      "Epoch 548/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.9593e-04 - val_loss: 6.0522e-04\n",
      "Epoch 549/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.7945e-04 - val_loss: 6.4951e-04\n",
      "Epoch 550/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.3385e-04 - val_loss: 5.7086e-04\n",
      "Epoch 551/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.9122e-04 - val_loss: 5.9637e-04\n",
      "Epoch 552/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.7110e-04 - val_loss: 5.6541e-04\n",
      "Epoch 553/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.0641e-04 - val_loss: 6.0868e-04\n",
      "Epoch 554/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.2403e-04 - val_loss: 5.9886e-04\n",
      "Epoch 555/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.7447e-04 - val_loss: 5.4514e-04\n",
      "Epoch 556/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.1328e-04 - val_loss: 5.1655e-04\n",
      "Epoch 557/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.8177e-04 - val_loss: 5.8039e-04\n",
      "Epoch 558/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.8540e-04 - val_loss: 5.5046e-04\n",
      "Epoch 559/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.8733e-04 - val_loss: 5.7300e-04\n",
      "Epoch 560/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.9260e-04 - val_loss: 5.8919e-04\n",
      "Epoch 561/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5074e-04 - val_loss: 5.6371e-04\n",
      "Epoch 562/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.7596e-04 - val_loss: 6.0278e-04\n",
      "Epoch 563/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.9328e-04 - val_loss: 5.4844e-04\n",
      "Epoch 564/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9701e-04 - val_loss: 5.5100e-04\n",
      "Epoch 565/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.6617e-04 - val_loss: 5.2458e-04\n",
      "Epoch 566/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.8652e-04 - val_loss: 5.4034e-04\n",
      "Epoch 567/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.2478e-04 - val_loss: 5.8523e-04\n",
      "Epoch 568/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5073e-04 - val_loss: 5.6333e-04\n",
      "Epoch 569/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.8980e-04 - val_loss: 6.0378e-04\n",
      "Epoch 570/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.2928e-04 - val_loss: 5.1306e-04\n",
      "Epoch 571/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.3029e-04 - val_loss: 5.1987e-04\n",
      "Epoch 572/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.8156e-04 - val_loss: 5.0057e-04\n",
      "Epoch 573/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.7431e-04 - val_loss: 5.2568e-04\n",
      "Epoch 574/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.5851e-04 - val_loss: 5.3852e-04\n",
      "Epoch 575/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.1647e-04 - val_loss: 5.6509e-04\n",
      "Epoch 576/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.6882e-04 - val_loss: 5.9035e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 577/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.6068e-04 - val_loss: 5.4408e-04\n",
      "Epoch 578/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.3550e-04 - val_loss: 5.7618e-04\n",
      "Epoch 579/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.7129e-04 - val_loss: 5.6823e-04\n",
      "Epoch 580/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.5790e-04 - val_loss: 6.0369e-04\n",
      "Epoch 581/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4045e-04 - val_loss: 5.3055e-04\n",
      "Epoch 582/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.8249e-04 - val_loss: 5.1336e-04\n",
      "Epoch 583/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.8650e-04 - val_loss: 5.2191e-04\n",
      "Epoch 584/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.0532e-04 - val_loss: 5.9110e-04\n",
      "Epoch 585/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.0577e-04 - val_loss: 5.9721e-04\n",
      "Epoch 586/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 7.1274e-04 - val_loss: 5.5058e-04\n",
      "Epoch 587/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.8975e-04 - val_loss: 5.4525e-04\n",
      "Epoch 588/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5416e-04 - val_loss: 5.4386e-04\n",
      "Epoch 589/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.0789e-04 - val_loss: 5.3766e-04\n",
      "Epoch 590/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5179e-04 - val_loss: 5.5256e-04\n",
      "Epoch 591/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3329e-04 - val_loss: 5.5911e-04\n",
      "Epoch 592/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.9989e-04 - val_loss: 5.4284e-04\n",
      "Epoch 593/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.7826e-04 - val_loss: 5.3152e-04\n",
      "Epoch 594/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.5236e-04 - val_loss: 5.6477e-04\n",
      "Epoch 595/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 7.4799e-04 - val_loss: 5.2117e-04\n",
      "Epoch 596/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.9826e-04 - val_loss: 5.7867e-04\n",
      "Epoch 597/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.6708e-04 - val_loss: 5.3657e-04\n",
      "Epoch 598/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.2351e-04 - val_loss: 5.8672e-04\n",
      "Epoch 599/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.6174e-04 - val_loss: 5.6849e-04\n",
      "Epoch 600/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.6335e-04 - val_loss: 5.2878e-04\n",
      "Epoch 601/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.6771e-04 - val_loss: 5.4509e-04\n",
      "Epoch 602/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.3674e-04 - val_loss: 5.4496e-04\n",
      "Epoch 603/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 7.1738e-04 - val_loss: 5.3950e-04\n",
      "Epoch 604/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.7574e-04 - val_loss: 5.4304e-04\n",
      "Epoch 605/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4131e-04 - val_loss: 5.5889e-04\n",
      "Epoch 606/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.4279e-04 - val_loss: 5.2617e-04\n",
      "Epoch 607/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 7.4579e-04 - val_loss: 5.4796e-04\n",
      "Epoch 608/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.8567e-04 - val_loss: 5.0929e-04\n",
      "Epoch 609/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9523e-04 - val_loss: 5.3563e-04\n",
      "Epoch 610/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.6885e-04 - val_loss: 5.4646e-04\n",
      "Epoch 611/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.0574e-04 - val_loss: 5.8380e-04\n",
      "Epoch 612/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5256e-04 - val_loss: 5.4705e-04\n",
      "Epoch 613/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.6514e-04 - val_loss: 5.5221e-04\n",
      "Epoch 614/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5714e-04 - val_loss: 5.5123e-04\n",
      "Epoch 615/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.8603e-04 - val_loss: 5.1241e-04\n",
      "Epoch 616/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9396e-04 - val_loss: 5.4235e-04\n",
      "Epoch 617/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.1862e-04 - val_loss: 5.6895e-04\n",
      "Epoch 618/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2716e-04 - val_loss: 5.1409e-04\n",
      "Epoch 619/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.3898e-04 - val_loss: 5.0731e-04\n",
      "Epoch 620/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.8978e-04 - val_loss: 5.4264e-04\n",
      "Epoch 621/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 7.0533e-04 - val_loss: 5.4665e-04\n",
      "Epoch 622/1000\n",
      "53/53 [==============================] - 3s 65ms/step - loss: 6.9268e-04 - val_loss: 5.7678e-04\n",
      "Epoch 623/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.5427e-04 - val_loss: 5.9067e-04\n",
      "Epoch 624/1000\n",
      "53/53 [==============================] - 3s 65ms/step - loss: 7.2738e-04 - val_loss: 5.0081e-04\n",
      "Epoch 625/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.5611e-04 - val_loss: 5.7772e-04\n",
      "Epoch 626/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.7603e-04 - val_loss: 5.8301e-04\n",
      "Epoch 627/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.2624e-04 - val_loss: 5.0731e-04\n",
      "Epoch 628/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.6834e-04 - val_loss: 4.9949e-04\n",
      "Epoch 629/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.8665e-04 - val_loss: 5.4559e-04\n",
      "Epoch 630/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2889e-04 - val_loss: 6.5189e-04\n",
      "Epoch 631/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.8880e-04 - val_loss: 5.5482e-04\n",
      "Epoch 632/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.8156e-04 - val_loss: 5.9687e-04\n",
      "Epoch 633/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3549e-04 - val_loss: 5.3634e-04\n",
      "Epoch 634/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3927e-04 - val_loss: 5.2144e-04\n",
      "Epoch 635/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.6799e-04 - val_loss: 5.4528e-04\n",
      "Epoch 636/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.6223e-04 - val_loss: 6.6514e-04\n",
      "Epoch 637/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.6233e-04 - val_loss: 5.0748e-04\n",
      "Epoch 638/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.3762e-04 - val_loss: 5.2441e-04\n",
      "Epoch 639/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5886e-04 - val_loss: 5.7359e-04\n",
      "Epoch 640/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.9501e-04 - val_loss: 5.1526e-04\n",
      "Epoch 641/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5919e-04 - val_loss: 6.1281e-04\n",
      "Epoch 642/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5805e-04 - val_loss: 5.4846e-04\n",
      "Epoch 643/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.2725e-04 - val_loss: 4.8477e-04\n",
      "Epoch 644/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.5393e-04 - val_loss: 5.3137e-04\n",
      "Epoch 645/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5533e-04 - val_loss: 5.4131e-04\n",
      "Epoch 646/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4627e-04 - val_loss: 5.8200e-04\n",
      "Epoch 647/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.5863e-04 - val_loss: 5.1653e-04\n",
      "Epoch 648/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.5987e-04 - val_loss: 5.7839e-04\n",
      "Epoch 649/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.6796e-04 - val_loss: 5.4687e-04\n",
      "Epoch 650/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.5122e-04 - val_loss: 5.6646e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 651/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.9443e-04 - val_loss: 5.6430e-04\n",
      "Epoch 652/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.7084e-04 - val_loss: 6.0100e-04\n",
      "Epoch 653/1000\n",
      "53/53 [==============================] - 4s 69ms/step - loss: 6.2322e-04 - val_loss: 5.2125e-04\n",
      "Epoch 654/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.0745e-04 - val_loss: 5.7572e-04\n",
      "Epoch 655/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.6815e-04 - val_loss: 5.1917e-04\n",
      "Epoch 656/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.9817e-04 - val_loss: 5.5851e-04\n",
      "Epoch 657/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.9458e-04 - val_loss: 6.0672e-04\n",
      "Epoch 658/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.6563e-04 - val_loss: 5.1672e-04\n",
      "Epoch 659/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3326e-04 - val_loss: 5.4680e-04\n",
      "Epoch 660/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5978e-04 - val_loss: 4.9240e-04\n",
      "Epoch 661/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3276e-04 - val_loss: 5.2702e-04\n",
      "Epoch 662/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.0551e-04 - val_loss: 4.9885e-04\n",
      "Epoch 663/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.7231e-04 - val_loss: 5.1475e-04\n",
      "Epoch 664/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.2193e-04 - val_loss: 6.0032e-04\n",
      "Epoch 665/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.7903e-04 - val_loss: 5.3429e-04\n",
      "Epoch 666/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.4869e-04 - val_loss: 5.4066e-04\n",
      "Epoch 667/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.3187e-04 - val_loss: 5.4949e-04\n",
      "Epoch 668/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.3028e-04 - val_loss: 5.1986e-04\n",
      "Epoch 669/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5810e-04 - val_loss: 6.6690e-04\n",
      "Epoch 670/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.7046e-04 - val_loss: 5.7315e-04\n",
      "Epoch 671/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.3218e-04 - val_loss: 5.2056e-04\n",
      "Epoch 672/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.8197e-04 - val_loss: 5.6890e-04\n",
      "Epoch 673/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.2612e-04 - val_loss: 5.3265e-04\n",
      "Epoch 674/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.1399e-04 - val_loss: 5.2924e-04\n",
      "Epoch 675/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3519e-04 - val_loss: 5.9824e-04\n",
      "Epoch 676/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.1441e-04 - val_loss: 5.5891e-04\n",
      "Epoch 677/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.5867e-04 - val_loss: 5.3417e-04\n",
      "Epoch 678/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5576e-04 - val_loss: 5.4893e-04\n",
      "Epoch 679/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2756e-04 - val_loss: 5.5525e-04\n",
      "Epoch 680/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.6969e-04 - val_loss: 5.3374e-04\n",
      "Epoch 681/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.2113e-04 - val_loss: 5.3036e-04\n",
      "Epoch 682/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.2968e-04 - val_loss: 5.2163e-04\n",
      "Epoch 683/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.7625e-04 - val_loss: 5.6431e-04\n",
      "Epoch 684/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.6001e-04 - val_loss: 4.8951e-04\n",
      "Epoch 685/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.8521e-04 - val_loss: 5.3096e-04\n",
      "Epoch 686/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.7762e-04 - val_loss: 4.9924e-04\n",
      "Epoch 687/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5822e-04 - val_loss: 5.2079e-04\n",
      "Epoch 688/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.0480e-04 - val_loss: 5.3290e-04\n",
      "Epoch 689/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1206e-04 - val_loss: 5.3830e-04\n",
      "Epoch 690/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.9968e-04 - val_loss: 6.0045e-04\n",
      "Epoch 691/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.4711e-04 - val_loss: 5.6677e-04\n",
      "Epoch 692/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.2602e-04 - val_loss: 5.0320e-04\n",
      "Epoch 693/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4298e-04 - val_loss: 5.7319e-04\n",
      "Epoch 694/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2311e-04 - val_loss: 5.4018e-04\n",
      "Epoch 695/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.9367e-04 - val_loss: 6.2045e-04\n",
      "Epoch 696/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3875e-04 - val_loss: 5.2218e-04\n",
      "Epoch 697/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.0021e-04 - val_loss: 5.0472e-04\n",
      "Epoch 698/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3923e-04 - val_loss: 5.4170e-04\n",
      "Epoch 699/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4996e-04 - val_loss: 5.4300e-04\n",
      "Epoch 700/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5546e-04 - val_loss: 5.1758e-04\n",
      "Epoch 701/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4243e-04 - val_loss: 5.2896e-04\n",
      "Epoch 702/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.6908e-04 - val_loss: 5.4772e-04\n",
      "Epoch 703/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.4516e-04 - val_loss: 5.7103e-04\n",
      "Epoch 704/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5358e-04 - val_loss: 5.1958e-04\n",
      "Epoch 705/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.4472e-04 - val_loss: 5.2334e-04\n",
      "Epoch 706/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.9968e-04 - val_loss: 5.2992e-04\n",
      "Epoch 707/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.7439e-04 - val_loss: 5.5026e-04\n",
      "Epoch 708/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3126e-04 - val_loss: 5.8325e-04\n",
      "Epoch 709/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1824e-04 - val_loss: 5.2654e-04\n",
      "Epoch 710/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3276e-04 - val_loss: 5.2726e-04\n",
      "Epoch 711/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5470e-04 - val_loss: 5.1539e-04\n",
      "Epoch 712/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5668e-04 - val_loss: 5.7380e-04\n",
      "Epoch 713/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.3412e-04 - val_loss: 5.1454e-04\n",
      "Epoch 714/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1431e-04 - val_loss: 5.8923e-04\n",
      "Epoch 715/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.6743e-04 - val_loss: 6.4154e-04\n",
      "Epoch 716/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 7.1329e-04 - val_loss: 5.0293e-04\n",
      "Epoch 717/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.5726e-04 - val_loss: 5.0475e-04\n",
      "Epoch 718/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.0210e-04 - val_loss: 5.2152e-04\n",
      "Epoch 719/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.5815e-04 - val_loss: 5.7522e-04\n",
      "Epoch 720/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3754e-04 - val_loss: 5.6225e-04\n",
      "Epoch 721/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.8052e-04 - val_loss: 5.8320e-04\n",
      "Epoch 722/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1085e-04 - val_loss: 5.2271e-04\n",
      "Epoch 723/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4688e-04 - val_loss: 5.4564e-04\n",
      "Epoch 724/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.5328e-04 - val_loss: 5.2587e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 725/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.8667e-04 - val_loss: 5.6879e-04\n",
      "Epoch 726/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3583e-04 - val_loss: 5.3998e-04\n",
      "Epoch 727/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.0868e-04 - val_loss: 5.0521e-04\n",
      "Epoch 728/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1328e-04 - val_loss: 5.1408e-04\n",
      "Epoch 729/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.9892e-04 - val_loss: 6.2584e-04\n",
      "Epoch 730/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.9671e-04 - val_loss: 5.5933e-04\n",
      "Epoch 731/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4514e-04 - val_loss: 5.3412e-04\n",
      "Epoch 732/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.9535e-04 - val_loss: 4.9103e-04\n",
      "Epoch 733/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4092e-04 - val_loss: 5.4252e-04\n",
      "Epoch 734/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.5916e-04 - val_loss: 4.9554e-04\n",
      "Epoch 735/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.6395e-04 - val_loss: 6.0441e-04\n",
      "Epoch 736/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.7287e-04 - val_loss: 4.9722e-04\n",
      "Epoch 737/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.5530e-04 - val_loss: 5.0836e-04\n",
      "Epoch 738/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4940e-04 - val_loss: 5.2225e-04\n",
      "Epoch 739/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3617e-04 - val_loss: 6.1019e-04\n",
      "Epoch 740/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.9403e-04 - val_loss: 5.5899e-04\n",
      "Epoch 741/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1605e-04 - val_loss: 5.9012e-04\n",
      "Epoch 742/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.8953e-04 - val_loss: 6.0695e-04\n",
      "Epoch 743/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.1747e-04 - val_loss: 5.2912e-04\n",
      "Epoch 744/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4041e-04 - val_loss: 5.3918e-04\n",
      "Epoch 745/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5047e-04 - val_loss: 4.9997e-04\n",
      "Epoch 746/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1755e-04 - val_loss: 6.4910e-04\n",
      "Epoch 747/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.3745e-04 - val_loss: 5.2530e-04\n",
      "Epoch 748/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3040e-04 - val_loss: 5.2291e-04\n",
      "Epoch 749/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2551e-04 - val_loss: 4.9483e-04\n",
      "Epoch 750/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4479e-04 - val_loss: 5.2929e-04\n",
      "Epoch 751/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.5398e-04 - val_loss: 5.0956e-04\n",
      "Epoch 752/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 7.0690e-04 - val_loss: 4.9987e-04\n",
      "Epoch 753/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.9352e-04 - val_loss: 5.2959e-04\n",
      "Epoch 754/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.0843e-04 - val_loss: 5.2126e-04\n",
      "Epoch 755/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.8617e-04 - val_loss: 5.3028e-04\n",
      "Epoch 756/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.6821e-04 - val_loss: 5.3440e-04\n",
      "Epoch 757/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.3234e-04 - val_loss: 5.9685e-04\n",
      "Epoch 758/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.0572e-04 - val_loss: 5.1868e-04\n",
      "Epoch 759/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1197e-04 - val_loss: 5.4168e-04\n",
      "Epoch 760/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3706e-04 - val_loss: 5.7929e-04\n",
      "Epoch 761/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.0327e-04 - val_loss: 5.2240e-04\n",
      "Epoch 762/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1692e-04 - val_loss: 5.0581e-04\n",
      "Epoch 763/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.9936e-04 - val_loss: 5.5922e-04\n",
      "Epoch 764/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2266e-04 - val_loss: 5.6663e-04\n",
      "Epoch 765/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4070e-04 - val_loss: 4.9166e-04\n",
      "Epoch 766/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4808e-04 - val_loss: 5.1477e-04\n",
      "Epoch 767/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3765e-04 - val_loss: 5.4945e-04\n",
      "Epoch 768/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.4843e-04 - val_loss: 5.2821e-04\n",
      "Epoch 769/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.2337e-04 - val_loss: 5.1629e-04\n",
      "Epoch 770/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1153e-04 - val_loss: 5.6548e-04\n",
      "Epoch 771/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4167e-04 - val_loss: 5.2605e-04\n",
      "Epoch 772/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3133e-04 - val_loss: 5.6775e-04\n",
      "Epoch 773/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.8022e-04 - val_loss: 5.1495e-04\n",
      "Epoch 774/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.6347e-04 - val_loss: 4.9308e-04\n",
      "Epoch 775/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.2096e-04 - val_loss: 5.7185e-04\n",
      "Epoch 776/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2288e-04 - val_loss: 5.2903e-04\n",
      "Epoch 777/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.0533e-04 - val_loss: 5.3131e-04\n",
      "Epoch 778/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.0057e-04 - val_loss: 5.1948e-04\n",
      "Epoch 779/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.7196e-04 - val_loss: 4.9608e-04\n",
      "Epoch 780/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.6798e-04 - val_loss: 5.4748e-04\n",
      "Epoch 781/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.7909e-04 - val_loss: 5.4407e-04\n",
      "Epoch 782/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.9937e-04 - val_loss: 5.3328e-04\n",
      "Epoch 783/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.1863e-04 - val_loss: 5.2503e-04\n",
      "Epoch 784/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.8770e-04 - val_loss: 5.5105e-04\n",
      "Epoch 785/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.7434e-04 - val_loss: 5.5851e-04\n",
      "Epoch 786/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.9708e-04 - val_loss: 5.3737e-04\n",
      "Epoch 787/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5754e-04 - val_loss: 5.1084e-04\n",
      "Epoch 788/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.1638e-04 - val_loss: 4.8917e-04\n",
      "Epoch 789/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.7068e-04 - val_loss: 5.3072e-04\n",
      "Epoch 790/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1747e-04 - val_loss: 5.7146e-04\n",
      "Epoch 791/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.9918e-04 - val_loss: 5.0627e-04\n",
      "Epoch 792/1000\n",
      "53/53 [==============================] - 3s 65ms/step - loss: 6.5746e-04 - val_loss: 5.0556e-04\n",
      "Epoch 793/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.8458e-04 - val_loss: 4.8810e-04\n",
      "Epoch 794/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.1716e-04 - val_loss: 5.2059e-04\n",
      "Epoch 795/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.7049e-04 - val_loss: 5.4166e-04\n",
      "Epoch 796/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.2296e-04 - val_loss: 6.8179e-04\n",
      "Epoch 797/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.7696e-04 - val_loss: 5.6418e-04\n",
      "Epoch 798/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4902e-04 - val_loss: 5.5035e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 799/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2515e-04 - val_loss: 5.8378e-04\n",
      "Epoch 800/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3166e-04 - val_loss: 5.3699e-04\n",
      "Epoch 801/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.1252e-04 - val_loss: 5.5929e-04\n",
      "Epoch 802/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.1310e-04 - val_loss: 4.9362e-04\n",
      "Epoch 803/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.0280e-04 - val_loss: 5.3059e-04\n",
      "Epoch 804/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.0784e-04 - val_loss: 5.4693e-04\n",
      "Epoch 805/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.6043e-04 - val_loss: 5.9598e-04\n",
      "Epoch 806/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.1509e-04 - val_loss: 5.3958e-04\n",
      "Epoch 807/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4495e-04 - val_loss: 5.2453e-04\n",
      "Epoch 808/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.9727e-04 - val_loss: 5.1369e-04\n",
      "Epoch 809/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1413e-04 - val_loss: 5.0276e-04\n",
      "Epoch 810/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.1276e-04 - val_loss: 5.4527e-04\n",
      "Epoch 811/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.1011e-04 - val_loss: 5.3776e-04\n",
      "Epoch 812/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.2538e-04 - val_loss: 5.1182e-04\n",
      "Epoch 813/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.3951e-04 - val_loss: 5.6185e-04\n",
      "Epoch 814/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.0041e-04 - val_loss: 5.0770e-04\n",
      "Epoch 815/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.0024e-04 - val_loss: 5.3188e-04\n",
      "Epoch 816/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 5.8521e-04 - val_loss: 5.5406e-04\n",
      "Epoch 817/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1589e-04 - val_loss: 5.2534e-04\n",
      "Epoch 818/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.4923e-04 - val_loss: 5.2277e-04\n",
      "Epoch 819/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3799e-04 - val_loss: 5.5675e-04\n",
      "Epoch 820/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1957e-04 - val_loss: 6.1966e-04\n",
      "Epoch 821/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 5.9279e-04 - val_loss: 5.3512e-04\n",
      "Epoch 822/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1570e-04 - val_loss: 5.3953e-04\n",
      "Epoch 823/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4483e-04 - val_loss: 5.3657e-04\n",
      "Epoch 824/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1596e-04 - val_loss: 5.0492e-04\n",
      "Epoch 825/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4793e-04 - val_loss: 5.2383e-04\n",
      "Epoch 826/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3394e-04 - val_loss: 4.9956e-04\n",
      "Epoch 827/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.1256e-04 - val_loss: 5.4543e-04\n",
      "Epoch 828/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.5056e-04 - val_loss: 5.1119e-04\n",
      "Epoch 829/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.7115e-04 - val_loss: 6.3193e-04\n",
      "Epoch 830/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.6968e-04 - val_loss: 5.7006e-04\n",
      "Epoch 831/1000\n",
      "53/53 [==============================] - 3s 65ms/step - loss: 6.6176e-04 - val_loss: 5.5240e-04\n",
      "Epoch 832/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2018e-04 - val_loss: 5.2537e-04\n",
      "Epoch 833/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.3025e-04 - val_loss: 5.1570e-04\n",
      "Epoch 834/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.8626e-04 - val_loss: 5.0051e-04\n",
      "Epoch 835/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2620e-04 - val_loss: 5.3026e-04\n",
      "Epoch 836/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.1545e-04 - val_loss: 5.3221e-04\n",
      "Epoch 837/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.3585e-04 - val_loss: 4.8762e-04\n",
      "Epoch 838/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2948e-04 - val_loss: 5.2982e-04\n",
      "Epoch 839/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3778e-04 - val_loss: 5.5942e-04\n",
      "Epoch 840/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.8751e-04 - val_loss: 6.5480e-04\n",
      "Epoch 841/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.6796e-04 - val_loss: 5.0285e-04\n",
      "Epoch 842/1000\n",
      "53/53 [==============================] - 3s 65ms/step - loss: 6.4820e-04 - val_loss: 5.5001e-04\n",
      "Epoch 843/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.6655e-04 - val_loss: 6.0983e-04\n",
      "Epoch 844/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.2971e-04 - val_loss: 5.0758e-04\n",
      "Epoch 845/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.0144e-04 - val_loss: 5.0572e-04\n",
      "Epoch 846/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.0635e-04 - val_loss: 4.8738e-04\n",
      "Epoch 847/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.9720e-04 - val_loss: 5.5736e-04\n",
      "Epoch 848/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.6170e-04 - val_loss: 4.9814e-04\n",
      "Epoch 849/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.0653e-04 - val_loss: 5.1017e-04\n",
      "Epoch 850/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.2702e-04 - val_loss: 5.3238e-04\n",
      "Epoch 851/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.6084e-04 - val_loss: 5.9867e-04\n",
      "Epoch 852/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.8674e-04 - val_loss: 5.3175e-04\n",
      "Epoch 853/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2547e-04 - val_loss: 4.7945e-04\n",
      "Epoch 854/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.3769e-04 - val_loss: 4.9041e-04\n",
      "Epoch 855/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4951e-04 - val_loss: 5.3245e-04\n",
      "Epoch 856/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3334e-04 - val_loss: 4.9247e-04\n",
      "Epoch 857/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.2725e-04 - val_loss: 5.8434e-04\n",
      "Epoch 858/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2224e-04 - val_loss: 5.3486e-04\n",
      "Epoch 859/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 5.8148e-04 - val_loss: 5.0020e-04\n",
      "Epoch 860/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5196e-04 - val_loss: 5.4138e-04\n",
      "Epoch 861/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1217e-04 - val_loss: 6.1457e-04\n",
      "Epoch 862/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.0516e-04 - val_loss: 5.1495e-04\n",
      "Epoch 863/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.6915e-04 - val_loss: 5.2544e-04\n",
      "Epoch 864/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.2293e-04 - val_loss: 5.3969e-04\n",
      "Epoch 865/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.6405e-04 - val_loss: 5.4251e-04\n",
      "Epoch 866/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.3552e-04 - val_loss: 4.9955e-04\n",
      "Epoch 867/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4688e-04 - val_loss: 5.8933e-04\n",
      "Epoch 868/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.8105e-04 - val_loss: 5.3283e-04\n",
      "Epoch 869/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.8092e-04 - val_loss: 5.0558e-04\n",
      "Epoch 870/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4243e-04 - val_loss: 5.0811e-04\n",
      "Epoch 871/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.8579e-04 - val_loss: 5.0874e-04\n",
      "Epoch 872/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.2935e-04 - val_loss: 5.3921e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 873/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 7.3660e-04 - val_loss: 5.8823e-04\n",
      "Epoch 874/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 7.1092e-04 - val_loss: 5.1280e-04\n",
      "Epoch 875/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.6798e-04 - val_loss: 5.5089e-04\n",
      "Epoch 876/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.0328e-04 - val_loss: 5.4285e-04\n",
      "Epoch 877/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1770e-04 - val_loss: 5.2588e-04\n",
      "Epoch 878/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.3136e-04 - val_loss: 5.1849e-04\n",
      "Epoch 879/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 5.9062e-04 - val_loss: 5.0498e-04\n",
      "Epoch 880/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2798e-04 - val_loss: 5.0267e-04\n",
      "Epoch 881/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.9115e-04 - val_loss: 5.4099e-04\n",
      "Epoch 882/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.0101e-04 - val_loss: 4.9104e-04\n",
      "Epoch 883/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.3795e-04 - val_loss: 4.8865e-04\n",
      "Epoch 884/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 5.7945e-04 - val_loss: 5.3295e-04\n",
      "Epoch 885/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.0569e-04 - val_loss: 5.1880e-04\n",
      "Epoch 886/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1601e-04 - val_loss: 5.7898e-04\n",
      "Epoch 887/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.1398e-04 - val_loss: 5.1072e-04\n",
      "Epoch 888/1000\n",
      "53/53 [==============================] - 4s 70ms/step - loss: 6.3906e-04 - val_loss: 4.9665e-04\n",
      "Epoch 889/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.9780e-04 - val_loss: 5.1947e-04\n",
      "Epoch 890/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.9377e-04 - val_loss: 4.9671e-04\n",
      "Epoch 891/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.1726e-04 - val_loss: 4.9965e-04\n",
      "Epoch 892/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3486e-04 - val_loss: 5.5911e-04\n",
      "Epoch 893/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.4017e-04 - val_loss: 4.8025e-04\n",
      "Epoch 894/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3213e-04 - val_loss: 5.7971e-04\n",
      "Epoch 895/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.1574e-04 - val_loss: 5.5712e-04\n",
      "Epoch 896/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.1027e-04 - val_loss: 4.9382e-04\n",
      "Epoch 897/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.2583e-04 - val_loss: 4.9191e-04\n",
      "Epoch 898/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2317e-04 - val_loss: 5.1117e-04\n",
      "Epoch 899/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3325e-04 - val_loss: 5.3190e-04\n",
      "Epoch 900/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.7995e-04 - val_loss: 5.0600e-04\n",
      "Epoch 901/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.0012e-04 - val_loss: 4.9394e-04\n",
      "Epoch 902/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.1516e-04 - val_loss: 5.2682e-04\n",
      "Epoch 903/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.0807e-04 - val_loss: 5.8238e-04\n",
      "Epoch 904/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.4719e-04 - val_loss: 5.5792e-04\n",
      "Epoch 905/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2621e-04 - val_loss: 5.2274e-04\n",
      "Epoch 906/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.0951e-04 - val_loss: 6.2813e-04\n",
      "Epoch 907/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.5611e-04 - val_loss: 5.0571e-04\n",
      "Epoch 908/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2376e-04 - val_loss: 5.1278e-04\n",
      "Epoch 909/1000\n",
      "53/53 [==============================] - 3s 65ms/step - loss: 6.3458e-04 - val_loss: 5.6346e-04\n",
      "Epoch 910/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.3954e-04 - val_loss: 5.7954e-04\n",
      "Epoch 911/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5279e-04 - val_loss: 5.3404e-04\n",
      "Epoch 912/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3980e-04 - val_loss: 5.3808e-04\n",
      "Epoch 913/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.0319e-04 - val_loss: 5.1537e-04\n",
      "Epoch 914/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.9357e-04 - val_loss: 5.9424e-04\n",
      "Epoch 915/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3700e-04 - val_loss: 5.5237e-04\n",
      "Epoch 916/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.5659e-04 - val_loss: 5.1195e-04\n",
      "Epoch 917/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.4514e-04 - val_loss: 5.4555e-04\n",
      "Epoch 918/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.3066e-04 - val_loss: 5.4954e-04\n",
      "Epoch 919/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.1974e-04 - val_loss: 5.0641e-04\n",
      "Epoch 920/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.8928e-04 - val_loss: 5.2414e-04\n",
      "Epoch 921/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3979e-04 - val_loss: 5.3169e-04\n",
      "Epoch 922/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 5.7782e-04 - val_loss: 5.4445e-04\n",
      "Epoch 923/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.1695e-04 - val_loss: 5.0223e-04\n",
      "Epoch 924/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 5.8703e-04 - val_loss: 5.6690e-04\n",
      "Epoch 925/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.4293e-04 - val_loss: 5.4164e-04\n",
      "Epoch 926/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.0940e-04 - val_loss: 5.0652e-04\n",
      "Epoch 927/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 5.8864e-04 - val_loss: 4.9332e-04\n",
      "Epoch 928/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.7979e-04 - val_loss: 5.0610e-04\n",
      "Epoch 929/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1896e-04 - val_loss: 4.9813e-04\n",
      "Epoch 930/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.7196e-04 - val_loss: 5.2170e-04\n",
      "Epoch 931/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.2574e-04 - val_loss: 5.3117e-04\n",
      "Epoch 932/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.5350e-04 - val_loss: 5.7686e-04\n",
      "Epoch 933/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.7590e-04 - val_loss: 5.3841e-04\n",
      "Epoch 934/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4966e-04 - val_loss: 5.1836e-04\n",
      "Epoch 935/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.9938e-04 - val_loss: 5.2889e-04\n",
      "Epoch 936/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5294e-04 - val_loss: 5.8455e-04\n",
      "Epoch 937/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.7905e-04 - val_loss: 5.5229e-04\n",
      "Epoch 938/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.1855e-04 - val_loss: 5.3650e-04\n",
      "Epoch 939/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.6223e-04 - val_loss: 5.5743e-04\n",
      "Epoch 940/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.2548e-04 - val_loss: 5.4763e-04\n",
      "Epoch 941/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2572e-04 - val_loss: 5.1950e-04\n",
      "Epoch 942/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1294e-04 - val_loss: 5.1230e-04\n",
      "Epoch 943/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.2748e-04 - val_loss: 5.5533e-04\n",
      "Epoch 944/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.8278e-04 - val_loss: 5.1721e-04\n",
      "Epoch 945/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.5370e-04 - val_loss: 4.9971e-04\n",
      "Epoch 946/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1932e-04 - val_loss: 5.2049e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 947/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.6492e-04 - val_loss: 5.1068e-04\n",
      "Epoch 948/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3544e-04 - val_loss: 5.4737e-04\n",
      "Epoch 949/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.6334e-04 - val_loss: 5.0918e-04\n",
      "Epoch 950/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.4578e-04 - val_loss: 5.4008e-04\n",
      "Epoch 951/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.6132e-04 - val_loss: 4.9925e-04\n",
      "Epoch 952/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.1685e-04 - val_loss: 5.3254e-04\n",
      "Epoch 953/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.9008e-04 - val_loss: 5.1157e-04\n",
      "Epoch 954/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4459e-04 - val_loss: 5.2415e-04\n",
      "Epoch 955/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.2379e-04 - val_loss: 5.0962e-04\n",
      "Epoch 956/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1693e-04 - val_loss: 4.9697e-04\n",
      "Epoch 957/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.9558e-04 - val_loss: 4.9753e-04\n",
      "Epoch 958/1000\n",
      "53/53 [==============================] - 3s 65ms/step - loss: 6.4244e-04 - val_loss: 5.0597e-04\n",
      "Epoch 959/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.5996e-04 - val_loss: 5.1183e-04\n",
      "Epoch 960/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2343e-04 - val_loss: 5.2293e-04\n",
      "Epoch 961/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.8338e-04 - val_loss: 5.0799e-04\n",
      "Epoch 962/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 5.9568e-04 - val_loss: 5.1395e-04\n",
      "Epoch 963/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.7741e-04 - val_loss: 5.0664e-04\n",
      "Epoch 964/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.7098e-04 - val_loss: 6.2796e-04\n",
      "Epoch 965/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1740e-04 - val_loss: 5.0664e-04\n",
      "Epoch 966/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.8199e-04 - val_loss: 5.1704e-04\n",
      "Epoch 967/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.2936e-04 - val_loss: 6.2353e-04\n",
      "Epoch 968/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4097e-04 - val_loss: 5.6488e-04\n",
      "Epoch 969/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.9131e-04 - val_loss: 5.6366e-04\n",
      "Epoch 970/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.8056e-04 - val_loss: 5.2364e-04\n",
      "Epoch 971/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.6490e-04 - val_loss: 5.1060e-04\n",
      "Epoch 972/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.2389e-04 - val_loss: 5.1191e-04\n",
      "Epoch 973/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.0642e-04 - val_loss: 5.3392e-04\n",
      "Epoch 974/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 5.8176e-04 - val_loss: 4.9106e-04\n",
      "Epoch 975/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3267e-04 - val_loss: 4.9990e-04\n",
      "Epoch 976/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.2724e-04 - val_loss: 5.6293e-04\n",
      "Epoch 977/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.3890e-04 - val_loss: 5.1188e-04\n",
      "Epoch 978/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.7714e-04 - val_loss: 4.8714e-04\n",
      "Epoch 979/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.5970e-04 - val_loss: 5.4270e-04\n",
      "Epoch 980/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.9447e-04 - val_loss: 5.5450e-04\n",
      "Epoch 981/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 6.0691e-04 - val_loss: 5.1561e-04\n",
      "Epoch 982/1000\n",
      "53/53 [==============================] - 4s 66ms/step - loss: 5.5992e-04 - val_loss: 5.2243e-04\n",
      "Epoch 983/1000\n",
      "53/53 [==============================] - 3s 66ms/step - loss: 6.1413e-04 - val_loss: 5.1727e-04\n",
      "Epoch 984/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.7925e-04 - val_loss: 5.1973e-04\n",
      "Epoch 985/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.2947e-04 - val_loss: 5.1467e-04\n",
      "Epoch 986/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.6333e-04 - val_loss: 5.1576e-04\n",
      "Epoch 987/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.6241e-04 - val_loss: 4.8621e-04\n",
      "Epoch 988/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.1169e-04 - val_loss: 4.8337e-04\n",
      "Epoch 989/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.8466e-04 - val_loss: 5.7596e-04\n",
      "Epoch 990/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.8826e-04 - val_loss: 5.4494e-04\n",
      "Epoch 991/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.9243e-04 - val_loss: 5.0122e-04\n",
      "Epoch 992/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.1833e-04 - val_loss: 5.1780e-04\n",
      "Epoch 993/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4787e-04 - val_loss: 5.2217e-04\n",
      "Epoch 994/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.9481e-04 - val_loss: 5.0831e-04\n",
      "Epoch 995/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.9428e-04 - val_loss: 5.0457e-04\n",
      "Epoch 996/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.6707e-04 - val_loss: 5.3276e-04\n",
      "Epoch 997/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 5.9622e-04 - val_loss: 5.1326e-04\n",
      "Epoch 998/1000\n",
      "53/53 [==============================] - 4s 68ms/step - loss: 6.5200e-04 - val_loss: 4.9950e-04\n",
      "Epoch 999/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 5.8217e-04 - val_loss: 5.7699e-04\n",
      "Epoch 1000/1000\n",
      "53/53 [==============================] - 4s 67ms/step - loss: 6.4255e-04 - val_loss: 5.7562e-04\n"
     ]
    }
   ],
   "source": [
    "g1_model3_adam = Sequential()\n",
    "g1_model3_adam.add(Conv2D(32,\n",
    "                 (6, 6),\n",
    "                 activation='relu',\n",
    "                input_shape=INPUT_SHAPE))\n",
    "g1_model3_adam.add(BatchNormalization())\n",
    "g1_model3_adam.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "g1_model3_adam.add(Conv2D(filters=64,\n",
    "                 kernel_size=(5, 5),\n",
    "                 activation='relu'))\n",
    "g1_model3_adam.add(BatchNormalization())\n",
    "g1_model3_adam.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "g1_model3_adam.add(Conv2D(filters=256,\n",
    "                 kernel_size=(4, 4),\n",
    "                 activation='relu'))\n",
    "g1_model3_adam.add(BatchNormalization())\n",
    "g1_model3_adam.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "g1_model3_adam.add(Conv2D(filters=64,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "g1_model3_adam.add(BatchNormalization())\n",
    "g1_model3_adam.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "g1_model3_adam.add(Conv2D(filters=128,\n",
    "                 kernel_size=(2, 2),\n",
    "                 activation='relu'))\n",
    "g1_model3_adam.add(BatchNormalization())\n",
    "g1_model3_adam.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "g1_model3_adam.add(Flatten())\n",
    "g1_model3_adam.add(Dense(500, activation = \"relu\"))\n",
    "g1_model3_adam.add(BatchNormalization())\n",
    "g1_model3_adam.add(Dense(500, activation = \"relu\"))\n",
    "g1_model3_adam.add(BatchNormalization())\n",
    "g1_model3_adam.add(Dropout(.3))\n",
    "g1_model3_adam.add(Dense(30))\n",
    "print(g1_model3_adam.summary())\n",
    "g1_CNN_aug_addedLayers_hist_adam, g1_model3_new_adam = fit_model(g1_model3_adam, data1,'g1_CNN_aug_addedLayers_adam',\n",
    "                                                       datagen, patience=1000, retrain = True, optimizer= 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 91, 91, 32)        1184      \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 91, 91, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 45, 45, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 41, 41, 64)        51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 41, 41, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 20, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 17, 17, 256)       262400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 17, 17, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 6, 6, 64)          147520    \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 2, 2, 128)         32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 2, 2, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 500)               64500     \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 30)                15030     \n",
      "=================================================================\n",
      "Total params: 831,470\n",
      "Trainable params: 828,382\n",
      "Non-trainable params: 3,088\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n",
      "53/53 [==============================] - 5s 94ms/step - loss: 1.7723 - val_loss: 0.9822\n",
      "Epoch 2/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 1.1475 - val_loss: 0.5666\n",
      "Epoch 3/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.9865 - val_loss: 0.3996\n",
      "Epoch 4/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.8914 - val_loss: 0.2669\n",
      "Epoch 5/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.8196 - val_loss: 0.2070\n",
      "Epoch 6/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.7768 - val_loss: 0.1593\n",
      "Epoch 7/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.7296 - val_loss: 0.1228\n",
      "Epoch 8/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.6898 - val_loss: 0.0946\n",
      "Epoch 9/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.6547 - val_loss: 0.0841\n",
      "Epoch 10/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.6300 - val_loss: 0.0775\n",
      "Epoch 11/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.6090 - val_loss: 0.0687\n",
      "Epoch 12/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.5863 - val_loss: 0.0605\n",
      "Epoch 13/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.5600 - val_loss: 0.0540\n",
      "Epoch 14/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.5283 - val_loss: 0.0466\n",
      "Epoch 15/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.5127 - val_loss: 0.0409\n",
      "Epoch 16/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.4930 - val_loss: 0.0378\n",
      "Epoch 17/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.4815 - val_loss: 0.0364\n",
      "Epoch 18/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.4611 - val_loss: 0.0332\n",
      "Epoch 19/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.4491 - val_loss: 0.0307\n",
      "Epoch 20/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.4333 - val_loss: 0.0278\n",
      "Epoch 21/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.4192 - val_loss: 0.0258\n",
      "Epoch 22/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.4051 - val_loss: 0.0241\n",
      "Epoch 23/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.3941 - val_loss: 0.0227\n",
      "Epoch 24/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.3818 - val_loss: 0.0221\n",
      "Epoch 25/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.3683 - val_loss: 0.0204\n",
      "Epoch 26/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.3557 - val_loss: 0.0196\n",
      "Epoch 27/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.3451 - val_loss: 0.0184\n",
      "Epoch 28/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.3367 - val_loss: 0.0190\n",
      "Epoch 29/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.3284 - val_loss: 0.0175\n",
      "Epoch 30/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.3239 - val_loss: 0.0162\n",
      "Epoch 31/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.3096 - val_loss: 0.0159\n",
      "Epoch 32/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.2972 - val_loss: 0.0153\n",
      "Epoch 33/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.2888 - val_loss: 0.0144\n",
      "Epoch 34/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.2817 - val_loss: 0.0138\n",
      "Epoch 35/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.2771 - val_loss: 0.0139\n",
      "Epoch 36/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.2670 - val_loss: 0.0133\n",
      "Epoch 37/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.2565 - val_loss: 0.0128\n",
      "Epoch 38/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.2521 - val_loss: 0.0123\n",
      "Epoch 39/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.2441 - val_loss: 0.0117\n",
      "Epoch 40/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.2370 - val_loss: 0.0117\n",
      "Epoch 41/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.2341 - val_loss: 0.0113\n",
      "Epoch 42/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.2246 - val_loss: 0.0110\n",
      "Epoch 43/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.2183 - val_loss: 0.0113\n",
      "Epoch 44/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.2124 - val_loss: 0.0102\n",
      "Epoch 45/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.2063 - val_loss: 0.0108\n",
      "Epoch 46/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.2013 - val_loss: 0.0100\n",
      "Epoch 47/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.1985 - val_loss: 0.0102\n",
      "Epoch 48/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.1898 - val_loss: 0.0097\n",
      "Epoch 49/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 3s 61ms/step - loss: 0.1844 - val_loss: 0.0103\n",
      "Epoch 50/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.1795 - val_loss: 0.0097\n",
      "Epoch 51/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.1789 - val_loss: 0.0092\n",
      "Epoch 52/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.1699 - val_loss: 0.0089\n",
      "Epoch 53/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.1659 - val_loss: 0.0088\n",
      "Epoch 54/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.1612 - val_loss: 0.0087\n",
      "Epoch 55/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.1595 - val_loss: 0.0089\n",
      "Epoch 56/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.1558 - val_loss: 0.0086\n",
      "Epoch 57/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.1493 - val_loss: 0.0083\n",
      "Epoch 58/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.1459 - val_loss: 0.0085\n",
      "Epoch 59/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.1415 - val_loss: 0.0082\n",
      "Epoch 60/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.1396 - val_loss: 0.0080\n",
      "Epoch 61/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.1348 - val_loss: 0.0078\n",
      "Epoch 62/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.1312 - val_loss: 0.0078\n",
      "Epoch 63/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.1282 - val_loss: 0.0077\n",
      "Epoch 64/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.1241 - val_loss: 0.0075\n",
      "Epoch 65/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.1212 - val_loss: 0.0076\n",
      "Epoch 66/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.1193 - val_loss: 0.0073\n",
      "Epoch 67/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.1147 - val_loss: 0.0077\n",
      "Epoch 68/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.1125 - val_loss: 0.0072\n",
      "Epoch 69/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.1085 - val_loss: 0.0071\n",
      "Epoch 70/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.1055 - val_loss: 0.0075\n",
      "Epoch 71/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.1043 - val_loss: 0.0070\n",
      "Epoch 72/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.1014 - val_loss: 0.0070\n",
      "Epoch 73/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0998 - val_loss: 0.0068\n",
      "Epoch 74/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0975 - val_loss: 0.0068\n",
      "Epoch 75/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0941 - val_loss: 0.0068\n",
      "Epoch 76/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0916 - val_loss: 0.0067\n",
      "Epoch 77/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0897 - val_loss: 0.0065\n",
      "Epoch 78/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0871 - val_loss: 0.0065\n",
      "Epoch 79/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0851 - val_loss: 0.0066\n",
      "Epoch 80/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0836 - val_loss: 0.0063\n",
      "Epoch 81/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0812 - val_loss: 0.0062\n",
      "Epoch 82/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0791 - val_loss: 0.0063\n",
      "Epoch 83/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0777 - val_loss: 0.0063\n",
      "Epoch 84/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0763 - val_loss: 0.0063\n",
      "Epoch 85/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0731 - val_loss: 0.0063\n",
      "Epoch 86/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0731 - val_loss: 0.0061\n",
      "Epoch 87/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0708 - val_loss: 0.0062\n",
      "Epoch 88/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0692 - val_loss: 0.0060\n",
      "Epoch 89/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0678 - val_loss: 0.0062\n",
      "Epoch 90/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0662 - val_loss: 0.0061\n",
      "Epoch 91/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0657 - val_loss: 0.0059\n",
      "Epoch 92/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0632 - val_loss: 0.0060\n",
      "Epoch 93/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0623 - val_loss: 0.0060\n",
      "Epoch 94/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0605 - val_loss: 0.0059\n",
      "Epoch 95/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0597 - val_loss: 0.0058\n",
      "Epoch 96/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0578 - val_loss: 0.0058\n",
      "Epoch 97/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0570 - val_loss: 0.0058\n",
      "Epoch 98/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0556 - val_loss: 0.0058\n",
      "Epoch 99/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0550 - val_loss: 0.0057\n",
      "Epoch 100/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0528 - val_loss: 0.0056\n",
      "Epoch 101/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0521 - val_loss: 0.0056\n",
      "Epoch 102/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0508 - val_loss: 0.0056\n",
      "Epoch 103/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0496 - val_loss: 0.0056\n",
      "Epoch 104/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0488 - val_loss: 0.0055\n",
      "Epoch 105/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0472 - val_loss: 0.0054\n",
      "Epoch 106/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0466 - val_loss: 0.0054\n",
      "Epoch 107/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0452 - val_loss: 0.0055\n",
      "Epoch 108/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0446 - val_loss: 0.0054\n",
      "Epoch 109/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0441 - val_loss: 0.0054\n",
      "Epoch 110/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0419 - val_loss: 0.0053\n",
      "Epoch 111/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0423 - val_loss: 0.0054\n",
      "Epoch 112/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0411 - val_loss: 0.0054\n",
      "Epoch 113/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0406 - val_loss: 0.0052\n",
      "Epoch 114/1000\n",
      "53/53 [==============================] - 3s 64ms/step - loss: 0.0390 - val_loss: 0.0052\n",
      "Epoch 115/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0390 - val_loss: 0.0053\n",
      "Epoch 116/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0384 - val_loss: 0.0052\n",
      "Epoch 117/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0368 - val_loss: 0.0052\n",
      "Epoch 118/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0361 - val_loss: 0.0052\n",
      "Epoch 119/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0355 - val_loss: 0.0052\n",
      "Epoch 120/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0352 - val_loss: 0.0051\n",
      "Epoch 121/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0345 - val_loss: 0.0051\n",
      "Epoch 122/1000\n",
      "53/53 [==============================] - 3s 64ms/step - loss: 0.0335 - val_loss: 0.0052\n",
      "Epoch 123/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0335 - val_loss: 0.0051\n",
      "Epoch 124/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0324 - val_loss: 0.0051\n",
      "Epoch 125/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0318 - val_loss: 0.0050\n",
      "Epoch 126/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0317 - val_loss: 0.0050\n",
      "Epoch 127/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0312 - val_loss: 0.0051\n",
      "Epoch 128/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0302 - val_loss: 0.0051\n",
      "Epoch 129/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0297 - val_loss: 0.0050\n",
      "Epoch 130/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0294 - val_loss: 0.0051\n",
      "Epoch 131/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0284 - val_loss: 0.0050\n",
      "Epoch 132/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0286 - val_loss: 0.0049\n",
      "Epoch 133/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0278 - val_loss: 0.0049\n",
      "Epoch 134/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0269 - val_loss: 0.0050\n",
      "Epoch 135/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0269 - val_loss: 0.0050\n",
      "Epoch 136/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0266 - val_loss: 0.0049\n",
      "Epoch 137/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0258 - val_loss: 0.0049\n",
      "Epoch 138/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0251 - val_loss: 0.0049\n",
      "Epoch 139/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0252 - val_loss: 0.0049\n",
      "Epoch 140/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0240 - val_loss: 0.0048\n",
      "Epoch 141/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0242 - val_loss: 0.0048\n",
      "Epoch 142/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0239 - val_loss: 0.0048\n",
      "Epoch 143/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0236 - val_loss: 0.0048\n",
      "Epoch 144/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0230 - val_loss: 0.0049\n",
      "Epoch 145/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0228 - val_loss: 0.0048\n",
      "Epoch 146/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0225 - val_loss: 0.0048\n",
      "Epoch 147/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0217 - val_loss: 0.0048\n",
      "Epoch 148/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0217 - val_loss: 0.0048\n",
      "Epoch 149/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0216 - val_loss: 0.0049\n",
      "Epoch 150/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0212 - val_loss: 0.0047\n",
      "Epoch 151/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0208 - val_loss: 0.0047\n",
      "Epoch 152/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0208 - val_loss: 0.0047\n",
      "Epoch 153/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0204 - val_loss: 0.0047\n",
      "Epoch 154/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0198 - val_loss: 0.0047\n",
      "Epoch 155/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0199 - val_loss: 0.0047\n",
      "Epoch 156/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0200 - val_loss: 0.0047\n",
      "Epoch 157/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0187 - val_loss: 0.0047\n",
      "Epoch 158/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0189 - val_loss: 0.0048\n",
      "Epoch 159/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0189 - val_loss: 0.0048\n",
      "Epoch 160/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0183 - val_loss: 0.0047\n",
      "Epoch 161/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0182 - val_loss: 0.0047\n",
      "Epoch 162/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0177 - val_loss: 0.0047\n",
      "Epoch 163/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0177 - val_loss: 0.0047\n",
      "Epoch 164/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0175 - val_loss: 0.0047\n",
      "Epoch 165/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0171 - val_loss: 0.0047\n",
      "Epoch 166/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0172 - val_loss: 0.0047\n",
      "Epoch 167/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0166 - val_loss: 0.0046\n",
      "Epoch 168/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0167 - val_loss: 0.0046\n",
      "Epoch 169/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0165 - val_loss: 0.0046\n",
      "Epoch 170/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0160 - val_loss: 0.0047\n",
      "Epoch 171/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0162 - val_loss: 0.0046\n",
      "Epoch 172/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0158 - val_loss: 0.0046\n",
      "Epoch 173/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0163 - val_loss: 0.0046\n",
      "Epoch 174/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0155 - val_loss: 0.0046\n",
      "Epoch 175/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0154 - val_loss: 0.0046\n",
      "Epoch 176/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0155 - val_loss: 0.0046\n",
      "Epoch 177/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0152 - val_loss: 0.0046\n",
      "Epoch 178/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0149 - val_loss: 0.0046\n",
      "Epoch 179/1000\n",
      "53/53 [==============================] - 3s 64ms/step - loss: 0.0149 - val_loss: 0.0046\n",
      "Epoch 180/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0148 - val_loss: 0.0045\n",
      "Epoch 181/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0146 - val_loss: 0.0046\n",
      "Epoch 182/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0144 - val_loss: 0.0046\n",
      "Epoch 183/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0141 - val_loss: 0.0045\n",
      "Epoch 184/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0143 - val_loss: 0.0045\n",
      "Epoch 185/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0138 - val_loss: 0.0045\n",
      "Epoch 186/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0137 - val_loss: 0.0046\n",
      "Epoch 187/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0139 - val_loss: 0.0047\n",
      "Epoch 188/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0137 - val_loss: 0.0046\n",
      "Epoch 189/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0134 - val_loss: 0.0046\n",
      "Epoch 190/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0132 - val_loss: 0.0045\n",
      "Epoch 191/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0132 - val_loss: 0.0045\n",
      "Epoch 192/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0133 - val_loss: 0.0045\n",
      "Epoch 193/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0129 - val_loss: 0.0045\n",
      "Epoch 194/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0127 - val_loss: 0.0045\n",
      "Epoch 195/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0129 - val_loss: 0.0045\n",
      "Epoch 196/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0127 - val_loss: 0.0045\n",
      "Epoch 197/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0123 - val_loss: 0.0045\n",
      "Epoch 198/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0126 - val_loss: 0.0045\n",
      "Epoch 199/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0124 - val_loss: 0.0045\n",
      "Epoch 200/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0126 - val_loss: 0.0045\n",
      "Epoch 201/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0125 - val_loss: 0.0045\n",
      "Epoch 202/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0122 - val_loss: 0.0045\n",
      "Epoch 203/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0119 - val_loss: 0.0045\n",
      "Epoch 204/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0123 - val_loss: 0.0045\n",
      "Epoch 205/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0120 - val_loss: 0.0045\n",
      "Epoch 206/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0116 - val_loss: 0.0045\n",
      "Epoch 207/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0118 - val_loss: 0.0045\n",
      "Epoch 208/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0115 - val_loss: 0.0045\n",
      "Epoch 209/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0120 - val_loss: 0.0045\n",
      "Epoch 210/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0115 - val_loss: 0.0044\n",
      "Epoch 211/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0116 - val_loss: 0.0045\n",
      "Epoch 212/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0113 - val_loss: 0.0044\n",
      "Epoch 213/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0114 - val_loss: 0.0044\n",
      "Epoch 214/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0114 - val_loss: 0.0044\n",
      "Epoch 215/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0108 - val_loss: 0.0044\n",
      "Epoch 216/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0112 - val_loss: 0.0044\n",
      "Epoch 217/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0111 - val_loss: 0.0044\n",
      "Epoch 218/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0112 - val_loss: 0.0045\n",
      "Epoch 219/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0111 - val_loss: 0.0044\n",
      "Epoch 220/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0114 - val_loss: 0.0044\n",
      "Epoch 221/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0109 - val_loss: 0.0044\n",
      "Epoch 222/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0108 - val_loss: 0.0045\n",
      "Epoch 223/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0110 - val_loss: 0.0045\n",
      "Epoch 224/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0110 - val_loss: 0.0044\n",
      "Epoch 225/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0108 - val_loss: 0.0044\n",
      "Epoch 226/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0107 - val_loss: 0.0044\n",
      "Epoch 227/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0107 - val_loss: 0.0044\n",
      "Epoch 228/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0106 - val_loss: 0.0044\n",
      "Epoch 229/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0104 - val_loss: 0.0044\n",
      "Epoch 230/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0104 - val_loss: 0.0045\n",
      "Epoch 231/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0102 - val_loss: 0.0044\n",
      "Epoch 232/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0104 - val_loss: 0.0044\n",
      "Epoch 233/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0103 - val_loss: 0.0044\n",
      "Epoch 234/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0103 - val_loss: 0.0044\n",
      "Epoch 235/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0102 - val_loss: 0.0044\n",
      "Epoch 236/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0103 - val_loss: 0.0044\n",
      "Epoch 237/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0102 - val_loss: 0.0044\n",
      "Epoch 238/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0100 - val_loss: 0.0044\n",
      "Epoch 239/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0104 - val_loss: 0.0044\n",
      "Epoch 240/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0100 - val_loss: 0.0044\n",
      "Epoch 241/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0100 - val_loss: 0.0044\n",
      "Epoch 242/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0101 - val_loss: 0.0044\n",
      "Epoch 243/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0100 - val_loss: 0.0044\n",
      "Epoch 244/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0101 - val_loss: 0.0044\n",
      "Epoch 245/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0098 - val_loss: 0.0044\n",
      "Epoch 246/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0097 - val_loss: 0.0044\n",
      "Epoch 247/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0097 - val_loss: 0.0044\n",
      "Epoch 248/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0096 - val_loss: 0.0044\n",
      "Epoch 249/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0099 - val_loss: 0.0044\n",
      "Epoch 250/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0098 - val_loss: 0.0044\n",
      "Epoch 251/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0098 - val_loss: 0.0044\n",
      "Epoch 252/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0096 - val_loss: 0.0044\n",
      "Epoch 253/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 254/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0094 - val_loss: 0.0044\n",
      "Epoch 255/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 256/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0096 - val_loss: 0.0044\n",
      "Epoch 257/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0096 - val_loss: 0.0044\n",
      "Epoch 258/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0094 - val_loss: 0.0044\n",
      "Epoch 259/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0094 - val_loss: 0.0044\n",
      "Epoch 260/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0093 - val_loss: 0.0044\n",
      "Epoch 261/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 262/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0093 - val_loss: 0.0044\n",
      "Epoch 263/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0098 - val_loss: 0.0044\n",
      "Epoch 264/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0094 - val_loss: 0.0044\n",
      "Epoch 265/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0091 - val_loss: 0.0043\n",
      "Epoch 266/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 267/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0096 - val_loss: 0.0044\n",
      "Epoch 268/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0093 - val_loss: 0.0043\n",
      "Epoch 269/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0093 - val_loss: 0.0043\n",
      "Epoch 270/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0094 - val_loss: 0.0043\n",
      "Epoch 271/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0092 - val_loss: 0.0043\n",
      "Epoch 272/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0092 - val_loss: 0.0043\n",
      "Epoch 273/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0092 - val_loss: 0.0043\n",
      "Epoch 274/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0091 - val_loss: 0.0044\n",
      "Epoch 275/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0091 - val_loss: 0.0043\n",
      "Epoch 276/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0091 - val_loss: 0.0044\n",
      "Epoch 277/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0089 - val_loss: 0.0043\n",
      "Epoch 278/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0090 - val_loss: 0.0043\n",
      "Epoch 279/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0091 - val_loss: 0.0043\n",
      "Epoch 280/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0090 - val_loss: 0.0043\n",
      "Epoch 281/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0091 - val_loss: 0.0043\n",
      "Epoch 282/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0088 - val_loss: 0.0043\n",
      "Epoch 283/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0089 - val_loss: 0.0043\n",
      "Epoch 284/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0087 - val_loss: 0.0044\n",
      "Epoch 285/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0091 - val_loss: 0.0044\n",
      "Epoch 286/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0090 - val_loss: 0.0044\n",
      "Epoch 287/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0088 - val_loss: 0.0043\n",
      "Epoch 288/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0089 - val_loss: 0.0043\n",
      "Epoch 289/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0088 - val_loss: 0.0043\n",
      "Epoch 290/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0088 - val_loss: 0.0043\n",
      "Epoch 291/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0087 - val_loss: 0.0043\n",
      "Epoch 292/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0089 - val_loss: 0.0043\n",
      "Epoch 293/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0090 - val_loss: 0.0043\n",
      "Epoch 294/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0087 - val_loss: 0.0043\n",
      "Epoch 295/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0088 - val_loss: 0.0043\n",
      "Epoch 296/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0088 - val_loss: 0.0043\n",
      "Epoch 297/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0088 - val_loss: 0.0043\n",
      "Epoch 298/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0091 - val_loss: 0.0043\n",
      "Epoch 299/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0088 - val_loss: 0.0043\n",
      "Epoch 300/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0091 - val_loss: 0.0043\n",
      "Epoch 301/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0087 - val_loss: 0.0043\n",
      "Epoch 302/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0088 - val_loss: 0.0043\n",
      "Epoch 303/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0088 - val_loss: 0.0043\n",
      "Epoch 304/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0086 - val_loss: 0.0043\n",
      "Epoch 305/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0089 - val_loss: 0.0043\n",
      "Epoch 306/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0086 - val_loss: 0.0043\n",
      "Epoch 307/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0087 - val_loss: 0.0043\n",
      "Epoch 308/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0086 - val_loss: 0.0043\n",
      "Epoch 309/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0087 - val_loss: 0.0043\n",
      "Epoch 310/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0087 - val_loss: 0.0043\n",
      "Epoch 311/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0086 - val_loss: 0.0043\n",
      "Epoch 312/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0086 - val_loss: 0.0043\n",
      "Epoch 313/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0086 - val_loss: 0.0043\n",
      "Epoch 314/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0084 - val_loss: 0.0043\n",
      "Epoch 315/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0083 - val_loss: 0.0043\n",
      "Epoch 316/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0086 - val_loss: 0.0043\n",
      "Epoch 317/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0087 - val_loss: 0.0043\n",
      "Epoch 318/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0086 - val_loss: 0.0043\n",
      "Epoch 319/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0085 - val_loss: 0.0043\n",
      "Epoch 320/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0086 - val_loss: 0.0043\n",
      "Epoch 321/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0086 - val_loss: 0.0043\n",
      "Epoch 322/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0085 - val_loss: 0.0043\n",
      "Epoch 323/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0085 - val_loss: 0.0043\n",
      "Epoch 324/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0085 - val_loss: 0.0043\n",
      "Epoch 325/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0083 - val_loss: 0.0044\n",
      "Epoch 326/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0085 - val_loss: 0.0043\n",
      "Epoch 327/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0084 - val_loss: 0.0043\n",
      "Epoch 328/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0085 - val_loss: 0.0043\n",
      "Epoch 329/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0082 - val_loss: 0.0042\n",
      "Epoch 330/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0085 - val_loss: 0.0042\n",
      "Epoch 331/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0086 - val_loss: 0.0043\n",
      "Epoch 332/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0084 - val_loss: 0.0043\n",
      "Epoch 333/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0084 - val_loss: 0.0043\n",
      "Epoch 334/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0084 - val_loss: 0.0043\n",
      "Epoch 335/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0085 - val_loss: 0.0043\n",
      "Epoch 336/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0083 - val_loss: 0.0043\n",
      "Epoch 337/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0085 - val_loss: 0.0043\n",
      "Epoch 338/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0085 - val_loss: 0.0043\n",
      "Epoch 339/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0083 - val_loss: 0.0043\n",
      "Epoch 340/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0084 - val_loss: 0.0043\n",
      "Epoch 341/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0083 - val_loss: 0.0043\n",
      "Epoch 342/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0082 - val_loss: 0.0043\n",
      "Epoch 343/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0085 - val_loss: 0.0043\n",
      "Epoch 344/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0084 - val_loss: 0.0043\n",
      "Epoch 345/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0083 - val_loss: 0.0043\n",
      "Epoch 346/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0083 - val_loss: 0.0042\n",
      "Epoch 347/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0086 - val_loss: 0.0043\n",
      "Epoch 348/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0082 - val_loss: 0.0042\n",
      "Epoch 349/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0082 - val_loss: 0.0043\n",
      "Epoch 350/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0083 - val_loss: 0.0043\n",
      "Epoch 351/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0083 - val_loss: 0.0043\n",
      "Epoch 352/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0084 - val_loss: 0.0043\n",
      "Epoch 353/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0084 - val_loss: 0.0042\n",
      "Epoch 354/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0083 - val_loss: 0.0042\n",
      "Epoch 355/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0082 - val_loss: 0.0043\n",
      "Epoch 356/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0084 - val_loss: 0.0043\n",
      "Epoch 357/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0082 - val_loss: 0.0042\n",
      "Epoch 358/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0082 - val_loss: 0.0042\n",
      "Epoch 359/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0083 - val_loss: 0.0042\n",
      "Epoch 360/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0084 - val_loss: 0.0043\n",
      "Epoch 361/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0082 - val_loss: 0.0043\n",
      "Epoch 362/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0084 - val_loss: 0.0043\n",
      "Epoch 363/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0084 - val_loss: 0.0042\n",
      "Epoch 364/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0083 - val_loss: 0.0042\n",
      "Epoch 365/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0082 - val_loss: 0.0042\n",
      "Epoch 366/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0081 - val_loss: 0.0042\n",
      "Epoch 367/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0082 - val_loss: 0.0042\n",
      "Epoch 368/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0082 - val_loss: 0.0043\n",
      "Epoch 369/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0082 - val_loss: 0.0042\n",
      "Epoch 370/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0081 - val_loss: 0.0042\n",
      "Epoch 371/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0081 - val_loss: 0.0042\n",
      "Epoch 372/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0081 - val_loss: 0.0043\n",
      "Epoch 373/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0083 - val_loss: 0.0043\n",
      "Epoch 374/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0083 - val_loss: 0.0042\n",
      "Epoch 375/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0081 - val_loss: 0.0043\n",
      "Epoch 376/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0081 - val_loss: 0.0042\n",
      "Epoch 377/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0081 - val_loss: 0.0042\n",
      "Epoch 378/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0081 - val_loss: 0.0043\n",
      "Epoch 379/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0081 - val_loss: 0.0042\n",
      "Epoch 380/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0083 - val_loss: 0.0042\n",
      "Epoch 381/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0083 - val_loss: 0.0042\n",
      "Epoch 382/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0081 - val_loss: 0.0042\n",
      "Epoch 383/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 384/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0082 - val_loss: 0.0042\n",
      "Epoch 385/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0082 - val_loss: 0.0042\n",
      "Epoch 386/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0081 - val_loss: 0.0042\n",
      "Epoch 387/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0081 - val_loss: 0.0042\n",
      "Epoch 388/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0083 - val_loss: 0.0042\n",
      "Epoch 389/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0081 - val_loss: 0.0042\n",
      "Epoch 390/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0082 - val_loss: 0.0043\n",
      "Epoch 391/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0081 - val_loss: 0.0042\n",
      "Epoch 392/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0081 - val_loss: 0.0042\n",
      "Epoch 393/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0081 - val_loss: 0.0042\n",
      "Epoch 394/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 395/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0083 - val_loss: 0.0042\n",
      "Epoch 396/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0078 - val_loss: 0.0042\n",
      "Epoch 397/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0082 - val_loss: 0.0042\n",
      "Epoch 398/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0081 - val_loss: 0.0042\n",
      "Epoch 399/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 400/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 401/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0081 - val_loss: 0.0042\n",
      "Epoch 402/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 403/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0082 - val_loss: 0.0042\n",
      "Epoch 404/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0081 - val_loss: 0.0042\n",
      "Epoch 405/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 406/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 407/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 408/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0042\n",
      "Epoch 409/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0082 - val_loss: 0.0042\n",
      "Epoch 410/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0081 - val_loss: 0.0042\n",
      "Epoch 411/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0042\n",
      "Epoch 412/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 413/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0082 - val_loss: 0.0042\n",
      "Epoch 414/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 415/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 416/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 417/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 418/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0081 - val_loss: 0.0042\n",
      "Epoch 419/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0078 - val_loss: 0.0042\n",
      "Epoch 420/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 421/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 422/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 423/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 424/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0042\n",
      "Epoch 425/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0082 - val_loss: 0.0042\n",
      "Epoch 426/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0042\n",
      "Epoch 427/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 428/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 429/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 430/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 431/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0077 - val_loss: 0.0042\n",
      "Epoch 432/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 433/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 434/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 435/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 436/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0042\n",
      "Epoch 437/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0081 - val_loss: 0.0042\n",
      "Epoch 438/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 439/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 440/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 441/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 442/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0078 - val_loss: 0.0042\n",
      "Epoch 443/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 444/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 445/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0042\n",
      "Epoch 446/1000\n",
      "53/53 [==============================] - 3s 64ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 447/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 448/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 449/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 450/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 451/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0042\n",
      "Epoch 452/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0043\n",
      "Epoch 453/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0041\n",
      "Epoch 454/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 455/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 456/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0042\n",
      "Epoch 457/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 458/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 459/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 460/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0041\n",
      "Epoch 461/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 462/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 463/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 464/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0079 - val_loss: 0.0041\n",
      "Epoch 465/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 466/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 467/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 468/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0042\n",
      "Epoch 469/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 470/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0078 - val_loss: 0.0042\n",
      "Epoch 471/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 472/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 473/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0042\n",
      "Epoch 474/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 475/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 476/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0041\n",
      "Epoch 477/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0076 - val_loss: 0.0041\n",
      "Epoch 478/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0080 - val_loss: 0.0041\n",
      "Epoch 479/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0042\n",
      "Epoch 480/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0042\n",
      "Epoch 481/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0041\n",
      "Epoch 482/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 483/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 484/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0041\n",
      "Epoch 485/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0041\n",
      "Epoch 486/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0080 - val_loss: 0.0041\n",
      "Epoch 487/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 488/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 489/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 490/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0079 - val_loss: 0.0041\n",
      "Epoch 491/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 492/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 493/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 494/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 495/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0076 - val_loss: 0.0041\n",
      "Epoch 496/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 497/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 498/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 499/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 500/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 501/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 502/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 503/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 504/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 505/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 506/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 507/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 508/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 509/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0076 - val_loss: 0.0041\n",
      "Epoch 510/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0042\n",
      "Epoch 511/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 512/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0041\n",
      "Epoch 513/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0076 - val_loss: 0.0041\n",
      "Epoch 514/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 515/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 516/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 517/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 518/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0076 - val_loss: 0.0041\n",
      "Epoch 519/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 520/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0076 - val_loss: 0.0041\n",
      "Epoch 521/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 522/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0076 - val_loss: 0.0041\n",
      "Epoch 523/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 524/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0041\n",
      "Epoch 525/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0041\n",
      "Epoch 526/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0079 - val_loss: 0.0041\n",
      "Epoch 527/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 528/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 529/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0041\n",
      "Epoch 530/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0041\n",
      "Epoch 531/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0041\n",
      "Epoch 532/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 533/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0076 - val_loss: 0.0041\n",
      "Epoch 534/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0076 - val_loss: 0.0041\n",
      "Epoch 535/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 536/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0041\n",
      "Epoch 537/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0041\n",
      "Epoch 538/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0079 - val_loss: 0.0041\n",
      "Epoch 539/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0078 - val_loss: 0.0040\n",
      "Epoch 540/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 541/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0078 - val_loss: 0.0040\n",
      "Epoch 542/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 543/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 544/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 545/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0077 - val_loss: 0.0040\n",
      "Epoch 546/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 547/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0076 - val_loss: 0.0041\n",
      "Epoch 548/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0077 - val_loss: 0.0040\n",
      "Epoch 549/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0076 - val_loss: 0.0040\n",
      "Epoch 550/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0077 - val_loss: 0.0041\n",
      "Epoch 551/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0076 - val_loss: 0.0040\n",
      "Epoch 552/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 553/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0078 - val_loss: 0.0041\n",
      "Epoch 554/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0076 - val_loss: 0.0041\n",
      "Epoch 555/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 556/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0077 - val_loss: 0.0040\n",
      "Epoch 557/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0076 - val_loss: 0.0041\n",
      "Epoch 558/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 559/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0076 - val_loss: 0.0040\n",
      "Epoch 560/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0040\n",
      "Epoch 561/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0041\n",
      "Epoch 562/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0077 - val_loss: 0.0040\n",
      "Epoch 563/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0041\n",
      "Epoch 564/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0040\n",
      "Epoch 565/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0040\n",
      "Epoch 566/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0076 - val_loss: 0.0040\n",
      "Epoch 567/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0076 - val_loss: 0.0040\n",
      "Epoch 568/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 569/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 570/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 571/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 572/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 573/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0074 - val_loss: 0.0040\n",
      "Epoch 574/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0076 - val_loss: 0.0040\n",
      "Epoch 575/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0040\n",
      "Epoch 576/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0074 - val_loss: 0.0040\n",
      "Epoch 577/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0076 - val_loss: 0.0040\n",
      "Epoch 578/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0040\n",
      "Epoch 579/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 580/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0077 - val_loss: 0.0040\n",
      "Epoch 581/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 582/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0074 - val_loss: 0.0040\n",
      "Epoch 583/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0040\n",
      "Epoch 584/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 585/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0076 - val_loss: 0.0040\n",
      "Epoch 586/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0040\n",
      "Epoch 587/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0076 - val_loss: 0.0040\n",
      "Epoch 588/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 589/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0076 - val_loss: 0.0040\n",
      "Epoch 590/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 591/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 592/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0040\n",
      "Epoch 593/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0076 - val_loss: 0.0040\n",
      "Epoch 594/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 595/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 596/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 597/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 598/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0040\n",
      "Epoch 599/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 600/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0076 - val_loss: 0.0040\n",
      "Epoch 601/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0076 - val_loss: 0.0040\n",
      "Epoch 602/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0076 - val_loss: 0.0040\n",
      "Epoch 603/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0076 - val_loss: 0.0040\n",
      "Epoch 604/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0074 - val_loss: 0.0040\n",
      "Epoch 605/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0039\n",
      "Epoch 606/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0074 - val_loss: 0.0040\n",
      "Epoch 607/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0076 - val_loss: 0.0040\n",
      "Epoch 608/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 609/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 610/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 611/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 612/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0039\n",
      "Epoch 613/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 614/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 615/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0039\n",
      "Epoch 616/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0073 - val_loss: 0.0040\n",
      "Epoch 617/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0074 - val_loss: 0.0040\n",
      "Epoch 618/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0074 - val_loss: 0.0040\n",
      "Epoch 619/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0040\n",
      "Epoch 620/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0076 - val_loss: 0.0040\n",
      "Epoch 621/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 622/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0072 - val_loss: 0.0039\n",
      "Epoch 623/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0040\n",
      "Epoch 624/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0040\n",
      "Epoch 625/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0074 - val_loss: 0.0040\n",
      "Epoch 626/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0077 - val_loss: 0.0039\n",
      "Epoch 627/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0077 - val_loss: 0.0040\n",
      "Epoch 628/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0073 - val_loss: 0.0039\n",
      "Epoch 629/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0039\n",
      "Epoch 630/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0074 - val_loss: 0.0040\n",
      "Epoch 631/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 632/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 633/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 634/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 635/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 636/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 637/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0039\n",
      "Epoch 638/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 639/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0076 - val_loss: 0.0039\n",
      "Epoch 640/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0039\n",
      "Epoch 641/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 642/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0072 - val_loss: 0.0039\n",
      "Epoch 643/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0073 - val_loss: 0.0039\n",
      "Epoch 644/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 645/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 646/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0072 - val_loss: 0.0039\n",
      "Epoch 647/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0039\n",
      "Epoch 648/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0075 - val_loss: 0.0039\n",
      "Epoch 649/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 650/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 651/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0072 - val_loss: 0.0039\n",
      "Epoch 652/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0039\n",
      "Epoch 653/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0072 - val_loss: 0.0039\n",
      "Epoch 654/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0075 - val_loss: 0.0039\n",
      "Epoch 655/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0072 - val_loss: 0.0039\n",
      "Epoch 656/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0073 - val_loss: 0.0039\n",
      "Epoch 657/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0072 - val_loss: 0.0039\n",
      "Epoch 658/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0073 - val_loss: 0.0039\n",
      "Epoch 659/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0039\n",
      "Epoch 660/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0073 - val_loss: 0.0039\n",
      "Epoch 661/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 662/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0039\n",
      "Epoch 663/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0073 - val_loss: 0.0039\n",
      "Epoch 664/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 665/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 666/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 667/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0039\n",
      "Epoch 668/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0039\n",
      "Epoch 669/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 670/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0073 - val_loss: 0.0039\n",
      "Epoch 671/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0072 - val_loss: 0.0039\n",
      "Epoch 672/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0076 - val_loss: 0.0039\n",
      "Epoch 673/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 674/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0073 - val_loss: 0.0039\n",
      "Epoch 675/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0073 - val_loss: 0.0039\n",
      "Epoch 676/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0076 - val_loss: 0.0039\n",
      "Epoch 677/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 678/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0072 - val_loss: 0.0039\n",
      "Epoch 679/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0039\n",
      "Epoch 680/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0039\n",
      "Epoch 681/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0075 - val_loss: 0.0039\n",
      "Epoch 682/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 683/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0039\n",
      "Epoch 684/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0038\n",
      "Epoch 685/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0039\n",
      "Epoch 686/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 687/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0072 - val_loss: 0.0038\n",
      "Epoch 688/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 689/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0038\n",
      "Epoch 690/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0073 - val_loss: 0.0039\n",
      "Epoch 691/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0039\n",
      "Epoch 692/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 693/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0072 - val_loss: 0.0038\n",
      "Epoch 694/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0072 - val_loss: 0.0038\n",
      "Epoch 695/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0072 - val_loss: 0.0039\n",
      "Epoch 696/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0071 - val_loss: 0.0039\n",
      "Epoch 697/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0073 - val_loss: 0.0039\n",
      "Epoch 698/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 699/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0073 - val_loss: 0.0038\n",
      "Epoch 700/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0073 - val_loss: 0.0038\n",
      "Epoch 701/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0072 - val_loss: 0.0039\n",
      "Epoch 702/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0073 - val_loss: 0.0039\n",
      "Epoch 703/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0073 - val_loss: 0.0039\n",
      "Epoch 704/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0070 - val_loss: 0.0038\n",
      "Epoch 705/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0073 - val_loss: 0.0038\n",
      "Epoch 706/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0073 - val_loss: 0.0039\n",
      "Epoch 707/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 708/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0072 - val_loss: 0.0038\n",
      "Epoch 709/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0073 - val_loss: 0.0038\n",
      "Epoch 710/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0071 - val_loss: 0.0039\n",
      "Epoch 711/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0072 - val_loss: 0.0038\n",
      "Epoch 712/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 713/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 714/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0074 - val_loss: 0.0039\n",
      "Epoch 715/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0073 - val_loss: 0.0038\n",
      "Epoch 716/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0038\n",
      "Epoch 717/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 718/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 719/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0073 - val_loss: 0.0038\n",
      "Epoch 720/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 721/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 722/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0072 - val_loss: 0.0038\n",
      "Epoch 723/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 724/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 725/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0038\n",
      "Epoch 726/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 727/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0075 - val_loss: 0.0038\n",
      "Epoch 728/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 729/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0038\n",
      "Epoch 730/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0073 - val_loss: 0.0038\n",
      "Epoch 731/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 732/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0073 - val_loss: 0.0038\n",
      "Epoch 733/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 734/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0072 - val_loss: 0.0038\n",
      "Epoch 735/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 736/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0072 - val_loss: 0.0038\n",
      "Epoch 737/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0038\n",
      "Epoch 738/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0070 - val_loss: 0.0038\n",
      "Epoch 739/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0073 - val_loss: 0.0038\n",
      "Epoch 740/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 741/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0074 - val_loss: 0.0038\n",
      "Epoch 742/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0038\n",
      "Epoch 743/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0072 - val_loss: 0.0038\n",
      "Epoch 744/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0073 - val_loss: 0.0038\n",
      "Epoch 745/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 746/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0038\n",
      "Epoch 747/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0070 - val_loss: 0.0038\n",
      "Epoch 748/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 749/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0070 - val_loss: 0.0038\n",
      "Epoch 750/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 751/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 752/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 753/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 754/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0038\n",
      "Epoch 755/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 756/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0070 - val_loss: 0.0037\n",
      "Epoch 757/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0073 - val_loss: 0.0038\n",
      "Epoch 758/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 759/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0070 - val_loss: 0.0038\n",
      "Epoch 760/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0069 - val_loss: 0.0038\n",
      "Epoch 761/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0072 - val_loss: 0.0038\n",
      "Epoch 762/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0070 - val_loss: 0.0038\n",
      "Epoch 763/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0072 - val_loss: 0.0038\n",
      "Epoch 764/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0070 - val_loss: 0.0038\n",
      "Epoch 765/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 766/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0038\n",
      "Epoch 767/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0038\n",
      "Epoch 768/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0070 - val_loss: 0.0037\n",
      "Epoch 769/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 770/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0037\n",
      "Epoch 771/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0072 - val_loss: 0.0038\n",
      "Epoch 772/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 773/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0038\n",
      "Epoch 774/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0037\n",
      "Epoch 775/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 776/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0037\n",
      "Epoch 777/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0071 - val_loss: 0.0037\n",
      "Epoch 778/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 779/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 780/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0037\n",
      "Epoch 781/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 782/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0037\n",
      "Epoch 783/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 784/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0067 - val_loss: 0.0037\n",
      "Epoch 785/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0038\n",
      "Epoch 786/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 787/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 788/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0038\n",
      "Epoch 789/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 790/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0066 - val_loss: 0.0037\n",
      "Epoch 791/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 792/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0071 - val_loss: 0.0038\n",
      "Epoch 793/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 794/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0070 - val_loss: 0.0037\n",
      "Epoch 795/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 796/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0068 - val_loss: 0.0038\n",
      "Epoch 797/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 798/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0037\n",
      "Epoch 799/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0037\n",
      "Epoch 800/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 801/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0037\n",
      "Epoch 802/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 803/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 804/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 805/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 806/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 807/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 808/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 809/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 810/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0070 - val_loss: 0.0037\n",
      "Epoch 811/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 812/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 813/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 814/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0067 - val_loss: 0.0037\n",
      "Epoch 815/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 816/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0037\n",
      "Epoch 817/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 818/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 819/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 820/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0037\n",
      "Epoch 821/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0066 - val_loss: 0.0037\n",
      "Epoch 822/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 823/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0067 - val_loss: 0.0036\n",
      "Epoch 824/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 825/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0070 - val_loss: 0.0037\n",
      "Epoch 826/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0067 - val_loss: 0.0037\n",
      "Epoch 827/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 828/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 829/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 830/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0070 - val_loss: 0.0037\n",
      "Epoch 831/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 832/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0067 - val_loss: 0.0037\n",
      "Epoch 833/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0066 - val_loss: 0.0037\n",
      "Epoch 834/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 835/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0067 - val_loss: 0.0036\n",
      "Epoch 836/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 837/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 838/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0067 - val_loss: 0.0037\n",
      "Epoch 839/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0067 - val_loss: 0.0037\n",
      "Epoch 840/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 841/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0067 - val_loss: 0.0037\n",
      "Epoch 842/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 843/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0067 - val_loss: 0.0037\n",
      "Epoch 844/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0066 - val_loss: 0.0037\n",
      "Epoch 845/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0067 - val_loss: 0.0037\n",
      "Epoch 846/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0067 - val_loss: 0.0037\n",
      "Epoch 847/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0067 - val_loss: 0.0037\n",
      "Epoch 848/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0067 - val_loss: 0.0036\n",
      "Epoch 849/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0066 - val_loss: 0.0036\n",
      "Epoch 850/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0068 - val_loss: 0.0037\n",
      "Epoch 851/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0066 - val_loss: 0.0037\n",
      "Epoch 852/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 853/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0067 - val_loss: 0.0037\n",
      "Epoch 854/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0067 - val_loss: 0.0037\n",
      "Epoch 855/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0066 - val_loss: 0.0037\n",
      "Epoch 856/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0067 - val_loss: 0.0036\n",
      "Epoch 857/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0065 - val_loss: 0.0037\n",
      "Epoch 858/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0067 - val_loss: 0.0037\n",
      "Epoch 859/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0067 - val_loss: 0.0037\n",
      "Epoch 860/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0066 - val_loss: 0.0037\n",
      "Epoch 861/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0065 - val_loss: 0.0037\n",
      "Epoch 862/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0067 - val_loss: 0.0037\n",
      "Epoch 863/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 864/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0067 - val_loss: 0.0036\n",
      "Epoch 865/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0066 - val_loss: 0.0036\n",
      "Epoch 866/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 867/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0067 - val_loss: 0.0036\n",
      "Epoch 868/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0066 - val_loss: 0.0036\n",
      "Epoch 869/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 870/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0067 - val_loss: 0.0036\n",
      "Epoch 871/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0067 - val_loss: 0.0037\n",
      "Epoch 872/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 873/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 874/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0066 - val_loss: 0.0036\n",
      "Epoch 875/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0066 - val_loss: 0.0037\n",
      "Epoch 876/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0062 - val_loss: 0.0036\n",
      "Epoch 877/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 878/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0065 - val_loss: 0.0037\n",
      "Epoch 879/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0066 - val_loss: 0.0036\n",
      "Epoch 880/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 881/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 882/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 883/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 884/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0066 - val_loss: 0.0036\n",
      "Epoch 885/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 886/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0064 - val_loss: 0.0036\n",
      "Epoch 887/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 888/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0064 - val_loss: 0.0036\n",
      "Epoch 889/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 890/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0066 - val_loss: 0.0036\n",
      "Epoch 891/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0063 - val_loss: 0.0036\n",
      "Epoch 892/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 893/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0066 - val_loss: 0.0036\n",
      "Epoch 894/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0064 - val_loss: 0.0036\n",
      "Epoch 895/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0068 - val_loss: 0.0036\n",
      "Epoch 896/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0064 - val_loss: 0.0036\n",
      "Epoch 897/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 898/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0064 - val_loss: 0.0036\n",
      "Epoch 899/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0064 - val_loss: 0.0036\n",
      "Epoch 900/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 901/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 902/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0063 - val_loss: 0.0036\n",
      "Epoch 903/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 904/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0063 - val_loss: 0.0036\n",
      "Epoch 905/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0066 - val_loss: 0.0036\n",
      "Epoch 906/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0064 - val_loss: 0.0036\n",
      "Epoch 907/1000\n",
      "53/53 [==============================] - 3s 60ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 908/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0066 - val_loss: 0.0036\n",
      "Epoch 909/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0062 - val_loss: 0.0036\n",
      "Epoch 910/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0064 - val_loss: 0.0036\n",
      "Epoch 911/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0063 - val_loss: 0.0036\n",
      "Epoch 912/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0064 - val_loss: 0.0036\n",
      "Epoch 913/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0065 - val_loss: 0.0035\n",
      "Epoch 914/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0064 - val_loss: 0.0036\n",
      "Epoch 915/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0062 - val_loss: 0.0036\n",
      "Epoch 916/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 917/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0064 - val_loss: 0.0036\n",
      "Epoch 918/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0064 - val_loss: 0.0035\n",
      "Epoch 919/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0064 - val_loss: 0.0036\n",
      "Epoch 920/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0064 - val_loss: 0.0035\n",
      "Epoch 921/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 922/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0063 - val_loss: 0.0035\n",
      "Epoch 923/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0064 - val_loss: 0.0035\n",
      "Epoch 924/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0060 - val_loss: 0.0035\n",
      "Epoch 925/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 926/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0063 - val_loss: 0.0035\n",
      "Epoch 927/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0063 - val_loss: 0.0036\n",
      "Epoch 928/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0063 - val_loss: 0.0035\n",
      "Epoch 929/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0062 - val_loss: 0.0036\n",
      "Epoch 930/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0062 - val_loss: 0.0036\n",
      "Epoch 931/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0063 - val_loss: 0.0035\n",
      "Epoch 932/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0064 - val_loss: 0.0036\n",
      "Epoch 933/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0062 - val_loss: 0.0036\n",
      "Epoch 934/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 935/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 936/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 937/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0062 - val_loss: 0.0036\n",
      "Epoch 938/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0062 - val_loss: 0.0036\n",
      "Epoch 939/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0064 - val_loss: 0.0035\n",
      "Epoch 940/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0063 - val_loss: 0.0036\n",
      "Epoch 941/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0060 - val_loss: 0.0035\n",
      "Epoch 942/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0063 - val_loss: 0.0035\n",
      "Epoch 943/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0061 - val_loss: 0.0035\n",
      "Epoch 944/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0062 - val_loss: 0.0036\n",
      "Epoch 945/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0061 - val_loss: 0.0035\n",
      "Epoch 946/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0065 - val_loss: 0.0035\n",
      "Epoch 947/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 948/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 949/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 950/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0063 - val_loss: 0.0035\n",
      "Epoch 951/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 952/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 953/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0061 - val_loss: 0.0035\n",
      "Epoch 954/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 955/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 956/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0060 - val_loss: 0.0035\n",
      "Epoch 957/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0063 - val_loss: 0.0036\n",
      "Epoch 958/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 959/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0060 - val_loss: 0.0035\n",
      "Epoch 960/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 961/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0063 - val_loss: 0.0035\n",
      "Epoch 962/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0059 - val_loss: 0.0035\n",
      "Epoch 963/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0061 - val_loss: 0.0035\n",
      "Epoch 964/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0060 - val_loss: 0.0035\n",
      "Epoch 965/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0063 - val_loss: 0.0035\n",
      "Epoch 966/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0061 - val_loss: 0.0035\n",
      "Epoch 967/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0061 - val_loss: 0.0035\n",
      "Epoch 968/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0061 - val_loss: 0.0035\n",
      "Epoch 969/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 970/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 971/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0060 - val_loss: 0.0035\n",
      "Epoch 972/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0059 - val_loss: 0.0035\n",
      "Epoch 973/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0061 - val_loss: 0.0035\n",
      "Epoch 974/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0061 - val_loss: 0.0035\n",
      "Epoch 975/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 976/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0059 - val_loss: 0.0034\n",
      "Epoch 977/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0061 - val_loss: 0.0035\n",
      "Epoch 978/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0060 - val_loss: 0.0035\n",
      "Epoch 979/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0062 - val_loss: 0.0035\n",
      "Epoch 980/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0060 - val_loss: 0.0035\n",
      "Epoch 981/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0060 - val_loss: 0.0035\n",
      "Epoch 982/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0059 - val_loss: 0.0035\n",
      "Epoch 983/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0059 - val_loss: 0.0034\n",
      "Epoch 984/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0060 - val_loss: 0.0034\n",
      "Epoch 985/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0059 - val_loss: 0.0035\n",
      "Epoch 986/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0059 - val_loss: 0.0035\n",
      "Epoch 987/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0059 - val_loss: 0.0035\n",
      "Epoch 988/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0059 - val_loss: 0.0034\n",
      "Epoch 989/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0058 - val_loss: 0.0034\n",
      "Epoch 990/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0059 - val_loss: 0.0034\n",
      "Epoch 991/1000\n",
      "53/53 [==============================] - 3s 63ms/step - loss: 0.0058 - val_loss: 0.0035\n",
      "Epoch 992/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0059 - val_loss: 0.0034\n",
      "Epoch 993/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0059 - val_loss: 0.0035\n",
      "Epoch 994/1000\n",
      "53/53 [==============================] - 3s 61ms/step - loss: 0.0058 - val_loss: 0.0035\n",
      "Epoch 995/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0058 - val_loss: 0.0034\n",
      "Epoch 996/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0059 - val_loss: 0.0034\n",
      "Epoch 997/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0061 - val_loss: 0.0034\n",
      "Epoch 998/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0060 - val_loss: 0.0034\n",
      "Epoch 999/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0057 - val_loss: 0.0034\n",
      "Epoch 1000/1000\n",
      "53/53 [==============================] - 3s 62ms/step - loss: 0.0060 - val_loss: 0.0034\n"
     ]
    }
   ],
   "source": [
    "g1_model3_sgd = Sequential()\n",
    "g1_model3_sgd.add(Conv2D(32,\n",
    "                 (6, 6),\n",
    "                 activation='relu',\n",
    "                input_shape=INPUT_SHAPE))\n",
    "g1_model3_sgd.add(BatchNormalization())\n",
    "g1_model3_sgd.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "g1_model3_sgd.add(Conv2D(filters=64,\n",
    "                 kernel_size=(5, 5),\n",
    "                 activation='relu'))\n",
    "g1_model3_sgd.add(BatchNormalization())\n",
    "g1_model3_sgd.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "g1_model3_sgd.add(Conv2D(filters=256,\n",
    "                 kernel_size=(4, 4),\n",
    "                 activation='relu'))\n",
    "g1_model3_sgd.add(BatchNormalization())\n",
    "g1_model3_sgd.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "g1_model3_sgd.add(Conv2D(filters=64,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "g1_model3_sgd.add(BatchNormalization())\n",
    "g1_model3_sgd.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "g1_model3_sgd.add(Conv2D(filters=128,\n",
    "                 kernel_size=(2, 2),\n",
    "                 activation='relu'))\n",
    "g1_model3_sgd.add(BatchNormalization())\n",
    "g1_model3_sgd.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "g1_model3_sgd.add(Flatten())\n",
    "g1_model3_sgd.add(Dense(500, activation = \"relu\"))\n",
    "g1_model3_sgd.add(BatchNormalization())\n",
    "g1_model3_sgd.add(Dense(500, activation = \"relu\"))\n",
    "g1_model3_sgd.add(BatchNormalization())\n",
    "g1_model3_sgd.add(Dropout(.3))\n",
    "g1_model3_sgd.add(Dense(30))\n",
    "print(g1_model3_sgd.summary())\n",
    "g1_CNN_aug_addedLayers_hist_sgd, g1_model3_new_sgd = fit_model(g1_model3_sgd, data1,'g1_CNN_aug_addedLayers_sgd',\n",
    "                                                       datagen, patience=1000, retrain=True, optimizer ='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss([g1_CNN_aug_addedLayers_hist_nadam, g1_CNN_aug_addedLayers_hist_adam, g1_CNN_aug_addedLayers_hist_sgd],\n",
    "           ['g1_CNN_aug_addedLayers_nadam','g1_CNN_aug_addedLayers_adam','g1_CNN_aug_addedLayers_sgd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Using VGG: </H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force grayscale into 3 channels\n",
    "X1_train_3chan = np.tile(X1_train,(1,1,1,3))\n",
    "X1_test_3chan = np.tile(X1_test,(1,1,1,3))\n",
    "data_vgg=[X1_train_3chan, X1_test_3chan, Y1_train, Y1_test]\n",
    "\n",
    "# new 3 channel data generator\n",
    "datagen_vgg = CustImageDataGenerator(\n",
    "    rotation_range=5. #degrees\n",
    "     ,horizontal_flip=True\n",
    "     ,width_shift_range=.05 # percent of image width\n",
    "     ,height_shift_range=.05 # percent of image height\n",
    "    ).flow(X1_train_3chan,Y1_train,whichlabels=list(labels1), batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the base pre-trained model\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# add the keypoints detection layer\n",
    "predictions = Dense(30)(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model_vgg = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(model_vgg.summary())\n",
    "model_vgg_hist, model_vgg = fit_model(model_vgg, data_vgg,'g1_vgg', datagen_vgg, patience=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model_vgg.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model_vgg.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model_vgg_hist2, model_vgg2 = fit_model(model_vgg, data_vgg,'g1_vgg2',datagen_vgg, patience=1000)\n",
    "\n",
    "# train all layers!\n",
    "for layer in model_vgg.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model_vgg_hist3, model_vgg3 = fit_model(model_vgg, data_vgg,'g1_vgg3',datagen_vgg, patience=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare loss\n",
    "model_vgg_hist_all = {}\n",
    "model_vgg_hist_all['loss'] = (model_vgg_hist['loss'] + model_vgg_hist2['loss'] + model_vgg_hist3['loss'])\n",
    "model_vgg_hist_all['val_loss'] = (model_vgg_hist['val_loss'] + model_vgg_hist2['val_loss'] + model_vgg_hist3['val_loss'])\n",
    "\n",
    "plot_loss([g1_CNN_aug_addedLayers_hist, model_vgg_hist_all],\n",
    "           ['g1_CNN_aug_addedLayers','g1_vgg'])\n",
    "\n",
    "\n",
    "plot_loss([g1_CNN_aug_addedLayers_hist, model_vgg_hist, model_vgg_hist2, model_vgg_hist3],\n",
    "           ['g1_CNN_aug_addedLayers','g1_vgg_1','g1_vgg_2','g1_vgg_3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a model from the zoo\n",
    "We try to train the InceptionV3 model to see if it does a better job. This model was chosen because it has good evaluation results, but has fewer parameters than some of the other competition-winning models.\n",
    "\n",
    "#### Setup\n",
    "In order to use InceptionV3, we need a 3 channel input. To fit this, we can just repeat the grayscale value for each of the 3 channels. We also have to change the data generator to create 3 channel images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force grayscale into 3 channels\n",
    "X1_train_3chan = np.tile(X1_train,(1,1,1,3))\n",
    "X1_test_3chan = np.tile(X1_test,(1,1,1,3))\n",
    "data1_inceptionV3=[X1_train_3chan, X1_test_3chan, Y1_train, Y1_test]\n",
    "\n",
    "# new 3 channel data generator\n",
    "datagen_inceptionV3 = CustImageDataGenerator(\n",
    "    rotation_range=5. #degrees\n",
    "     ,horizontal_flip=True\n",
    "     ,width_shift_range=.05 # percent of image width\n",
    "     ,height_shift_range=.05 # percent of image height\n",
    "    ).flow(X1_train_3chan,Y1_train,whichlabels=list(labels1), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "We train the model in 3 steps.\n",
    "1. Create the base model and add a global average pooling layer, a dense layer, and an output layer. Then train the 3 added layers.\n",
    "1. Train layers further up from the end (layer 250 through to the output layer).\n",
    "1. Train all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# add the keypoints detection layer\n",
    "predictions = Dense(30)(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model_inceptionV3 = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(model_inceptionV3.summary())\n",
    "model_inceptionV3_hist, model_inceptionV3 = fit_model(model_inceptionV3, data1_inceptionV3,\n",
    "                                                      'g1_zoo', datagen_inceptionV3,\n",
    "                                                      patience=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model_inceptionV3.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model_inceptionV3.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model_inceptionV3_hist2, model_inceptionV3 = fit_model(model_inceptionV3, data1_inceptionV3,\n",
    "                                'g1_zoo2',datagen_inceptionV3,\n",
    "                                patience=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train all layers!\n",
    "for layer in model_inceptionV3.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model_inceptionV3_hist3, model_inceptionV3 = fit_model(model_inceptionV3, data1_inceptionV3,\n",
    "                                'g1_zoo3',datagen_inceptionV3,\n",
    "                                patience=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compare loss\n",
    "model_inceptionV3_hist_all = {}\n",
    "model_inceptionV3_hist_all['loss'] = (model_inceptionV3_hist['loss'] +\n",
    "                                      model_inceptionV3_hist2['loss'] +\n",
    "                                      model_inceptionV3_hist3['loss']\n",
    "                                      )\n",
    "model_inceptionV3_hist_all['val_loss'] = (model_inceptionV3_hist['val_loss'] +\n",
    "                                          model_inceptionV3_hist2['val_loss'] +\n",
    "                                          model_inceptionV3_hist3['val_loss']\n",
    "                                          )\n",
    "plot_loss([g1_CNN_aug_addedLayers_hist, model_inceptionV3_hist, model_inceptionV3_hist2, model_inceptionV3_hist3],\n",
    "           ['g1_CNN_aug_addedLayers','g1_inceptionV3_1','g1_inceptionV3_2','g1_inceptionV3_3'])\n",
    "plot_loss([g1_CNN_aug_addedLayers_hist, model_vgg_hist, model_vgg_hist2, model_vgg_hist3],\n",
    "           ['g1_CNN_aug_addedLayers','g1_vgg_1','g1_vgg_2','g1_vgg_3'])\n",
    "plot_loss([g1_CNN_aug_addedLayers_hist, model_vgg_hist, model_vgg_hist3, model_inceptionV3_hist, model_inceptionV3_hist3],\n",
    "           ['g1_CNN_aug_addedLayers','g1_vgg_1','g1_vgg_3','g1_inceptionV3_1','g1_inceptionV3_3'])\n",
    "plot_loss([g1_CNN_aug_addedLayers_hist, model_vgg_hist, model_vgg_hist2, model_inceptionV3_hist, model_inceptionV3_hist3],\n",
    "           ['g1_CNN_aug_addedLayers','g1_vgg_1','g1_vgg_2','g1_inceptionV3_1','g1_inceptionV3_3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears InceptionV3 loses out to our first data-augmented model. This is likely because we just don't have enough training examples to get enough variance into the Inception model that its generalization functionality works. It does appear the validation and training loss go hand in hand for the InceptionV3 model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
