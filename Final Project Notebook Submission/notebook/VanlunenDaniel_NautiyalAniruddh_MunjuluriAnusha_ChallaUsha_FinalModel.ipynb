{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Keypoint Detection Project\n",
    "The objective of this task is to predict keypoint positions on face images. We explore various nerual network structures for our predictions and ultimately train two final models to make the predictions. Our final model's predictions submitted to kaggle produce a RMSE error of 1.71947 on the unseen test data labels. This places us 4th on the public Kaggle leaderboard. \n",
    "\n",
    "## Group Members\n",
    "- Vanlunen, Daniel\n",
    "- Nautiyal, Aniruddh\n",
    "- Munjuluri, Anusha\n",
    "- Challa, Usha\n",
    "\n",
    "### Notes about the directory structure\n",
    "The zip file submitted contains 3 folders.\n",
    "1. data: This folder contains the competition data. You need the Idlookuptable, test, training, and samplesubmission files in this folder for the notebook below to run. Make sure the training data is unzipped before running the below notebook.\n",
    "2. notebook: This folder contains this main subission notebook. It also contains a subfolder with a screenshot (img) and another subfolder that defines a custom image generating class (augmentdata folder).\n",
    "3. saved-models: this folder contains the saved final models weights in .h5 files. It also has a subfolder that contains the training history in pickle files.\n",
    "\n",
    "### Remarks\n",
    "- The images clearly came from two different marking schemes so training separate models for the images that came with different numbers of keypoints reduced our loss.\n",
    "- Convolutions significantly improve results.\n",
    "- Different optimizers work differently under different circumstances - https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f. Nadam converged faster and seemed to approach the same minimum as Adam. However, Adam did achieve a better minimum loss for the simple models without convolutions.\n",
    "- Normalization is helpful (both of the features and the output) and helps prevent divergence\n",
    "- Batch normalization works well after the activation https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md\n",
    "- Dropout and batch normalization together can cause weird differences between training and validation error due to \"variance shift\". One fix is to only apply dropout after all the batch normalization layers have been applied. http://arxiv.org/abs/1801.05134v1\n",
    "- After bringing validation loss more in line with training loss through normalization and dropout, adding additional complexity through more layers tends to reduce the validation loss.\n",
    "- Though ReLu is the most popular activation function, Exponential Linear Units (ELU) can give better results because their mean value is closer to zero and thus propagates less bias through the network. https://arxiv.org/pdf/1511.07289.pdf\n",
    "- Though training pre-tuned models (\"from the zoo\") is likely a good choice in real world applications, our results training InceptionNetV3 and VGG did not show losses below those from our simpler architectures. This is likely due to the small size of our training dataset.\n",
    "- large strides and convolution window sizes generally worked worse than smaller convolutions with a stride of 1. Smaller windows are in line with the ideas presented in the InceptionV3 model paper of replacing larger windows with multiple small windows. https://arxiv.org/pdf/1512.00567.pdf\n",
    "- Augmentation in image data is a nice way to artifically increase the variance in your training dataset and improve your model.\n",
    "- Depth and additional data can greatly increase training time\n",
    "- GPUs gave a ~30x speedup over CPU training. They are necessary to effectively test.\n",
    "\n",
    "### Next Steps\n",
    "Given more time to test more models, there are a number of other things we would have tested:\n",
    "- Use cross validation to see if the architectures we found were best hold up when the seed we set isn't 42.\n",
    "- Explore more advanced inception based architectures\n",
    "- Utilize more of the images within some missing keypoints (total keypoints != 30 or 8)\n",
    "- Train a separate model for the eye keypoints that uses the data in both groups of images because it appears the two labeling schemes were the same for the eye keypoints\n",
    "- Tune the amounts of data augmentation.\n",
    "- Try models between groups to see if the architectures we found in the other group work better than the architecture found.\n",
    "- If predictions are outside of the image, would predicting the average be better?\n",
    "\n",
    "\n",
    "### Other models tried\n",
    "https://drive.google.com/file/d/1dK069Q08DIZ6g_HIi4osF20DjV2zktC3/view?usp=sharing\n",
    "https://drive.google.com/drive/folders/1IcfVLCy_btNzYmqzvKzyVPFQQkKrYVvZ?usp=sharing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2\\envs\\tf\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15051119247665944028\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4964850073\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 8919657366125453045\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from pandas.io.parsers import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import EarlyStopping, Callback, History\n",
    "\n",
    "# from keras.layers import GlobalAveragePooling2D\n",
    "# from keras.applications.inception_v3  import InceptionV3\n",
    "\n",
    "from augmentdata.CustImageDataGenerator import CustImageDataGenerator,CustNumpyArrayIterator\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices()) # confirm using GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = '../data/training.csv'                # train dataset downloaded from Kaggle\n",
    "TEST_DATA = '../data/test.csv'                     # test dataset downloaded from Kaggle\n",
    "IMAGE_ROWS = 96\n",
    "IMAGE_COLS = 96\n",
    "INPUT_SHAPE = (IMAGE_ROWS, IMAGE_COLS, 1)\n",
    "RETRAIN = False                                     # bool to load and use existing saved models\n",
    "VERBOSE_TRAIN = True                               # bool to show/hide progress while training a model\n",
    "NUM_KEYPOINTS = 30                                 # maximum no. of facial keypoints for any image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "** Aniruddh to add things here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicies of images found in the EDA that should be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final indices of images to be dropped from the dataset\n",
    "\n",
    "IDX_BAD_IMAGES = np.array( [1621, 1862, 1748, 1878, 1927, 2200, 2431, 2584, 2647, \n",
    "                            2671, 2765, 4198, 1627, 1628, 1637, 1957, 4477, 1820, \n",
    "                            2064, 2089, 2091, 2109, 2195, 4264, 4491, 6490, 6493, \n",
    "                            6494, 1655, 2096, 2454, 3206, 3287, 5628, 5653, 6754, \n",
    "                            6755, 2321, 2322, 2414, 2428, 2462, 2574, 2584, 2663, \n",
    "                            2691, 2694, 2830, 2910, 2916, 3126, 3176, 3291, 3299, \n",
    "                            3361, 4061, 4483, 4484, 4494, 4766, 4809, 4837, 4880, \n",
    "                            4905, 5068, 5362, 5566, 5868, 6535, 6538, 6588, 6605, \n",
    "                            6659, 6724, 6733, 6753, 6758, 6766, 6907 ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "Load two separate groups of data. One group has all 30 keypoints the other is for the images that having 8 keypoints. Separate models will be trained on each of these groups because they appeared to have different labeling schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean up the train dataset, normalize it, drop bad images & labels, \n",
    "# and finally split the dataset into 2, for group1 and group2 modelling.\n",
    "\n",
    "def loaderV2(test=False, seed=None, keeplabels=None):\n",
    "    \n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    fileloc = TEST_DATA if test else TRAIN_DATA\n",
    "    \n",
    "    df = read_csv(fileloc)\n",
    "    \n",
    "    df['Image'] = df['Image'].apply(lambda x: np.fromstring(x, sep=' '))\n",
    "    \n",
    "    if keeplabels:\n",
    "        df = df[list(keeplabels) + ['Image']]\n",
    "        \n",
    "    X = np.vstack(df['Image'])\n",
    "    \n",
    "    if not test:                                                  # process train dataset\n",
    "        Y = df[df.columns.difference(['Image'])].values\n",
    "        Y = Y.astype(np.float32)\n",
    "        \n",
    "        # remove rows having bad images or labels\n",
    "        X = np.delete( X, (IDX_BAD_IMAGES - 1), axis=0 )\n",
    "        Y = np.delete( Y, (IDX_BAD_IMAGES - 1), axis=0 )\n",
    "        \n",
    "        # normalize - by pixel across the whole dataset subtract mean and divide by stdev\n",
    "        X = X - np.tile(np.mean(X,axis=0),(X.shape[0],1))\n",
    "        X = X / np.tile(np.std(X,axis=0),(X.shape[0],1))\n",
    "        X = X.astype(np.float32)\n",
    "    \n",
    "        Y = (Y - 48) / 48                     # this helps, but tanh on output doesnt\n",
    "        shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "        X, Y = X[shuffle], Y[shuffle]\n",
    "    \n",
    "        X = X.reshape(-1, 96, 96, 1)\n",
    "        \n",
    "        # split X and Y into dataset for model1 (more than 8 keypoints) and model2 (less than 8 keypoints)\n",
    "        X_model1 = np.zeros( (X.shape[0], 96,96,1), dtype=np.float32)\n",
    "        X_model2 = np.zeros( (X.shape[0], 96,96,1), dtype=np.float32)\n",
    "        Y_model1 = np.zeros( Y.shape, dtype=float)\n",
    "        Y_model2 = np.zeros( Y.shape, dtype=float)\n",
    "        tempIdx1 = 0\n",
    "        tempIdx2 = 0\n",
    "        \n",
    "        for thisIdx in range(0, Y.shape[0]):\n",
    "            numKeyps  = NUM_KEYPOINTS - np.isnan(Y[thisIdx]).sum()\n",
    "            \n",
    "            if( ( numKeyps > 8 ) ):\n",
    "                X_model1[tempIdx1] = X[thisIdx,:,:]\n",
    "                Y_model1[tempIdx1] = Y[thisIdx,:]\n",
    "                tempIdx1 = tempIdx1 + 1\n",
    "            else:\n",
    "                X_model2[tempIdx2] = X[thisIdx,:,:]\n",
    "                Y_model2[tempIdx2] = Y[thisIdx,:]\n",
    "                tempIdx2 = tempIdx2 + 1\n",
    "    \n",
    "        # remove empty rows\n",
    "        drop_idx1 = []\n",
    "        drop_idx2 = []\n",
    "        \n",
    "        for idx in range(0, X.shape[0]):\n",
    "            if( (np.all(Y_model1[idx] == 0)) | (np.isnan(Y_model1[idx]).sum() != 0) ):\n",
    "                drop_idx1.append(idx)\n",
    "            if( (np.all(Y_model2[idx] == 0)) | (np.isnan(Y_model2[idx]).sum() != 22) ):\n",
    "                drop_idx2.append(idx)\n",
    "        \n",
    "        X_model1 = np.delete( X_model1, np.array(drop_idx1), axis=0 )\n",
    "        Y_model1 = np.delete( Y_model1, np.array(drop_idx1), axis=0 )\n",
    "        X_model2 = np.delete( X_model2, np.array(drop_idx2), axis=0 )\n",
    "        Y_model2 = np.delete( Y_model2, np.array(drop_idx2), axis=0 )            \n",
    "        \n",
    "        # remove empty columns, setup lists of labels\n",
    "        labels = df.columns.difference(['Image'])\n",
    "        labels1 = labels\n",
    "        labels2 = []\n",
    "        drop_idx3 = []\n",
    "        for idx in range(0, Y.shape[1]):\n",
    "            if( (np.all(Y_model2[:,idx] == 0)) | (np.isnan(Y_model2[:,idx]).sum() != 0) ):\n",
    "                drop_idx3.append(idx)\n",
    "            else:\n",
    "                labels2.append(labels[idx])\n",
    "        Y_model2 = np.delete( Y_model2, np.array(drop_idx3), axis=1 ) \n",
    "        \n",
    "        # return the original dataset and the group splits\n",
    "        return X_model1, Y_model1, labels1, X_model2, Y_model2, labels2, X, Y, labels\n",
    "    \n",
    "    else:                                             # for test dataset\n",
    "        Y = None\n",
    "        \n",
    "        # normalize - by pixel across the whole dataset subtract mean and divide by stdev\n",
    "        X = X - np.tile(np.mean(X,axis=0),(X.shape[0],1))\n",
    "        X = X / np.tile(np.std(X,axis=0),(X.shape[0],1))\n",
    "        X = X.reshape(-1, 96, 96, 1)\n",
    "        labels = df.columns.difference(['Image'])\n",
    "        \n",
    "        return X, Y, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group1 data Y1 shape:  (2137, 30) , X1 shape:  (2137, 96, 96, 1)\n",
      "Group2 data Y2 shape:  (4697, 8) , X2 shape:  (4697, 96, 96, 1)\n"
     ]
    }
   ],
   "source": [
    "X1, Y1, labels1,   X2, Y2, labels2,   X, Y, labels = loaderV2(seed=42)\n",
    "\n",
    "print(\"Group1 data Y1 shape: \", Y1.shape, \", X1 shape: \", X1.shape)\n",
    "print(\"Group2 data Y2 shape: \", Y2.shape, \", X2 shape: \", X2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting\n",
    "\n",
    "### Model fitting function\n",
    "Here we defined a general model fitting function to run each architecture on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, data, modelname,\n",
    "              generator=None,retrain=RETRAIN,\n",
    "              epochs=10000, patience=1000,optimizer=Nadam()):\n",
    "    # check if the user wants to retrain or if the saved model doesn't exist\n",
    "    if retrain or not os.path.exists('../saved-models/' + modelname + '.h5'):\n",
    "        # data setup\n",
    "        X_train = data[0]\n",
    "        y_train = data[2]\n",
    "        if len(data) == 4:\n",
    "            valid_dat = (data[1], data[3])\n",
    "        else:\n",
    "            valid_dat = None\n",
    "\n",
    "        # default optimization routine to use Nadam and minimize mse\n",
    "        model.compile(loss='mse', optimizer=optimizer)\n",
    "        \n",
    "        # set an early stopping criteria\n",
    "        if valid_dat:\n",
    "            earlystop = EarlyStopping(monitor='val_loss',\n",
    "                                     patience=patience,\n",
    "                                     verbose=1,\n",
    "                                     mode=\"auto\")\n",
    "            \n",
    "        else:\n",
    "            earlystop = EarlyStopping(monitor='loss',\n",
    "                                     patience=patience,\n",
    "                                     verbose=1,\n",
    "                                     mode=\"auto\")\n",
    "        \n",
    "        callbacks = [earlystop]\n",
    "        \n",
    "        # fit the model (with a data generator if present)\n",
    "        if generator:\n",
    "            history = model.fit_generator(generator,\n",
    "                        epochs=epochs,\n",
    "                        steps_per_epoch=data[0].shape[0]//32,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=valid_dat\n",
    "             )\n",
    "        else:\n",
    "            history = model.fit(X_train, y_train,\n",
    "                                epochs=epochs,\n",
    "                                batch_size=32,\n",
    "                                callbacks=callbacks,\n",
    "                                validation_data=valid_dat,\n",
    "                                verbose=VERBOSE_TRAIN\n",
    "                     )\n",
    "        # save the model weights\n",
    "        model.save('../saved-models/'+ modelname + '.h5')\n",
    "        \n",
    "        # save the model loss history\n",
    "        with open('../saved-models/histories/'+modelname+'_hist',\n",
    "                  'wb') as file_pi:\n",
    "            pickle.dump(history.history, file_pi)\n",
    "        history = history.history\n",
    "    \n",
    "    # if the user doesnt want to retrain, load the weights and model history\n",
    "    else:\n",
    "        model = load_model('../saved-models/'+modelname+'.h5')\n",
    "        history = pickle.load(open( \"../saved-models/histories/\" + modelname + '_hist',\n",
    "                                   \"rb\" ))\n",
    "        \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmenting function\n",
    "Keras offers an image data generator class that automatically does transformations to input data images. This helps artifically increase the size of your training data set by adding variance to the training images without distorting their meaning.\n",
    "\n",
    "However, this generator does not change the labels of the images. In our case, when the image changes, we need the labels to change with it. Therefore, we needed to create a custom subclass of the Keras ImageDataGenerator that not only transformed the images, but also transformed the labels with the images. The generator randomly rotates the images by up to 5 degrees, flips the images horizontally, and translates the images up to 5% of the total height/weight of the image. These transformations are only applied if they will not move the keypoints outside of the image.\n",
    "\n",
    "For more detail, see the CustImageDataGenerator.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator for the models that predict 30 keypoints\n",
    "datagen = CustImageDataGenerator(\n",
    "    rotation_range=5. #degrees\n",
    "     ,horizontal_flip=True\n",
    "     ,width_shift_range=.05 # percent of image width\n",
    "     ,height_shift_range=.05 # percent of image height\n",
    "    ).flow(X1,Y1,whichlabels=list(labels1), batch_size=32)\n",
    "\n",
    "# Generator for the models that predict 8 keypoints\n",
    "g2_datagen = CustImageDataGenerator(\n",
    "    rotation_range=5. #degrees\n",
    "     ,horizontal_flip=True\n",
    "     ,width_shift_range=.05 # percent of image width\n",
    "     ,height_shift_range=.05 # percent of image height\n",
    "    ).flow(X2,Y2,whichlabels=list(labels2), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train the Group 1 Images' Model (30 keypoints to predict)\n",
    "If you want to see the model train, pass in retrain=True. BE CAREFUL, this will overwrite the old saved model in the saved-models directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_46 (Conv2D)           (None, 91, 91, 32)        1184      \n",
      "_________________________________________________________________\n",
      "batch_normalization_64 (Batc (None, 91, 91, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 45, 45, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 41, 41, 64)        51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_65 (Batc (None, 41, 41, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling (None, 20, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 17, 17, 256)       262400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_66 (Batc (None, 17, 17, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_46 (MaxPooling (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 6, 6, 64)          147520    \n",
      "_________________________________________________________________\n",
      "batch_normalization_67 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_47 (MaxPooling (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 2, 2, 128)         32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_68 (Batc (None, 2, 2, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 500)               64500     \n",
      "_________________________________________________________________\n",
      "batch_normalization_69 (Batc (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_70 (Batc (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 30)                15030     \n",
      "=================================================================\n",
      "Total params: 831,470\n",
      "Trainable params: 828,382\n",
      "Non-trainable params: 3,088\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "66/66 [==============================] - 6s 86ms/step - loss: 0.9586\n"
     ]
    }
   ],
   "source": [
    "g1_model3_new = Sequential()\n",
    "g1_model3_new.add(Conv2D(32,\n",
    "                 (6, 6),\n",
    "                 activation='relu',\n",
    "                input_shape=INPUT_SHAPE))\n",
    "g1_model3_new.add(BatchNormalization())\n",
    "g1_model3_new.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "g1_model3_new.add(Conv2D(filters=64,\n",
    "                 kernel_size=(5, 5),\n",
    "                 activation='relu'))\n",
    "g1_model3_new.add(BatchNormalization())\n",
    "g1_model3_new.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "g1_model3_new.add(Conv2D(filters=256,\n",
    "                 kernel_size=(4, 4),\n",
    "                 activation='relu'))\n",
    "g1_model3_new.add(BatchNormalization())\n",
    "g1_model3_new.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "g1_model3_new.add(Conv2D(filters=64,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "g1_model3_new.add(BatchNormalization())\n",
    "g1_model3_new.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "g1_model3_new.add(Conv2D(filters=128,\n",
    "                 kernel_size=(2, 2),\n",
    "                 activation='relu'))\n",
    "g1_model3_new.add(BatchNormalization())\n",
    "g1_model3_new.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "g1_model3_new.add(Flatten())\n",
    "g1_model3_new.add(Dense(500, activation = \"relu\"))\n",
    "g1_model3_new.add(BatchNormalization())\n",
    "g1_model3_new.add(Dense(500, activation = \"relu\"))\n",
    "g1_model3_new.add(BatchNormalization())\n",
    "g1_model3_new.add(Dropout(.3))\n",
    "g1_model3_new.add(Dense(30))\n",
    "print(g1_model3_new.summary())\n",
    "g1_model3_new_hist, g1_model3_new = fit_model(g1_model3_new, [X1, None, Y1],\n",
    "                                'g1_CNN_aug_addedLayers',datagen,retrain=RETRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train the Group 2 Images' Model (8 keypoints to predict)\n",
    "If you want to see the model train, pass in retrain=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 91, 91, 64)        2368      \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 91, 91, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 91, 91, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 87, 87, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 87, 87, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 43, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 43, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 40, 40, 256)       524544    \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 40, 40, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 18, 18, 512)       2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 8, 8, 512)         1049088   \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 500)               4096500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 8)                 808       \n",
      "=================================================================\n",
      "Total params: 7,116,784\n",
      "Trainable params: 7,112,640\n",
      "Non-trainable params: 4,144\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10000\n",
      " 39/146 [=======>......................] - ETA: 30s - loss: 0.5565"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-449e37a1cb78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m g2_model4_hist, g2_model4 = fit_model(g2_model4,[X2, None, Y2],\n\u001b[0;32m     53\u001b[0m                                 \u001b[1;34m'g2_CNNv2_aug'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mg2_datagen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m                                      ,retrain=True, patience=100)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-17af9aa3c515>\u001b[0m in \u001b[0;36mfit_model\u001b[1;34m(model, data, modelname, generator, retrain, epochs, patience, optimizer)\u001b[0m\n\u001b[0;32m     36\u001b[0m                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_dat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m              )\n\u001b[0;32m     40\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\envs\\tf\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\envs\\tf\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1251\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1253\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\envs\\tf\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2242\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2243\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2244\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2246\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1888\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1890\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1891\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1892\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\envs\\tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "g2_model4 = Sequential()\n",
    "\n",
    "g2_model4.add(Conv2D(filters=64,\n",
    "                 kernel_size=(6, 6),\n",
    "                 strides=1,\n",
    "                 activation='elu',\n",
    "                 input_shape=INPUT_SHAPE))\n",
    "g2_model4.add(BatchNormalization())\n",
    "g2_model4.add(Dropout(.1))\n",
    "\n",
    "g2_model4.add(Conv2D(filters=128,\n",
    "                 kernel_size=(5, 5),\n",
    "                 strides=1,\n",
    "                 activation='elu'))\n",
    "g2_model4.add(BatchNormalization())\n",
    "g2_model4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "g2_model4.add(Dropout(.2))\n",
    "\n",
    "g2_model4.add(Conv2D(filters=256,\n",
    "                 kernel_size=(4, 4),\n",
    "                 activation='elu'))\n",
    "g2_model4.add(BatchNormalization())\n",
    "g2_model4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "g2_model4.add(Dropout(.2))\n",
    "\n",
    "g2_model4.add(Conv2D(filters=512,\n",
    "                 kernel_size=(3, 3),\n",
    "                 activation='elu'))\n",
    "g2_model4.add(BatchNormalization())\n",
    "g2_model4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "g2_model4.add(Dropout(.3))\n",
    "\n",
    "g2_model4.add(Conv2D(filters=512,\n",
    "                 kernel_size=(2, 2),\n",
    "                 activation='elu'))\n",
    "g2_model4.add(BatchNormalization())\n",
    "g2_model4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "g2_model4.add(Dropout(.4))\n",
    "\n",
    "g2_model4.add(Flatten())\n",
    "g2_model4.add(Dense(500, activation = \"elu\"))\n",
    "g2_model4.add(BatchNormalization())\n",
    "g2_model4.add(Dropout(.4))\n",
    "\n",
    "g2_model4.add(Dense(100, activation = \"elu\"))\n",
    "g2_model4.add(BatchNormalization())\n",
    "\n",
    "g2_model4.add(Dense(8))\n",
    "\n",
    "print(g2_model4.summary())\n",
    "\n",
    "g2_model4_hist, g2_model4 = fit_model(g2_model4,[X2, None, Y2],\n",
    "                                'g2_CNNv2_aug', generator=g2_datagen\n",
    "                                     ,retrain=RETRAIN, patience=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "### Load the test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1783, 96, 96, 1)\n"
     ]
    }
   ],
   "source": [
    "out_images, _ , _ = loaderV2(test=True, seed=None, keeplabels=None)\n",
    "print(out_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create predictions\n",
    "Make predictions with both models. g1 predicts all 30 points, g2 predicts 8 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_prediction = g1_model3_new.predict(out_images)\n",
    "g2_prediction = g2_model4.predict(out_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the predictions into the submission format\n",
    "#### Add keypoints per test image to be predicted\n",
    "This is necessary to know which model's predictions to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test images: (1783,)\n"
     ]
    }
   ],
   "source": [
    "# aggregate the number of keypoints needed for each distinct Image in test dataset\n",
    "IdLookupTable = read_csv('../data/IdLookupTable.csv')\n",
    "\n",
    "rowIDs = np.array(IdLookupTable['RowId'])\n",
    "imgs_numKeyps = np.zeros(max(IdLookupTable['ImageId']), dtype=int)      # keypoints to be predicted for each imageID\n",
    "thisImgKeyps = 0\n",
    "\n",
    "for rowIdx in range(0, rowIDs.shape[0]):\n",
    "    thisImgID = IdLookupTable.loc[rowIdx,'ImageId']\n",
    "    imgs_numKeyps[thisImgID - 1] += 1\n",
    "\n",
    "print(\"Total test images: {}\".format(imgs_numKeyps.shape) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the prediction from the correct model to each image\n",
    "That is if the test data requests 8 or fewer keypoints use the group 2 model, otherwise use the group 1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowId</th>\n",
       "      <th>ImageId</th>\n",
       "      <th>FeatureName</th>\n",
       "      <th>Location</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>left_eye_center_x</td>\n",
       "      <td>66.238445</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>left_eye_center_y</td>\n",
       "      <td>37.892195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>right_eye_center_x</td>\n",
       "      <td>28.828826</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>right_eye_center_y</td>\n",
       "      <td>36.806757</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>left_eye_inner_corner_x</td>\n",
       "      <td>59.648207</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowId  ImageId              FeatureName   Location  test\n",
       "0      1        1        left_eye_center_x  66.238445     0\n",
       "1      2        1        left_eye_center_y  37.892195     1\n",
       "2      3        1       right_eye_center_x  28.828826    20\n",
       "3      4        1       right_eye_center_y  36.806757    21\n",
       "4      5        1  left_eye_inner_corner_x  59.648207     2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_locs1 = {}\n",
    "label_locs2 = {}\n",
    "\n",
    "for i, label in enumerate(labels1):\n",
    "    label_locs1[label]=i\n",
    "\n",
    "for i, label in enumerate(labels2):\n",
    "    label_locs2[label]=i\n",
    "    \n",
    "IdLookupTable['test'] = IdLookupTable['FeatureName'].replace(label_locs1)\n",
    "\n",
    "thisRowId = 0\n",
    "modelUsed = np.zeros(IdLookupTable['RowId'].shape[0], dtype=int)\n",
    "\n",
    "for imgIdx in range(0, imgs_numKeyps.shape[0]):\n",
    "    \n",
    "    if( imgs_numKeyps[imgIdx] > 8):\n",
    "        for keypIdx in range(0, imgs_numKeyps[imgIdx]):\n",
    "            map_label_idx = label_locs1[IdLookupTable.loc[thisRowId, 'FeatureName']]\n",
    "            IdLookupTable.loc[thisRowId, 'Location'] =  (48*g1_prediction[imgIdx, map_label_idx]) + 48\n",
    "            modelUsed[thisRowId] = 1\n",
    "            thisRowId += 1\n",
    "    else:\n",
    "        for keypIdx in range(0, imgs_numKeyps[imgIdx]):\n",
    "            map_label_idx = label_locs2[IdLookupTable.loc[thisRowId, 'FeatureName']]\n",
    "            IdLookupTable.loc[thisRowId, 'Location'] =  (48*g2_prediction[imgIdx, map_label_idx]) + 48\n",
    "            modelUsed[thisRowId] = 2\n",
    "            thisRowId += 1\n",
    "\n",
    "IdLookupTable['Location'] = (IdLookupTable['Location'].\n",
    "                             where(IdLookupTable['Location']<=96, 96).\n",
    "                             where(IdLookupTable['Location']>=0, 0)\n",
    "                            )\n",
    "IdLookupTable.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission = IdLookupTable[['RowId','Location']]\n",
    "Submission.to_csv(path_or_buf='Final_FacialKeypoints.csv',\n",
    "                  index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutation\n",
    "Our final model's predictions submitted to kaggle produce a RMSE error of 1.71947 on the unseen test data labels. This places us 4th on the public Kaggle leaderboard. This is much better than the baseline we set of 3.138 just using the mean value of the labels as our prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/Score.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
